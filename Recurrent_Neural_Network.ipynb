{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+FlJbFTrfLCrbp1ftJhjY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cliffochi/aviva_data_science_course/blob/main/Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement a basic Recurrent Neural Network (RNN) from scratch, we'll create a ScratchSimpleRNNClassifier class, with its core component being the SimpleRNN layer.\n",
        "\n",
        "We'll structure it to only support forward propagation first (with minimal libraries like numpy) and validate the output using the provided test example."
      ],
      "metadata": {
        "id": "61K-b-i5OC9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### 1. Create the `SimpleRNN` class\n",
        "\n",
        "This class handles forward propagation. It takes:\n",
        "\n",
        "* input `x` of shape `(batch_size, n_sequences, n_features)`\n",
        "* initial state `h` of shape `(batch_size, n_nodes)`\n",
        "* weights `W_x` and `W_h`, bias `B`\n",
        "\n",
        "It computes each time step using:\n",
        "\n",
        "$$\n",
        "a_t = x_t \\cdot W_x + h_{t-1} \\cdot W_h + B\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(a_t)\n",
        "$$"
      ],
      "metadata": {
        "id": "kTgQvRhOOWpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Create the `ScratchSimpleRNNClassifier` class\n",
        "\n",
        "This wraps the RNN layer and adds a simple output layer with sigmoid or softmax (for classification).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EoO-cmEnO0M8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PROclsjENqFp"
      },
      "outputs": [],
      "source": [
        "# Forward Propagation Only\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    def __init__(self, n_features, n_nodes):\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes = n_nodes\n",
        "        # Weight initialization\n",
        "        self.W_x = np.random.randn(n_features, n_nodes) * 0.01\n",
        "        self.W_h = np.random.randn(n_nodes, n_nodes) * 0.01\n",
        "        self.B = np.zeros(n_nodes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: shape (batch_size, n_sequences, n_features)\n",
        "        returns:\n",
        "        h: hidden state at final time step, shape (batch_size, n_nodes)\n",
        "        \"\"\"\n",
        "        batch_size, n_sequences, _ = x.shape\n",
        "        h = np.zeros((batch_size, self.n_nodes))\n",
        "        self.hs = []  # Save intermediate h for backpropagation if needed\n",
        "        for t in range(n_sequences):\n",
        "            x_t = x[:, t, :]  # shape: (batch_size, n_features)\n",
        "            a_t = np.dot(x_t, self.W_x) + np.dot(h, self.W_h) + self.B\n",
        "            h = np.tanh(a_t)\n",
        "            self.hs.append(h)\n",
        "        return h  # Return final h_t"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### Test Case from the Sprint"
      ],
      "metadata": {
        "id": "alRX_35jPSaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provided small-scale inputs\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]]) / 100  # (1, 3, 2)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]]) / 100  # (2, 4)\n",
        "w_h = np.array([[1, 3, 5, 7],\n",
        "                [2, 4, 6, 8],\n",
        "                [3, 5, 7, 8],\n",
        "                [4, 6, 8, 10]]) / 100  # (4, 4)\n",
        "b = np.array([1, 1, 1, 1])  # (4,)\n",
        "\n",
        "# Override weights for testing\n",
        "rnn = SimpleRNN(n_features=2, n_nodes=4)\n",
        "rnn.W_x = w_x\n",
        "rnn.W_h = w_h\n",
        "rnn.B = b\n",
        "\n",
        "# Forward pass\n",
        "h = rnn.forward(x)\n",
        "print(\"h =\", h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0lngL57PQ58",
        "outputId": "85d530a8-d141-45bc-8850-873dd0fb8cdc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h = [[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Implementing **backward propagation** for the `SimpleRNN` class step by step.\n",
        "\n",
        "---\n",
        "\n",
        "#### Overview of RNN Backward Propagation\n",
        "\n",
        "Given:\n",
        "\n",
        "* Forward equations:\n",
        "\n",
        "  $$\n",
        "  a_t = x_t \\cdot W_x + h_{t-1} \\cdot W_h + B\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  h_t = \\tanh(a_t)\n",
        "  $$\n",
        "\n",
        "* Backward derivatives:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} \\cdot (1 - \\tanh^2(a_t))\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial W_x} += x_t^T \\cdot \\frac{\\partial L}{\\partial a_t}\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial W_h} += h_{t-1}^T \\cdot \\frac{\\partial L}{\\partial a_t}\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial B} += \\frac{\\partial L}{\\partial a_t}\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial L}{\\partial a_t} \\cdot W_h^T\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "#### Updated `SimpleRNN` Class with Backward Pass"
      ],
      "metadata": {
        "id": "_vTtgIYmSfBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    def __init__(self, n_features, n_nodes):\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes = n_nodes\n",
        "        self.W_x = np.random.randn(n_features, n_nodes) * 0.01\n",
        "        self.W_h = np.random.randn(n_nodes, n_nodes) * 0.01\n",
        "        self.B = np.zeros(n_nodes, dtype=np.float64) # Initialize with float dtype\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size, n_sequences, _ = x.shape\n",
        "        self.h_list = []\n",
        "        self.a_list = []\n",
        "        h = np.zeros((batch_size, self.n_nodes))\n",
        "        self.h_list.append(h.copy())  # h_0\n",
        "\n",
        "        for t in range(n_sequences):\n",
        "            x_t = x[:, t, :]\n",
        "            a_t = np.dot(x_t, self.W_x) + np.dot(h, self.W_h) + self.B\n",
        "            h = np.tanh(a_t)\n",
        "            self.a_list.append(a_t)\n",
        "            self.h_list.append(h)\n",
        "        return h  # return final state\n",
        "\n",
        "    def backward(self, dh_last, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        dh_last: gradient of the loss w.r.t. h at the final time step (batch_size, n_nodes)\n",
        "        \"\"\"\n",
        "        batch_size, n_sequences, _ = self.x.shape\n",
        "\n",
        "        # Initialize gradients\n",
        "        dW_x = np.zeros_like(self.W_x)\n",
        "        dW_h = np.zeros_like(self.W_h)\n",
        "        dB = np.zeros_like(self.B)\n",
        "        dh_next = dh_last.copy()\n",
        "\n",
        "        for t in reversed(range(n_sequences)):\n",
        "            a_t = self.a_list[t]\n",
        "            h_prev = self.h_list[t]\n",
        "            x_t = self.x[:, t, :]\n",
        "\n",
        "            # Derivative through tanh activation\n",
        "            da_t = dh_next * (1 - np.tanh(a_t) ** 2)  # (batch_size, n_nodes)\n",
        "\n",
        "            # Gradient accumulation\n",
        "            dW_x += np.dot(x_t.T, da_t)  # (n_features, n_nodes)\n",
        "            dW_h += np.dot(h_prev.T, da_t)  # (n_nodes, n_nodes)\n",
        "            dB += np.sum(da_t, axis=0)\n",
        "\n",
        "            # Update for next step\n",
        "            dh_next = np.dot(da_t, self.W_h.T)  # to pass to h_{t-1}\n",
        "\n",
        "        # Gradient descent update\n",
        "        self.W_x -= learning_rate * dW_x\n",
        "        self.W_h -= learning_rate * dW_h\n",
        "        self.B -= learning_rate * dB"
      ],
      "metadata": {
        "id": "mvk59Ck_S0O8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Example: Using `forward()` and `backward()`"
      ],
      "metadata": {
        "id": "Wnh_qYLsS8aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]]) / 100\n",
        "rnn = SimpleRNN(n_features=2, n_nodes=4)\n",
        "\n",
        "# Overriding weights and bias for testability\n",
        "rnn.W_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]]) / 100\n",
        "rnn.W_h = np.array([[1, 3, 5, 7],\n",
        "                    [2, 4, 6, 8],\n",
        "                    [3, 5, 7, 8],\n",
        "                    [4, 6, 8, 10]]) / 100\n",
        "rnn.B = np.array([1, 1, 1, 1])\n",
        "\n",
        "# Forward pass\n",
        "h = rnn.forward(x)\n",
        "\n",
        "# Assume dummy gradient from a loss (normally from output layer)\n",
        "dh_last = np.ones_like(h)\n",
        "\n",
        "# Backward pass\n",
        "rnn.backward(dh_last, learning_rate=0.1)\n"
      ],
      "metadata": {
        "id": "mpPXCGb-T7j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Next Steps\n",
        "\n",
        "---\n",
        "* Add a fully connected output layer with cross-entropy/sigmoid for training.\n",
        "* Create a full `ScratchSimpleRNNClassifier` that trains on toy sequence classification datasets.\n",
        "* Wrap this in a classifier with a softmax/sigmoid output layer for classification.\n",
        "* Train on real datasets."
      ],
      "metadata": {
        "id": "c-CtAsX8PpAx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0NAp-Fb6Pj6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}