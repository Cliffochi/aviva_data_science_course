{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eNYLA6JpgbLfuWFkj9T16NOF0l2REbxN",
      "authorship_tag": "ABX9TyO19DfW//5FwAXrAoj40qNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cliffochi/aviva_data_science_course/blob/main/Ensemble_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Learning\n",
        "\n",
        "###Implementation for House Price Prediction\n",
        "\n",
        "Let's implement blending, bagging, and stacking techniques to improve prediction accuracy on the Kaggle house prices dataset."
      ],
      "metadata": {
        "id": "XPxyf37oWA-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')[['GrLivArea', 'YearBuilt', 'SalePrice']]\n",
        "\n",
        "# Handle missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Log transform target for better normality\n",
        "data['SalePrice'] = np.log1p(data['SalePrice'])\n",
        "\n",
        "# Split data\n",
        "X = data[['GrLivArea', 'YearBuilt']]\n",
        "y = data['SalePrice']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "rxeCd4WFWKjM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blending implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Initialize base models\n",
        "linear = LinearRegression()\n",
        "svm = SVR(kernel='rbf', C=100, gamma=0.1)\n",
        "tree = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "# Train models\n",
        "linear.fit(X_train_scaled, y_train)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "tree.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predictions\n",
        "pred_linear = linear.predict(X_val_scaled)\n",
        "pred_svm = svm.predict(X_val_scaled)\n",
        "pred_tree = tree.predict(X_val_scaled)\n",
        "\n",
        "# Blend with equal weights\n",
        "blended_pred = (pred_linear + pred_svm + pred_tree) / 3\n",
        "\n",
        "# Evaluate\n",
        "mse_blended = mean_squared_error(y_val, blended_pred)\n",
        "print(f\"Blended MSE: {mse_blended:.5f}\")\n",
        "\n",
        "# Compare with individual models\n",
        "for name, pred in [('Linear', pred_linear), ('SVM', pred_svm), ('Tree', pred_tree)]:\n",
        "    mse = mean_squared_error(y_val, pred)\n",
        "    print(f\"{name} MSE: {mse:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IAqqVrKWjaR",
        "outputId": "08339150-6090-41a1-8a50-43e730622b9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended MSE: 0.04546\n",
            "Linear MSE: 0.05186\n",
            "SVM MSE: 0.04603\n",
            "Tree MSE: 0.04778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging implementation\n",
        "from sklearn.utils import resample\n",
        "\n",
        "class BaggingRegressor:\n",
        "    def __init__(self, base_model, n_estimators=10):\n",
        "        self.models = [base_model() for _ in range(n_estimators)]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            X_sample, y_sample = resample(X, y)\n",
        "            model.fit(X_sample, y_sample)\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = np.array([model.predict(X) for model in self.models])\n",
        "        return np.mean(preds, axis=0)\n",
        "\n",
        "# Create and fit bagging model\n",
        "bagged_tree = BaggingRegressor(lambda: DecisionTreeRegressor(max_depth=5), n_estimators=50)\n",
        "bagged_tree.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "pred_bagged = bagged_tree.predict(X_val_scaled)\n",
        "mse_bagged = mean_squared_error(y_val, pred_bagged)\n",
        "print(f\"\\nBagged Tree MSE: {mse_bagged:.5f}\")\n",
        "print(f\"Single Tree MSE: {mean_squared_error(y_val, pred_tree):.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6-5yY9cW2Kr",
        "outputId": "b100d50d-7e77-4811-af3d-e7362bf968d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bagged Tree MSE: 0.04630\n",
            "Single Tree MSE: 0.04778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stacking implementation\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('linear', LinearRegression()),\n",
        "    ('svm', SVR(kernel='rbf')),\n",
        "    ('tree', DecisionTreeRegressor(max_depth=5))\n",
        "]\n",
        "\n",
        "# Define meta-model\n",
        "meta_model = RidgeCV()\n",
        "\n",
        "# Create stacking model\n",
        "stacked_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Fit and predict\n",
        "stacked_model.fit(X_train_scaled, y_train)\n",
        "pred_stacked = stacked_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mse_stacked = mean_squared_error(y_val, pred_stacked)\n",
        "print(f\"\\nStacked Model MSE: {mse_stacked:.5f}\")\n",
        "\n",
        "# Show weights learned by meta-model\n",
        "print(\"\\nMeta-model coefficients:\", stacked_model.final_estimator_.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASAaGrKWXJX9",
        "outputId": "fd93f219-5577-457f-e66f-9dd1da90b54d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stacked Model MSE: 0.04541\n",
            "\n",
            "Meta-model coefficients: [0.30570458 0.42234448 0.27543512]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning Results Comparison and Recommendations\n",
        "\n",
        "## Performance Comparison\n",
        "\n",
        "| Method          | MSE       | Improvement vs Best Base Model | Model Weights (Stacking) |\n",
        "|-----------------|-----------|--------------------------------|--------------------------|\n",
        "| Base - Linear   | 0.05186   | -                              | -                        |\n",
        "| Base - SVM      | 0.04603   | -                              | -                        |\n",
        "| Base - Tree     | 0.04778   | -                              | -                        |\n",
        "| **Blending**    | 0.04546   | 1.24% better than SVM          | Equal weights            |\n",
        "| **Bagging**     | 0.04630   | 0.59% worse than SVM           | -                        |\n",
        "| **Stacking**    | **0.04541** | **1.35% better than SVM**      | Linear: 0.31, SVM: 0.42, Tree: 0.28 |\n",
        "\n",
        "## Key Insights\n",
        "\n",
        "1. **Model Effectiveness**:\n",
        "   - All ensemble methods outperformed the base Linear model (4.6-12.4% improvement)\n",
        "   - Stacking provided the best overall performance (1.35% better than the best base model)\n",
        "   - Blending showed nearly comparable results to stacking with simpler implementation\n",
        "   - Bagging improved the Tree model but didn't surpass SVM's performance\n",
        "\n",
        "2. **Weight Analysis**:\n",
        "   - The stacking meta-model assigned highest weight to SVM (0.42), confirming it as the strongest individual predictor\n",
        "   - Linear regression contributed more than the tree (0.31 vs 0.28), suggesting its stability adds value\n",
        "   - The balanced weights indicate each model brings unique predictive value\n",
        "\n",
        "3. **Performance Patterns**:\n",
        "   - The relatively small margins between methods suggest the base models were already reasonably strong\n",
        "   - SVM's strong individual performance limited potential ensemble gains\n",
        "   - The consistent outperformance of ensembles demonstrates their robustness\n",
        "\n",
        "## Practical Recommendations\n",
        "\n",
        "1. **For Production Deployment**:\n",
        "   *\"Implement the stacking model as it provides the best accuracy (MSE 0.04541) with reasonable complexity. The 1.35% improvement over the best base model could translate to significant business value at scale.\"*\n",
        "\n",
        "2. **For Rapid Implementation**:\n",
        "   *\"Use blending with equal weights if development resources are limited - it achieves 99% of stacking's performance with much simpler implementation and maintenance.\"*\n",
        "\n",
        "3. **For Model Improvement**:\n",
        "   *\"Experiment with adding more diverse base models to the stacking ensemble, particularly models that might capture different patterns than SVM and linear regression.\"*\n",
        "\n",
        "4. **For Resource Optimization**:\n",
        "   *\"Consider using just the SVM model if computational resources are extremely constrained, as the ensemble gains are modest in this case.\"*\n",
        "\n",
        "5. **Next Steps**:\n",
        "   *\"Investigate why bagging underperformed - try increasing the number of estimators or using different base models. Also explore feature engineering to create more differentiation between models' strengths.\"*\n",
        "\n",
        "###Conclusion\n",
        "The results demonstrate that while all ensemble methods provided value, stacking delivered the best performance by intelligently combining model strengths through learned weights. The choice between methods should balance performance needs with implementation complexity in specific environment."
      ],
      "metadata": {
        "id": "dC2E4TdhYXkj"
      }
    }
  ]
}