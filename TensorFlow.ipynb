{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Vl2JcOnfp0Z0H9Nbu3xQCZT0TDNdTwvQ",
      "authorship_tag": "ABX9TyNTZNBbF9KE/hP4iQNrIjQl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cliffochi/aviva_data_science_course/blob/main/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TensorFlow\n",
        "\n",
        "###[Question 1] Looking back at Scratch"
      ],
      "metadata": {
        "id": "TerXbwbfnrQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking back at implementing deep learning from scratch, here are the key components that were needed:  \n",
        "\n",
        "### **1. Model Architecture**  \n",
        "- **Neural Network Layers**: Defining the structure (e.g., Dense, Conv2D, LSTM).  \n",
        "- **Weight Initialization**: Setting initial values for weights (e.g., random, Xavier, He).  \n",
        "- **Bias Initialization**: Initializing bias terms.  \n",
        "\n",
        "### **2. Forward Propagation**  \n",
        "- **Input Handling**: Passing input data through the network.  \n",
        "- **Activation Functions**: Applying ReLU, Sigmoid, Tanh, etc.  \n",
        "- **Loss Calculation**: Computing loss (e.g., Cross-Entropy, MSE).  \n",
        "\n",
        "### **3. Backward Propagation (Gradient Calculation)**  \n",
        "- **Gradient Computation**: Calculating gradients using chain rule.  \n",
        "- **Loss Gradient**: Deriving gradients w.r.t. loss.  \n",
        "- **Weight & Bias Updates**: Adjusting parameters using gradients.  \n",
        "\n",
        "### **4. Optimization**  \n",
        "- **Optimizer Implementation**: Updating weights (e.g., SGD, Adam, RMSprop).  \n",
        "- **Learning Rate**: Managing step size for updates.  \n",
        "\n",
        "### **5. Training Loop**  \n",
        "- **Epoch Loop**: Iterating over the entire dataset.  \n",
        "- **Batch Processing**: Splitting data into mini-batches.  \n",
        "- **Validation**: Monitoring performance on validation data.  \n",
        "\n",
        "### **6. Evaluation**  \n",
        "- **Accuracy/Loss Metrics**: Measuring model performance.  \n",
        "- **Prediction**: Running inference on test data.  \n",
        "\n",
        "### **7. Data Handling**  \n",
        "- **Data Loading**: Reading input data (e.g., CSV, images).  \n",
        "- **Preprocessing**: Normalization, reshaping, one-hot encoding.  \n",
        "- **Batching**: Creating mini-batches for training.  \n",
        "\n",
        "### **8. Debugging & Monitoring**  \n",
        "- **Gradient Checking**: Ensuring correct backpropagation.  \n",
        "- **Logging**: Tracking loss/accuracy over epochs.  \n",
        "\n",
        "---  \n",
        "### **How Frameworks (Like TensorFlow) Implement These**  \n",
        "1. **Automatic Differentiation** → No manual gradient computation (uses `tf.GradientTape`).  \n",
        "2. **Predefined Layers** → `tf.keras.layers` handles weight initialization and forward pass.  \n",
        "3. **Built-in Optimizers** → `tf.optimizers` (SGD, Adam, etc.) manage updates.  \n",
        "4. **Loss Functions** → `tf.losses` provides common loss computations.  \n",
        "5. **Training Loop Abstraction** → `model.fit()` automates epochs/batches.  \n"
      ],
      "metadata": {
        "id": "ZPymLcO3nzml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Question 2] Considering compatibility between Scratch and TensorFlow"
      ],
      "metadata": {
        "id": "-By6cOGbh0mg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26RKUyNDhfLR",
        "outputId": "42a1340c-df5f-4558-fe25-3b47999f1de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc : 0.950\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Iris.csv\")\n",
        "\n",
        "# Filter the DataFrame by specific conditions\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Convert labels to numeric\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Further split train into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator for retrieving mini-batches\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray of shape (n_samples, n_features)\n",
        "        Training data\n",
        "    y : ndarray of shape (n_samples, 1)\n",
        "        Ground truth labels\n",
        "    batch_size : int\n",
        "        Size of each mini-batch\n",
        "    seed : int\n",
        "        Seed for NumPy's random number generator\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        # Use np.intp instead of np.int as np.int is deprecated\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.intp)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Define the model using Keras\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu', input_shape=(n_input,)),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Use Adam optimizer and BinaryCrossentropy loss for binary classification\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Mini-batch iterator for training\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "# Create a TensorFlow dataset from the generator\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: get_mini_batch_train, # Use the instance of your iterator\n",
        "    output_types=(tf.float64, tf.int64), # Specify the data types of the output\n",
        "    output_shapes=((None, n_input), (None, n_classes)) # Specify the shapes of the output (None for batch size)\n",
        ")\n",
        "\n",
        "# Train the model using the created dataset\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=0, # Set to 1 to see progress\n",
        "                    validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of how the **\"things needed to implement deep learning\"** from scratch map to TensorFlow’s implementation in the sample code.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model Architecture**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Manually define layers (e.g., `DenseLayer` class). | Layers are defined via `tf.Variable` for weights/biases and `tf.matmul` + `tf.add`. |\n",
        "| Explicit weight initialization (e.g., He initialization). | Uses `tf.random_normal` for initialization. |\n",
        "| Hand-coded forward pass (e.g., `forward()` method). | Forward pass is a sequence of `tf.matmul`, `tf.add`, and `tf.nn.relu`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "# TensorFlow's manual layer definition\n",
        "layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "layer_1 = tf.nn.relu(layer_1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Forward & Backward Propagation**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Manual gradient calculations (chain rule). | **Automatic differentiation** via `tf.GradientTape` (not shown here, but `train_op` handles it). |\n",
        "| Hand-written loss computation (e.g., cross-entropy). | Uses `tf.nn.sigmoid_cross_entropy_with_logits`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "# Loss and gradients handled automatically\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)  # Backpropagation happens here\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Training Loop**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Custom epoch/batch loops (e.g., `for epoch in epochs`). | Same epoch loop, but batching is abstracted via `GetMiniBatch` class. |\n",
        "| Manual parameter updates (e.g., `weights -= lr * gradients`). | Optimizer (`AdamOptimizer`) handles updates via `train_op`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})  # Updates weights\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Data Handling**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Manual data splitting/shuffling. | Uses `sklearn.model_selection.train_test_split` and custom `GetMiniBatch` iterator. |\n",
        "| One-hot encoding by hand. | Labels converted to `0`/`1` manually. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "# Manual batching (similar to scratch)\n",
        "class GetMiniBatch:\n",
        "    def __next__(self):\n",
        "        return self.X[p0:p1], self.y[p0:p1]  # Returns mini-batch\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Evaluation**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Custom accuracy/loss calculations. | Uses `tf.reduce_mean` and `tf.cast` for metrics. |\n",
        "| Manual validation/testing loops. | Evaluated in-session via `sess.run`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**  \n",
        "1. **TensorFlow Automates Gradients**: No need for manual backpropagation (handled by `optimizer.minimize()`).  \n",
        "2. **Low-Level Still Requires Manual Setup**: Layers, batching, and training loops are explicit (unlike `tf.keras`).  \n",
        "3. **Placeholders Feed Data**: `tf.placeholder` acts as an input pipeline (replaced by `tf.data` in TF 2.x).  \n",
        "\n",
        "---\n",
        "### **Why This Matters**  \n",
        "Understanding this mapping helps:  \n",
        "- Transition from scratch to frameworks.  \n",
        "- Debug low-level TensorFlow code.  \n",
        "- Appreciate high-level APIs (like `tf.keras`) that abstract these steps.  "
      ],
      "metadata": {
        "id": "drxStzOdh-kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Application to other datasets\n",
        "###[Problem 3] Create a model for Iris using all three objective variables"
      ],
      "metadata": {
        "id": "iYYYQ4vkvrEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Iris (3-Class Classification)\n",
        "Key Changes from Binary to Multi-Class:\n",
        "\n",
        "- Labels: One-hot encode Species (3 classes: setosa, versicolor, virginica).\n",
        "\n",
        "- Loss Function: Use tf.nn.softmax_cross_entropy_with_logits instead of sigmoid.\n",
        "\n",
        "- Output Layer: 3 neurons (one per class) with linear activation (logits).\n",
        "\n",
        "- Accuracy: Compare predicted class (tf.argmax) with true class."
      ],
      "metadata": {
        "id": "BiUp-Jjsxn7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load Iris dataset (all 3 classes)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Iris.csv\")\n",
        "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
        "y = pd.get_dummies(df['Species']).values  # One-hot encoding\n",
        "\n",
        "# Split into train/val/test (60/20/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "# Convert data types to match TensorFlow defaults (float32)\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val = X_val.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_val = y_val.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_input = X_train.shape[1] # Number of features\n",
        "n_classes = 3  # Now 3 classes!\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "\n",
        "\n",
        "# Define the model using Keras Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu', input_shape=(n_input,)),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    # For multi-class classification, the output layer has n_classes neurons.\n",
        "    # No activation is needed here if using from_logits=True in the loss function.\n",
        "    tf.keras.layers.Dense(n_classes)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Use Adam optimizer and CategoricalCrossentropy loss for multi-class classification\n",
        "# from_logits=True means the output layer does not have a softmax activation\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data directly\n",
        "# Keras handles the batching internally when you provide numpy arrays\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=0, # Set to 1 to see progress\n",
        "                    validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOsCPSO4iVLY",
        "outputId": "9dce2ae4-39b1-4bb7-c0ad-65c9957d055c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###House Prices (Regression)\n",
        "Key Changes for Regression:\n",
        "\n",
        "- Labels: Continuous values (no one-hot encoding).\n",
        "\n",
        "- Loss Function: Use Mean Squared Error (tf.losses.mean_squared_error).\n",
        "\n",
        "- Output Layer: 1 neuron with linear activation.\n",
        "\n",
        "- Metrics: Track MSE or RMSE instead of accuracy."
      ],
      "metadata": {
        "id": "pSz2zuF6yMsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load House Prices dataset (example)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n",
        "X = df[['feature1', 'feature2', ...]].values  # Select features\n",
        "y = df['price'].values.reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
        "\n",
        "# Split into train/val/test (same as Iris)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "# Placeholders\n",
        "X_ph = tf.placeholder(tf.float32, [None, X.shape[1]])\n",
        "Y_ph = tf.placeholder(tf.float32, [None, 1])  # Single output\n",
        "\n",
        "# Model (regression)\n",
        "def house_net(x):\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([X.shape[1], 50])),\n",
        "        'w2': tf.Variable(tf.random_normal([50, 100])),\n",
        "        'w3': tf.Variable(tf.random_normal([100, 1]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([50])),\n",
        "        'b2': tf.Variable(tf.random_normal([100])),\n",
        "        'b3': tf.Variable(tf.random_normal([1]))\n",
        "    }\n",
        "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['w1']), biases['b1']))\n",
        "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']))\n",
        "    return tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])  # Linear output\n",
        "\n",
        "pred = house_net(X_ph)\n",
        "\n",
        "# Loss (MSE)\n",
        "loss_op = tf.losses.mean_squared_error(labels=Y_ph, predictions=pred)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Training (same loop as Iris, but track MSE/RMSE)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for epoch in range(num_epochs):\n",
        "        # ... (same training loop)\n",
        "        val_mse = sess.run(loss_op, feed_dict={X_ph: X_val, Y_ph: y_val})\n",
        "        print(f\"Epoch {epoch}, Val MSE: {val_mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hnLCkLtDxgFC",
        "outputId": "253c16d0-96a8-454e-a969-6fe0ce263c16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['feature1', 'feature2', Ellipsis], dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e0eed7f9e282>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load House Prices dataset (example)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[0;31m# Select features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape to (n_samples, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['feature1', 'feature2', Ellipsis], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load House Prices dataset (example)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n",
        "\n",
        "X = df[['GrLivArea', 'YearBuilt']].values  # Select features\n",
        "y = df['SalePrice'].values.reshape(-1, 1)  # Assuming 'SalePrice' is the target variable and reshaping it\n",
        "\n",
        "# Convert data types to match TensorFlow defaults (float32)\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# Split into train/val/test (same as Iris)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001 # Using the learning rate from the Iris example\n",
        "batch_size = 10 # Using the batch size from the Iris example\n",
        "num_epochs = 100\n",
        "n_input = X_train.shape[1] # Number of features\n",
        "n_output = 1 # Single output for regression\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "\n",
        "# Define the model using Keras Sequential API for regression\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu', input_shape=(n_input,)),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    # For regression, the output layer has 1 neuron and no activation (linear output)\n",
        "    tf.keras.layers.Dense(n_output)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Use Adam optimizer and MeanSquaredError loss for regression\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['mse']) # Use Mean Squared Error as a metric\n",
        "\n",
        "# Train the model using the data directly\n",
        "# Keras handles the batching internally when you provide numpy arrays\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=0, # Set to 1 to see progress\n",
        "                    validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test MSE: {test_mse:.2f}\")\n",
        "\n",
        "# You might also want to calculate RMSE for regression\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "print(f\"Test RMSE: {test_rmse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc9CLXmi0yqp",
        "outputId": "160bae5d-e87c-4a10-972c-5b657a895f1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 3921277952.00\n",
            "Test RMSE: 62620.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Question 4] Create a model for House Prices"
      ],
      "metadata": {
        "id": "3KCAst_01PeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset (ensure 'train.csv' is in your directory)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n",
        "\n",
        "# Select features and target. Using more features than the example to potentially improve performance.\n",
        "# Added numerical features that are likely relevant to house prices.\n",
        "features = ['GrLivArea', 'YearBuilt', 'OverallQual', 'TotalBsmtSF',\n",
        "            '1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'GarageArea', 'WoodDeckSF',\n",
        "            'Fireplaces', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BedroomAbvGr',\n",
        "            'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n",
        "\n",
        "# Filter out rows with missing values in the selected features for simplicity in this example\n",
        "# In a real project, you would handle missing data more robustly (e.g., imputation)\n",
        "df = df.dropna(subset=features + ['SalePrice'])\n",
        "\n",
        "X = df[features].values\n",
        "y = df['SalePrice'].values.reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
        "\n",
        "# Split into train/val/test (60/20/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25*0.8=0.2\n",
        "\n",
        "# Standardize features (critical for regression). Fit only on training data.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data types to float32, which is common for TensorFlow\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val = X_val.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_val = y_val.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "# Adjusted learning rate and epochs slightly for a potentially larger model\n",
        "learning_rate = 0.005\n",
        "batch_size = 32\n",
        "num_epochs = 200\n",
        "n_features = X_train.shape[1] # Number of features\n",
        "\n",
        "# Define the model using Keras Sequential API for regression\n",
        "# Using slightly larger hidden layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(n_features,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    # For regression, the output layer has 1 neuron and no activation (linear output)\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Use Adam optimizer and MeanSquaredError loss for regression\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['mse']) # Use Mean Squared Error as a metric\n",
        "\n",
        "# Train the model using the data directly\n",
        "# Keras handles the batching internally when you provide numpy arrays\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=1, # Set to 1 to see progress during training\n",
        "                    validation_data=(X_val, y_val))\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_loss, test_mse = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\nTest MSE: {test_mse:.2f}\")\n",
        "\n",
        "# Calculate and print RMSE for regression\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "print(f\"Test RMSE: ${test_rmse:,.2f}\") # Format as USD\n",
        "\n",
        "# Example prediction\n",
        "# Create a sample house data point using the selected features and scale it\n",
        "# Make sure the order of features matches the 'features' list\n",
        "sample_house_data = np.array([[1500, 2003, 7, 1000, 1500, 0, 1000, 400, 100, 1, 70, 8000, 200, 3, 1, 7, 1]])\n",
        "sample_house_scaled = scaler.transform(sample_house_data.astype(np.float32))\n",
        "\n",
        "predicted_price = model.predict(sample_house_scaled)\n",
        "print(f\"Predicted Price for sample house: ${predicted_price[0][0]:,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALuGUgB919hW",
        "outputId": "dddc64b3-3bac-4a25-cdc4-8ee61a168b76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 37558546432.0000 - mse: 37558546432.0000 - val_loss: 35265757184.0000 - val_mse: 35265757184.0000\n",
            "Epoch 2/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 39505211392.0000 - mse: 39505211392.0000 - val_loss: 35102789632.0000 - val_mse: 35102789632.0000\n",
            "Epoch 3/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 39800643584.0000 - mse: 39800643584.0000 - val_loss: 34530549760.0000 - val_mse: 34530549760.0000\n",
            "Epoch 4/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 38116704256.0000 - mse: 38116704256.0000 - val_loss: 33213003776.0000 - val_mse: 33213003776.0000\n",
            "Epoch 5/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 34550554624.0000 - mse: 34550554624.0000 - val_loss: 30827575296.0000 - val_mse: 30827575296.0000\n",
            "Epoch 6/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 34424692736.0000 - mse: 34424692736.0000 - val_loss: 27319504896.0000 - val_mse: 27319504896.0000\n",
            "Epoch 7/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29490393088.0000 - mse: 29490393088.0000 - val_loss: 23040825344.0000 - val_mse: 23040825344.0000\n",
            "Epoch 8/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24529524736.0000 - mse: 24529524736.0000 - val_loss: 19174103040.0000 - val_mse: 19174103040.0000\n",
            "Epoch 9/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17607596032.0000 - mse: 17607596032.0000 - val_loss: 16937845760.0000 - val_mse: 16937845760.0000\n",
            "Epoch 10/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 12783812608.0000 - mse: 12783812608.0000 - val_loss: 16544582656.0000 - val_mse: 16544582656.0000\n",
            "Epoch 11/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10322420736.0000 - mse: 10322420736.0000 - val_loss: 16840376320.0000 - val_mse: 16840376320.0000\n",
            "Epoch 12/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9497065472.0000 - mse: 9497065472.0000 - val_loss: 16788614144.0000 - val_mse: 16788614144.0000\n",
            "Epoch 13/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8044942848.0000 - mse: 8044942848.0000 - val_loss: 16407619584.0000 - val_mse: 16407619584.0000\n",
            "Epoch 14/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6876099072.0000 - mse: 6876099072.0000 - val_loss: 15857104896.0000 - val_mse: 15857104896.0000\n",
            "Epoch 15/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6432920576.0000 - mse: 6432920576.0000 - val_loss: 15011045376.0000 - val_mse: 15011045376.0000\n",
            "Epoch 16/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6296844288.0000 - mse: 6296844288.0000 - val_loss: 13935220736.0000 - val_mse: 13935220736.0000\n",
            "Epoch 17/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6661808640.0000 - mse: 6661808640.0000 - val_loss: 13691352064.0000 - val_mse: 13691352064.0000\n",
            "Epoch 18/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5925600256.0000 - mse: 5925600256.0000 - val_loss: 13472433152.0000 - val_mse: 13472433152.0000\n",
            "Epoch 19/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5742067712.0000 - mse: 5742067712.0000 - val_loss: 13012592640.0000 - val_mse: 13012592640.0000\n",
            "Epoch 20/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5654209024.0000 - mse: 5654209024.0000 - val_loss: 12546631680.0000 - val_mse: 12546631680.0000\n",
            "Epoch 21/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5065136640.0000 - mse: 5065136640.0000 - val_loss: 12209638400.0000 - val_mse: 12209638400.0000\n",
            "Epoch 22/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4494868480.0000 - mse: 4494868480.0000 - val_loss: 11930333184.0000 - val_mse: 11930333184.0000\n",
            "Epoch 23/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5169590784.0000 - mse: 5169590784.0000 - val_loss: 11189740544.0000 - val_mse: 11189740544.0000\n",
            "Epoch 24/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4258505216.0000 - mse: 4258505216.0000 - val_loss: 10906620928.0000 - val_mse: 10906620928.0000\n",
            "Epoch 25/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3939273984.0000 - mse: 3939273984.0000 - val_loss: 10481462272.0000 - val_mse: 10481462272.0000\n",
            "Epoch 26/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3914099200.0000 - mse: 3914099200.0000 - val_loss: 9967698944.0000 - val_mse: 9967698944.0000\n",
            "Epoch 27/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3461332736.0000 - mse: 3461332736.0000 - val_loss: 9856481280.0000 - val_mse: 9856481280.0000\n",
            "Epoch 28/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4183595264.0000 - mse: 4183595264.0000 - val_loss: 9272525824.0000 - val_mse: 9272525824.0000\n",
            "Epoch 29/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3635237888.0000 - mse: 3635237888.0000 - val_loss: 8972057600.0000 - val_mse: 8972057600.0000\n",
            "Epoch 30/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3379123200.0000 - mse: 3379123200.0000 - val_loss: 8620399616.0000 - val_mse: 8620399616.0000\n",
            "Epoch 31/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3890359296.0000 - mse: 3890359296.0000 - val_loss: 8387535872.0000 - val_mse: 8387535872.0000\n",
            "Epoch 32/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2262175744.0000 - mse: 2262175744.0000 - val_loss: 8271598592.0000 - val_mse: 8271598592.0000\n",
            "Epoch 33/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2590665984.0000 - mse: 2590665984.0000 - val_loss: 7308438528.0000 - val_mse: 7308438528.0000\n",
            "Epoch 34/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2169205504.0000 - mse: 2169205504.0000 - val_loss: 7334465536.0000 - val_mse: 7334465536.0000\n",
            "Epoch 35/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2208953344.0000 - mse: 2208953344.0000 - val_loss: 7121521152.0000 - val_mse: 7121521152.0000\n",
            "Epoch 36/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2250687744.0000 - mse: 2250687744.0000 - val_loss: 6855920640.0000 - val_mse: 6855920640.0000\n",
            "Epoch 37/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2060626944.0000 - mse: 2060626944.0000 - val_loss: 6610659840.0000 - val_mse: 6610659840.0000\n",
            "Epoch 38/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2041295744.0000 - mse: 2041295744.0000 - val_loss: 6515420160.0000 - val_mse: 6515420160.0000\n",
            "Epoch 39/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1603569280.0000 - mse: 1603569280.0000 - val_loss: 6274015232.0000 - val_mse: 6274015232.0000\n",
            "Epoch 40/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1664631808.0000 - mse: 1664631808.0000 - val_loss: 6108616704.0000 - val_mse: 6108616704.0000\n",
            "Epoch 41/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1867752960.0000 - mse: 1867752960.0000 - val_loss: 5835254272.0000 - val_mse: 5835254272.0000\n",
            "Epoch 42/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1593198592.0000 - mse: 1593198592.0000 - val_loss: 5768954880.0000 - val_mse: 5768954880.0000\n",
            "Epoch 43/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1423251840.0000 - mse: 1423251840.0000 - val_loss: 5693088768.0000 - val_mse: 5693088768.0000\n",
            "Epoch 44/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1911775872.0000 - mse: 1911775872.0000 - val_loss: 5399209472.0000 - val_mse: 5399209472.0000\n",
            "Epoch 45/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1353685632.0000 - mse: 1353685632.0000 - val_loss: 5376815616.0000 - val_mse: 5376815616.0000\n",
            "Epoch 46/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1838164096.0000 - mse: 1838164096.0000 - val_loss: 5158780928.0000 - val_mse: 5158780928.0000\n",
            "Epoch 47/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1837960448.0000 - mse: 1837960448.0000 - val_loss: 5062608384.0000 - val_mse: 5062608384.0000\n",
            "Epoch 48/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1473770624.0000 - mse: 1473770624.0000 - val_loss: 4926817792.0000 - val_mse: 4926817792.0000\n",
            "Epoch 49/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1334828672.0000 - mse: 1334828672.0000 - val_loss: 4911545344.0000 - val_mse: 4911545344.0000\n",
            "Epoch 50/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1420818816.0000 - mse: 1420818816.0000 - val_loss: 4786071552.0000 - val_mse: 4786071552.0000\n",
            "Epoch 51/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1123135232.0000 - mse: 1123135232.0000 - val_loss: 4780865024.0000 - val_mse: 4780865024.0000\n",
            "Epoch 52/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1847232896.0000 - mse: 1847232896.0000 - val_loss: 4544821760.0000 - val_mse: 4544821760.0000\n",
            "Epoch 53/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2081371136.0000 - mse: 2081371136.0000 - val_loss: 4539184640.0000 - val_mse: 4539184640.0000\n",
            "Epoch 54/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1465260160.0000 - mse: 1465260160.0000 - val_loss: 4459560448.0000 - val_mse: 4459560448.0000\n",
            "Epoch 55/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1711073408.0000 - mse: 1711073408.0000 - val_loss: 4345845248.0000 - val_mse: 4345845248.0000\n",
            "Epoch 56/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 966577024.0000 - mse: 966577024.0000 - val_loss: 4528775168.0000 - val_mse: 4528775168.0000\n",
            "Epoch 57/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1310118016.0000 - mse: 1310118016.0000 - val_loss: 4243436288.0000 - val_mse: 4243436288.0000\n",
            "Epoch 58/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1512449920.0000 - mse: 1512449920.0000 - val_loss: 4285624064.0000 - val_mse: 4285624064.0000\n",
            "Epoch 59/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1079946240.0000 - mse: 1079946240.0000 - val_loss: 4203617792.0000 - val_mse: 4203617792.0000\n",
            "Epoch 60/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1360185856.0000 - mse: 1360185856.0000 - val_loss: 4075849728.0000 - val_mse: 4075849728.0000\n",
            "Epoch 61/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1196845952.0000 - mse: 1196845952.0000 - val_loss: 4115085056.0000 - val_mse: 4115085056.0000\n",
            "Epoch 62/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 965101952.0000 - mse: 965101952.0000 - val_loss: 4129932544.0000 - val_mse: 4129932544.0000\n",
            "Epoch 63/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1104215680.0000 - mse: 1104215680.0000 - val_loss: 3913748992.0000 - val_mse: 3913748992.0000\n",
            "Epoch 64/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1014617664.0000 - mse: 1014617664.0000 - val_loss: 4010216704.0000 - val_mse: 4010216704.0000\n",
            "Epoch 65/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1026230528.0000 - mse: 1026230528.0000 - val_loss: 3826259456.0000 - val_mse: 3826259456.0000\n",
            "Epoch 66/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1096953344.0000 - mse: 1096953344.0000 - val_loss: 3750941184.0000 - val_mse: 3750941184.0000\n",
            "Epoch 67/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1384388224.0000 - mse: 1384388224.0000 - val_loss: 3877080064.0000 - val_mse: 3877080064.0000\n",
            "Epoch 68/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1295790336.0000 - mse: 1295790336.0000 - val_loss: 3827650816.0000 - val_mse: 3827650816.0000\n",
            "Epoch 69/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 880808704.0000 - mse: 880808704.0000 - val_loss: 3888312832.0000 - val_mse: 3888312832.0000\n",
            "Epoch 70/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 844781440.0000 - mse: 844781504.0000 - val_loss: 3775009280.0000 - val_mse: 3775009280.0000\n",
            "Epoch 71/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 807925504.0000 - mse: 807925504.0000 - val_loss: 3690984192.0000 - val_mse: 3690984192.0000\n",
            "Epoch 72/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 981979264.0000 - mse: 981979264.0000 - val_loss: 3785091840.0000 - val_mse: 3785091840.0000\n",
            "Epoch 73/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1498910848.0000 - mse: 1498910848.0000 - val_loss: 3507955712.0000 - val_mse: 3507955712.0000\n",
            "Epoch 74/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 975026816.0000 - mse: 975026816.0000 - val_loss: 3630915840.0000 - val_mse: 3630915840.0000\n",
            "Epoch 75/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1065930880.0000 - mse: 1065930880.0000 - val_loss: 3629651456.0000 - val_mse: 3629651456.0000\n",
            "Epoch 76/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 723575872.0000 - mse: 723575872.0000 - val_loss: 3682732544.0000 - val_mse: 3682732544.0000\n",
            "Epoch 77/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1065220992.0000 - mse: 1065220992.0000 - val_loss: 3483212544.0000 - val_mse: 3483212544.0000\n",
            "Epoch 78/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 749161408.0000 - mse: 749161408.0000 - val_loss: 3664429312.0000 - val_mse: 3664429312.0000\n",
            "Epoch 79/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1262950784.0000 - mse: 1262950784.0000 - val_loss: 3288781312.0000 - val_mse: 3288781312.0000\n",
            "Epoch 80/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1047010752.0000 - mse: 1047010752.0000 - val_loss: 3439060224.0000 - val_mse: 3439060224.0000\n",
            "Epoch 81/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1038620224.0000 - mse: 1038620224.0000 - val_loss: 3447651328.0000 - val_mse: 3447651328.0000\n",
            "Epoch 82/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 956972416.0000 - mse: 956972416.0000 - val_loss: 3412199168.0000 - val_mse: 3412199168.0000\n",
            "Epoch 83/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1472639488.0000 - mse: 1472639488.0000 - val_loss: 3419233536.0000 - val_mse: 3419233536.0000\n",
            "Epoch 84/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 944625664.0000 - mse: 944625664.0000 - val_loss: 3369463296.0000 - val_mse: 3369463296.0000\n",
            "Epoch 85/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 991088000.0000 - mse: 991088000.0000 - val_loss: 3368618752.0000 - val_mse: 3368618752.0000\n",
            "Epoch 86/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 870601216.0000 - mse: 870601216.0000 - val_loss: 3408957440.0000 - val_mse: 3408957440.0000\n",
            "Epoch 87/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 725717312.0000 - mse: 725717312.0000 - val_loss: 3482981632.0000 - val_mse: 3482981632.0000\n",
            "Epoch 88/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1527958400.0000 - mse: 1527958400.0000 - val_loss: 3269050112.0000 - val_mse: 3269050112.0000\n",
            "Epoch 89/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 995638336.0000 - mse: 995638336.0000 - val_loss: 3490624512.0000 - val_mse: 3490624512.0000\n",
            "Epoch 90/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 955307264.0000 - mse: 955307264.0000 - val_loss: 3486072320.0000 - val_mse: 3486072320.0000\n",
            "Epoch 91/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 935682176.0000 - mse: 935682176.0000 - val_loss: 3363975424.0000 - val_mse: 3363975424.0000\n",
            "Epoch 92/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 767083968.0000 - mse: 767083968.0000 - val_loss: 3226220544.0000 - val_mse: 3226220544.0000\n",
            "Epoch 93/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 923817792.0000 - mse: 923817792.0000 - val_loss: 3412002048.0000 - val_mse: 3412002048.0000\n",
            "Epoch 94/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 860856000.0000 - mse: 860856000.0000 - val_loss: 3269123328.0000 - val_mse: 3269123328.0000\n",
            "Epoch 95/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 942936128.0000 - mse: 942936128.0000 - val_loss: 3321818368.0000 - val_mse: 3321818368.0000\n",
            "Epoch 96/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1177539328.0000 - mse: 1177539328.0000 - val_loss: 3289715712.0000 - val_mse: 3289715712.0000\n",
            "Epoch 97/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 874846336.0000 - mse: 874846336.0000 - val_loss: 3378793472.0000 - val_mse: 3378793472.0000\n",
            "Epoch 98/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 865073792.0000 - mse: 865073792.0000 - val_loss: 3243838976.0000 - val_mse: 3243838976.0000\n",
            "Epoch 99/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 952980352.0000 - mse: 952980352.0000 - val_loss: 3207564032.0000 - val_mse: 3207564032.0000\n",
            "Epoch 100/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 837448896.0000 - mse: 837448896.0000 - val_loss: 3306692608.0000 - val_mse: 3306692608.0000\n",
            "Epoch 101/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 930337216.0000 - mse: 930337216.0000 - val_loss: 3214895360.0000 - val_mse: 3214895360.0000\n",
            "Epoch 102/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 873937024.0000 - mse: 873937024.0000 - val_loss: 3275473920.0000 - val_mse: 3275473920.0000\n",
            "Epoch 103/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1040894400.0000 - mse: 1040894400.0000 - val_loss: 3206120960.0000 - val_mse: 3206120960.0000\n",
            "Epoch 104/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1100168576.0000 - mse: 1100168576.0000 - val_loss: 3254647808.0000 - val_mse: 3254647808.0000\n",
            "Epoch 105/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 814403584.0000 - mse: 814403584.0000 - val_loss: 3239143168.0000 - val_mse: 3239143168.0000\n",
            "Epoch 106/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1636929280.0000 - mse: 1636929280.0000 - val_loss: 3312866304.0000 - val_mse: 3312866304.0000\n",
            "Epoch 107/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 881470528.0000 - mse: 881470528.0000 - val_loss: 3236184064.0000 - val_mse: 3236184064.0000\n",
            "Epoch 108/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 835864128.0000 - mse: 835864128.0000 - val_loss: 3236602112.0000 - val_mse: 3236602112.0000\n",
            "Epoch 109/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1247995904.0000 - mse: 1247995904.0000 - val_loss: 3203718400.0000 - val_mse: 3203718400.0000\n",
            "Epoch 110/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 861669696.0000 - mse: 861669696.0000 - val_loss: 3239594752.0000 - val_mse: 3239594752.0000\n",
            "Epoch 111/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 804300160.0000 - mse: 804300160.0000 - val_loss: 3239904256.0000 - val_mse: 3239904256.0000\n",
            "Epoch 112/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 934949696.0000 - mse: 934949696.0000 - val_loss: 3165792512.0000 - val_mse: 3165792512.0000\n",
            "Epoch 113/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 755621696.0000 - mse: 755621696.0000 - val_loss: 3294642688.0000 - val_mse: 3294642688.0000\n",
            "Epoch 114/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 726646528.0000 - mse: 726646528.0000 - val_loss: 3167142144.0000 - val_mse: 3167142144.0000\n",
            "Epoch 115/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 927480896.0000 - mse: 927480896.0000 - val_loss: 3186714880.0000 - val_mse: 3186714880.0000\n",
            "Epoch 116/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 885547072.0000 - mse: 885547072.0000 - val_loss: 3174079488.0000 - val_mse: 3174079488.0000\n",
            "Epoch 117/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 754767232.0000 - mse: 754767232.0000 - val_loss: 3163035136.0000 - val_mse: 3163035136.0000\n",
            "Epoch 118/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 778290752.0000 - mse: 778290752.0000 - val_loss: 3286758912.0000 - val_mse: 3286758912.0000\n",
            "Epoch 119/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 803155904.0000 - mse: 803155904.0000 - val_loss: 3099661056.0000 - val_mse: 3099661056.0000\n",
            "Epoch 120/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1283466624.0000 - mse: 1283466624.0000 - val_loss: 3114365952.0000 - val_mse: 3114365952.0000\n",
            "Epoch 121/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 741543424.0000 - mse: 741543424.0000 - val_loss: 3331453440.0000 - val_mse: 3331453440.0000\n",
            "Epoch 122/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 878013440.0000 - mse: 878013440.0000 - val_loss: 2952662784.0000 - val_mse: 2952662784.0000\n",
            "Epoch 123/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1144317184.0000 - mse: 1144317184.0000 - val_loss: 3044811008.0000 - val_mse: 3044811008.0000\n",
            "Epoch 124/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1419139328.0000 - mse: 1419139328.0000 - val_loss: 3124012544.0000 - val_mse: 3124012544.0000\n",
            "Epoch 125/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1076978304.0000 - mse: 1076978304.0000 - val_loss: 3074202112.0000 - val_mse: 3074202112.0000\n",
            "Epoch 126/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 701723392.0000 - mse: 701723392.0000 - val_loss: 3118039552.0000 - val_mse: 3118039552.0000\n",
            "Epoch 127/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1016509056.0000 - mse: 1016509056.0000 - val_loss: 3075796224.0000 - val_mse: 3075796224.0000\n",
            "Epoch 128/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 951477888.0000 - mse: 951477888.0000 - val_loss: 3122803200.0000 - val_mse: 3122803200.0000\n",
            "Epoch 129/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 665852160.0000 - mse: 665852160.0000 - val_loss: 3195021312.0000 - val_mse: 3195021312.0000\n",
            "Epoch 130/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 838356992.0000 - mse: 838356992.0000 - val_loss: 3146301696.0000 - val_mse: 3146301696.0000\n",
            "Epoch 131/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 714899520.0000 - mse: 714899520.0000 - val_loss: 3029844224.0000 - val_mse: 3029844224.0000\n",
            "Epoch 132/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 836129920.0000 - mse: 836129920.0000 - val_loss: 3107340288.0000 - val_mse: 3107340288.0000\n",
            "Epoch 133/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 829086400.0000 - mse: 829086400.0000 - val_loss: 3147144960.0000 - val_mse: 3147144960.0000\n",
            "Epoch 134/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 830577536.0000 - mse: 830577536.0000 - val_loss: 3149829120.0000 - val_mse: 3149829120.0000\n",
            "Epoch 135/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 908277184.0000 - mse: 908277184.0000 - val_loss: 3021359616.0000 - val_mse: 3021359616.0000\n",
            "Epoch 136/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 815353792.0000 - mse: 815353792.0000 - val_loss: 3173710592.0000 - val_mse: 3173710592.0000\n",
            "Epoch 137/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 811476416.0000 - mse: 811476416.0000 - val_loss: 3188563200.0000 - val_mse: 3188563200.0000\n",
            "Epoch 138/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1219398144.0000 - mse: 1219398144.0000 - val_loss: 3125446144.0000 - val_mse: 3125446144.0000\n",
            "Epoch 139/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 990507136.0000 - mse: 990507136.0000 - val_loss: 3124670208.0000 - val_mse: 3124670208.0000\n",
            "Epoch 140/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1022450240.0000 - mse: 1022450240.0000 - val_loss: 3199662592.0000 - val_mse: 3199662592.0000\n",
            "Epoch 141/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 905747008.0000 - mse: 905747008.0000 - val_loss: 3108788224.0000 - val_mse: 3108788224.0000\n",
            "Epoch 142/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 948145728.0000 - mse: 948145728.0000 - val_loss: 3042581760.0000 - val_mse: 3042581760.0000\n",
            "Epoch 143/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 845609792.0000 - mse: 845609792.0000 - val_loss: 3138910976.0000 - val_mse: 3138910976.0000\n",
            "Epoch 144/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 926443712.0000 - mse: 926443712.0000 - val_loss: 3114204672.0000 - val_mse: 3114204672.0000\n",
            "Epoch 145/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1495738752.0000 - mse: 1495738752.0000 - val_loss: 3093509888.0000 - val_mse: 3093509888.0000\n",
            "Epoch 146/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1040192128.0000 - mse: 1040192128.0000 - val_loss: 3128831488.0000 - val_mse: 3128831488.0000\n",
            "Epoch 147/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 835682112.0000 - mse: 835682112.0000 - val_loss: 3139876608.0000 - val_mse: 3139876608.0000\n",
            "Epoch 148/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 917305344.0000 - mse: 917305344.0000 - val_loss: 3056904448.0000 - val_mse: 3056904448.0000\n",
            "Epoch 149/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 716315584.0000 - mse: 716315584.0000 - val_loss: 3121736448.0000 - val_mse: 3121736448.0000\n",
            "Epoch 150/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 676997184.0000 - mse: 676997184.0000 - val_loss: 3119930368.0000 - val_mse: 3119930368.0000\n",
            "Epoch 151/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 841904640.0000 - mse: 841904640.0000 - val_loss: 3062530304.0000 - val_mse: 3062530304.0000\n",
            "Epoch 152/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 864968512.0000 - mse: 864968512.0000 - val_loss: 3030499584.0000 - val_mse: 3030499584.0000\n",
            "Epoch 153/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 761621696.0000 - mse: 761621696.0000 - val_loss: 3074790656.0000 - val_mse: 3074790656.0000\n",
            "Epoch 154/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 873239616.0000 - mse: 873239616.0000 - val_loss: 3009584896.0000 - val_mse: 3009584896.0000\n",
            "Epoch 155/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1021072064.0000 - mse: 1021072064.0000 - val_loss: 3174062080.0000 - val_mse: 3174062080.0000\n",
            "Epoch 156/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 868158784.0000 - mse: 868158784.0000 - val_loss: 3076696064.0000 - val_mse: 3076696064.0000\n",
            "Epoch 157/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 771426816.0000 - mse: 771426816.0000 - val_loss: 3123523328.0000 - val_mse: 3123523328.0000\n",
            "Epoch 158/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 775253632.0000 - mse: 775253632.0000 - val_loss: 3047439616.0000 - val_mse: 3047439616.0000\n",
            "Epoch 159/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 727294016.0000 - mse: 727294016.0000 - val_loss: 3057713920.0000 - val_mse: 3057713920.0000\n",
            "Epoch 160/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 714837568.0000 - mse: 714837568.0000 - val_loss: 3136678912.0000 - val_mse: 3136678912.0000\n",
            "Epoch 161/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1396223360.0000 - mse: 1396223360.0000 - val_loss: 2956788224.0000 - val_mse: 2956788224.0000\n",
            "Epoch 162/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 816675072.0000 - mse: 816675072.0000 - val_loss: 3085110016.0000 - val_mse: 3085110016.0000\n",
            "Epoch 163/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1006549376.0000 - mse: 1006549376.0000 - val_loss: 3109871360.0000 - val_mse: 3109871360.0000\n",
            "Epoch 164/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 698721280.0000 - mse: 698721280.0000 - val_loss: 3058917376.0000 - val_mse: 3058917376.0000\n",
            "Epoch 165/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 720417216.0000 - mse: 720417216.0000 - val_loss: 3026559488.0000 - val_mse: 3026559488.0000\n",
            "Epoch 166/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 819625088.0000 - mse: 819625088.0000 - val_loss: 3147772160.0000 - val_mse: 3147772160.0000\n",
            "Epoch 167/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 906812736.0000 - mse: 906812736.0000 - val_loss: 2977712128.0000 - val_mse: 2977712128.0000\n",
            "Epoch 168/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 871734144.0000 - mse: 871734144.0000 - val_loss: 3109732352.0000 - val_mse: 3109732352.0000\n",
            "Epoch 169/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 949130048.0000 - mse: 949130048.0000 - val_loss: 3047308288.0000 - val_mse: 3047308288.0000\n",
            "Epoch 170/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 873074304.0000 - mse: 873074304.0000 - val_loss: 3053605888.0000 - val_mse: 3053605888.0000\n",
            "Epoch 171/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1081310208.0000 - mse: 1081310208.0000 - val_loss: 2995074048.0000 - val_mse: 2995074048.0000\n",
            "Epoch 172/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1136027904.0000 - mse: 1136027904.0000 - val_loss: 3103722240.0000 - val_mse: 3103722240.0000\n",
            "Epoch 173/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1448937216.0000 - mse: 1448937216.0000 - val_loss: 3150739712.0000 - val_mse: 3150739712.0000\n",
            "Epoch 174/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 708745024.0000 - mse: 708745024.0000 - val_loss: 3061126144.0000 - val_mse: 3061126144.0000\n",
            "Epoch 175/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 979409344.0000 - mse: 979409344.0000 - val_loss: 3059743232.0000 - val_mse: 3059743232.0000\n",
            "Epoch 176/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 766898944.0000 - mse: 766898944.0000 - val_loss: 3129420544.0000 - val_mse: 3129420544.0000\n",
            "Epoch 177/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 990947328.0000 - mse: 990947328.0000 - val_loss: 3040816896.0000 - val_mse: 3040816896.0000\n",
            "Epoch 178/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1029780544.0000 - mse: 1029780544.0000 - val_loss: 3086995968.0000 - val_mse: 3086995968.0000\n",
            "Epoch 179/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1176661504.0000 - mse: 1176661504.0000 - val_loss: 3057642496.0000 - val_mse: 3057642496.0000\n",
            "Epoch 180/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1352137728.0000 - mse: 1352137728.0000 - val_loss: 3128659200.0000 - val_mse: 3128659200.0000\n",
            "Epoch 181/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1233115264.0000 - mse: 1233115264.0000 - val_loss: 3057041408.0000 - val_mse: 3057041408.0000\n",
            "Epoch 182/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 736271488.0000 - mse: 736271488.0000 - val_loss: 3098620160.0000 - val_mse: 3098620160.0000\n",
            "Epoch 183/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1642593280.0000 - mse: 1642593280.0000 - val_loss: 3084875264.0000 - val_mse: 3084875264.0000\n",
            "Epoch 184/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 801052672.0000 - mse: 801052672.0000 - val_loss: 3032418560.0000 - val_mse: 3032418560.0000\n",
            "Epoch 185/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 681860608.0000 - mse: 681860608.0000 - val_loss: 3124614400.0000 - val_mse: 3124614400.0000\n",
            "Epoch 186/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 883373056.0000 - mse: 883373056.0000 - val_loss: 2991022336.0000 - val_mse: 2991022336.0000\n",
            "Epoch 187/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1369866240.0000 - mse: 1369866240.0000 - val_loss: 3028600320.0000 - val_mse: 3028600320.0000\n",
            "Epoch 188/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1205536384.0000 - mse: 1205536384.0000 - val_loss: 3210130432.0000 - val_mse: 3210130432.0000\n",
            "Epoch 189/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 812041600.0000 - mse: 812041600.0000 - val_loss: 3042884352.0000 - val_mse: 3042884352.0000\n",
            "Epoch 190/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1215443456.0000 - mse: 1215443456.0000 - val_loss: 3054109440.0000 - val_mse: 3054109440.0000\n",
            "Epoch 191/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1220751104.0000 - mse: 1220751104.0000 - val_loss: 3105016064.0000 - val_mse: 3105016064.0000\n",
            "Epoch 192/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 918276992.0000 - mse: 918276992.0000 - val_loss: 3118383360.0000 - val_mse: 3118383360.0000\n",
            "Epoch 193/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1114456576.0000 - mse: 1114456576.0000 - val_loss: 3059233536.0000 - val_mse: 3059233536.0000\n",
            "Epoch 194/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 762250432.0000 - mse: 762250432.0000 - val_loss: 3070390272.0000 - val_mse: 3070390272.0000\n",
            "Epoch 195/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 924423680.0000 - mse: 924423680.0000 - val_loss: 3041095168.0000 - val_mse: 3041095168.0000\n",
            "Epoch 196/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 951338816.0000 - mse: 951338816.0000 - val_loss: 3042698240.0000 - val_mse: 3042698240.0000\n",
            "Epoch 197/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 725673664.0000 - mse: 725673664.0000 - val_loss: 3125670656.0000 - val_mse: 3125670656.0000\n",
            "Epoch 198/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 818604416.0000 - mse: 818604416.0000 - val_loss: 2971757824.0000 - val_mse: 2971757824.0000\n",
            "Epoch 199/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 716928768.0000 - mse: 716928768.0000 - val_loss: 3104856576.0000 - val_mse: 3104856576.0000\n",
            "Epoch 200/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 847919616.0000 - mse: 847919616.0000 - val_loss: 3028954880.0000 - val_mse: 3028954880.0000\n",
            "Training finished.\n",
            "Evaluating on test set...\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1430914432.0000 - mse: 1430914432.0000 \n",
            "\n",
            "Test MSE: 1405460224.00\n",
            "Test RMSE: $37,489.47\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "Predicted Price for sample house: $208,181.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Question 5] Create an MNIST model"
      ],
      "metadata": {
        "id": "gLyf1NdN45gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load MNIST data using tf.keras.datasets\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Reshape images to add a channel dimension (for compatibility with some layers, though not strictly needed for Dense layers)\n",
        "# Flatten the images for the Dense layers\n",
        "x_train = x_train.reshape(x_train.shape[0], 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Split off a validation set from the training data\n",
        "# Using a simple split here, similar to previous examples\n",
        "# In a real scenario, you might use train_test_split from sklearn or tf.data.Dataset tools\n",
        "validation_split = 0.1 # 10% of training data for validation\n",
        "split_index = int(x_train.shape[0] * (1 - validation_split))\n",
        "\n",
        "x_val, y_val = x_train[split_index:], y_train[split_index:]\n",
        "x_train, y_train = x_train[:split_index], y_train[:split_index]\n",
        "\n",
        "\n",
        "# Hyperparameters (adjusted slightly from the original to match Keras style better)\n",
        "learning_rate = 0.001 # Using a common starting learning rate\n",
        "batch_size = 32     # Keras typically defaults to 32\n",
        "num_epochs = 10\n",
        "n_input = x_train.shape[1]    # 28x28 flattened = 784\n",
        "n_hidden1 = 512   # Hidden layer size\n",
        "n_hidden2 = 256   # Add another hidden layer for potentially better performance\n",
        "n_classes = num_classes  # Digits 0-9\n",
        "\n",
        "# Define the model using Keras Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu', input_shape=(n_input,)),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    # Output layer for multi-class classification\n",
        "    # Use softmax activation if not using from_logits=True in loss\n",
        "    # Using linear output here and from_logits=True in loss is also valid.\n",
        "    tf.keras.layers.Dense(n_classes)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# Use Adam optimizer and CategoricalCrossentropy loss for multi-class classification\n",
        "# from_logits=True means the output layer does not have a softmax activation\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data directly\n",
        "# Keras handles the batching internally when you provide numpy arrays\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=1, # Set to 1 to see progress during training\n",
        "                    validation_data=(x_val, y_val))\n",
        "print(\"Training finished.\")\n",
        "\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Example prediction (predict on a single test image)\n",
        "# Reshape the test image to (1, 784) for prediction\n",
        "sample_image = x_test[0].reshape(1, n_input)\n",
        "predicted_probabilities = model.predict(sample_image)\n",
        "# Get the class with the highest probability\n",
        "predicted_class = np.argmax(predicted_probabilities)\n",
        "\n",
        "# Find the true label (which is one-hot encoded, so find the index of 1)\n",
        "true_class = np.argmax(y_test[0])\n",
        "\n",
        "print(f\"\\nSample Image Prediction:\")\n",
        "print(f\"  Predicted Probabilities (logits): {predicted_probabilities[0]}\")\n",
        "# Apply softmax to logits to get actual probabilities if desired\n",
        "# predicted_probs_softmax = tf.nn.softmax(predicted_probabilities).numpy()[0]\n",
        "# print(f\"  Predicted Probabilities (softmax): {predicted_probs_softmax}\")\n",
        "print(f\"  Predicted Class: {predicted_class}\")\n",
        "print(f\"  True Class: {true_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChoUvFcD3YGW",
        "outputId": "7d273c14-a3b2-477b-af50-62eaa67543af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9004 - loss: 0.3324 - val_accuracy: 0.9750 - val_loss: 0.0842\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.9748 - loss: 0.0816 - val_accuracy: 0.9803 - val_loss: 0.0666\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9830 - loss: 0.0536 - val_accuracy: 0.9738 - val_loss: 0.0963\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.9882 - loss: 0.0357 - val_accuracy: 0.9768 - val_loss: 0.0870\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - accuracy: 0.9908 - loss: 0.0278 - val_accuracy: 0.9795 - val_loss: 0.0701\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.9924 - loss: 0.0240 - val_accuracy: 0.9807 - val_loss: 0.0739\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9953 - loss: 0.0160 - val_accuracy: 0.9822 - val_loss: 0.0847\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9943 - loss: 0.0178 - val_accuracy: 0.9823 - val_loss: 0.0835\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9947 - loss: 0.0182 - val_accuracy: 0.9803 - val_loss: 0.0960\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.9956 - loss: 0.0135 - val_accuracy: 0.9820 - val_loss: 0.0930\n",
            "Training finished.\n",
            "Evaluating on test set...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9774 - loss: 0.1156\n",
            "\n",
            "Test Accuracy: 0.9812\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "\n",
            "Sample Image Prediction:\n",
            "  Predicted Probabilities (logits): [-16.632267   -7.049389   -2.817748   -3.8967566 -18.627626  -12.408664\n",
            " -27.301329   20.60875   -11.434559    4.169455 ]\n",
            "  Predicted Class: 7\n",
            "  True Class: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mhm9dH-X6Q1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}