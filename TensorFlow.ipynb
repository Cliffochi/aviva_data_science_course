{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Vl2JcOnfp0Z0H9Nbu3xQCZT0TDNdTwvQ",
      "authorship_tag": "ABX9TyNJNGd8AjNRKPztTzKKMBIf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cliffochi/aviva_data_science_course/blob/main/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TensorFlow\n",
        "\n",
        "###[Question 1] Looking back at Scratch"
      ],
      "metadata": {
        "id": "TerXbwbfnrQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking back at implementing deep learning from scratch, here are the key components that were needed:  \n",
        "\n",
        "### **1. Model Architecture**  \n",
        "- **Neural Network Layers**: Defining the structure (e.g., Dense, Conv2D, LSTM).  \n",
        "- **Weight Initialization**: Setting initial values for weights (e.g., random, Xavier, He).  \n",
        "- **Bias Initialization**: Initializing bias terms.  \n",
        "\n",
        "### **2. Forward Propagation**  \n",
        "- **Input Handling**: Passing input data through the network.  \n",
        "- **Activation Functions**: Applying ReLU, Sigmoid, Tanh, etc.  \n",
        "- **Loss Calculation**: Computing loss (e.g., Cross-Entropy, MSE).  \n",
        "\n",
        "### **3. Backward Propagation (Gradient Calculation)**  \n",
        "- **Gradient Computation**: Calculating gradients using chain rule.  \n",
        "- **Loss Gradient**: Deriving gradients w.r.t. loss.  \n",
        "- **Weight & Bias Updates**: Adjusting parameters using gradients.  \n",
        "\n",
        "### **4. Optimization**  \n",
        "- **Optimizer Implementation**: Updating weights (e.g., SGD, Adam, RMSprop).  \n",
        "- **Learning Rate**: Managing step size for updates.  \n",
        "\n",
        "### **5. Training Loop**  \n",
        "- **Epoch Loop**: Iterating over the entire dataset.  \n",
        "- **Batch Processing**: Splitting data into mini-batches.  \n",
        "- **Validation**: Monitoring performance on validation data.  \n",
        "\n",
        "### **6. Evaluation**  \n",
        "- **Accuracy/Loss Metrics**: Measuring model performance.  \n",
        "- **Prediction**: Running inference on test data.  \n",
        "\n",
        "### **7. Data Handling**  \n",
        "- **Data Loading**: Reading input data (e.g., CSV, images).  \n",
        "- **Preprocessing**: Normalization, reshaping, one-hot encoding.  \n",
        "- **Batching**: Creating mini-batches for training.  \n",
        "\n",
        "### **8. Debugging & Monitoring**  \n",
        "- **Gradient Checking**: Ensuring correct backpropagation.  \n",
        "- **Logging**: Tracking loss/accuracy over epochs.  \n",
        "\n",
        "---  \n",
        "### **How Frameworks (Like TensorFlow) Implement These**  \n",
        "1. **Automatic Differentiation** → No manual gradient computation (uses `tf.GradientTape`).  \n",
        "2. **Predefined Layers** → `tf.keras.layers` handles weight initialization and forward pass.  \n",
        "3. **Built-in Optimizers** → `tf.optimizers` (SGD, Adam, etc.) manage updates.  \n",
        "4. **Loss Functions** → `tf.losses` provides common loss computations.  \n",
        "5. **Training Loop Abstraction** → `model.fit()` automates epochs/batches.  \n"
      ],
      "metadata": {
        "id": "ZPymLcO3nzml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Question 2] Considering compatibility between Scratch and TensorFlow"
      ],
      "metadata": {
        "id": "-By6cOGbh0mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Iris.csv\")\n",
        "\n",
        "# Filter the DataFrame by specific conditions\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Convert labels to numeric\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size=10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.intp)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item * self.batch_size\n",
        "        p1 = item * self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter * self.batch_size\n",
        "        p1 = self._counter * self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(n_input,)),\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Create training dataset using tf.data\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: get_mini_batch_train,\n",
        "    output_types=(tf.float64, tf.int64),\n",
        "    output_shapes=((None, n_input), (None, n_classes))\n",
        ").repeat()\n",
        "\n",
        "steps_per_epoch = len(get_mini_batch_train)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"test_acc : {:.3f}\".format(test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2_Quh0p-34s",
        "outputId": "7176b436-5521-4ec3-a217-35d97f69597c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc : 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of how the **\"things needed to implement deep learning\"** from scratch map to TensorFlow’s implementation in the sample code.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Model Architecture**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Manually define layers (e.g., `DenseLayer` class). | Layers are defined via `tf.Variable` for weights/biases and `tf.matmul` + `tf.add`. |\n",
        "| Explicit weight initialization (e.g., He initialization). | Uses `tf.random_normal` for initialization. |\n",
        "| Hand-coded forward pass (e.g., `forward()` method). | Forward pass is a sequence of `tf.matmul`, `tf.add`, and `tf.nn.relu`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "# TensorFlow's manual layer definition\n",
        "layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "layer_1 = tf.nn.relu(layer_1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Forward & Backward Propagation**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Manual gradient calculations (chain rule). | **Automatic differentiation** via `tf.GradientTape` (not shown here, but `train_op` handles it). |\n",
        "| Hand-written loss computation (e.g., cross-entropy). | Uses `tf.nn.sigmoid_cross_entropy_with_logits`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "# Loss and gradients handled automatically\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)  # Backpropagation happens here\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Training Loop**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Custom epoch/batch loops (e.g., `for epoch in epochs`). | Same epoch loop, but batching is abstracted via `GetMiniBatch` class. |\n",
        "| Manual parameter updates (e.g., `weights -= lr * gradients`). | Optimizer (`AdamOptimizer`) handles updates via `train_op`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
        "        sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})  # Updates weights\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Data Handling**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Manual data splitting/shuffling. | Uses `sklearn.model_selection.train_test_split` and custom `GetMiniBatch` iterator. |\n",
        "| One-hot encoding by hand. | Labels converted to `0`/`1` manually. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "# Manual batching (similar to scratch)\n",
        "class GetMiniBatch:\n",
        "    def __next__(self):\n",
        "        return self.X[p0:p1], self.y[p0:p1]  # Returns mini-batch\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Evaluation**  \n",
        "| **Scratch** | **TensorFlow (Low-Level)** |\n",
        "|-------------|---------------------------|\n",
        "| Custom accuracy/loss calculations. | Uses `tf.reduce_mean` and `tf.cast` for metrics. |\n",
        "| Manual validation/testing loops. | Evaluated in-session via `sess.run`. |\n",
        "\n",
        "**Code Example:**  \n",
        "```python\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**  \n",
        "1. **TensorFlow Automates Gradients**: No need for manual backpropagation (handled by `optimizer.minimize()`).  \n",
        "2. **Low-Level Still Requires Manual Setup**: Layers, batching, and training loops are explicit (unlike `tf.keras`).  \n",
        "3. **Placeholders Feed Data**: `tf.placeholder` acts as an input pipeline (replaced by `tf.data` in TF 2.x).  \n",
        "\n",
        "---\n",
        "### **Why This Matters**  \n",
        "Understanding this mapping helps:  \n",
        "- Transition from scratch to frameworks.  \n",
        "- Debug low-level TensorFlow code.  \n",
        "- Appreciate high-level APIs (like `tf.keras`) that abstract these steps.  "
      ],
      "metadata": {
        "id": "drxStzOdh-kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Application to other datasets\n",
        "###[Problem 3] Create a model for Iris using all three objective variables"
      ],
      "metadata": {
        "id": "iYYYQ4vkvrEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Iris (3-Class Classification)\n",
        "Key Changes from Binary to Multi-Class:\n",
        "\n",
        "- Labels: One-hot encode Species (3 classes: setosa, versicolor, virginica).\n",
        "\n",
        "- Loss Function: Use tf.nn.softmax_cross_entropy_with_logits instead of sigmoid.\n",
        "\n",
        "- Output Layer: 3 neurons (one per class) with linear activation (logits).\n",
        "\n",
        "- Accuracy: Compare predicted class (tf.argmax) with true class."
      ],
      "metadata": {
        "id": "BiUp-Jjsxn7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load Iris dataset (all 3 classes)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Iris.csv\")\n",
        "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
        "y = pd.get_dummies(df['Species']).values  # One-hot encoding\n",
        "\n",
        "# Split into train/val/test (60/20/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "# Convert data types to match TensorFlow defaults (float32)\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val = X_val.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_val = y_val.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_input = X_train.shape[1]  # Number of features\n",
        "n_classes = 3  # 3 classes in Iris dataset\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "\n",
        "# Define the model using Keras Sequential API with explicit Input layer\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(n_input,)),  # Fix for the input_shape warning\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes)  # No softmax, using from_logits=True\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=0,  # Change to 1 to view training progress\n",
        "                    validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOsCPSO4iVLY",
        "outputId": "653e42e3-f40a-4225-c269-80685b6da57b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###House Prices (Regression)\n",
        "Key Changes for Regression:\n",
        "\n",
        "- Labels: Continuous values (no one-hot encoding).\n",
        "\n",
        "- Loss Function: Use Mean Squared Error (tf.losses.mean_squared_error).\n",
        "\n",
        "- Output Layer: 1 neuron with linear activation.\n",
        "\n",
        "- Metrics: Track MSE or RMSE instead of accuracy."
      ],
      "metadata": {
        "id": "pSz2zuF6yMsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load House Prices dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n",
        "\n",
        "# Select features and target\n",
        "X = df[['GrLivArea', 'YearBuilt']].values\n",
        "y = df['SalePrice'].values.reshape(-1, 1)\n",
        "\n",
        "# Convert data types to float32 for TensorFlow\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "# Split data: 60% train, 20% val, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_output = 1\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "\n",
        "# Define the model with an explicit Input layer\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(n_input,)),  # Fixes the input_shape warning\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_output)  # Linear output for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=0,\n",
        "                    validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test MSE: {test_mse:.2f}\")\n",
        "\n",
        "# Calculate RMSE\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "print(f\"Test RMSE: {test_rmse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc9CLXmi0yqp",
        "outputId": "c0b28bb4-70b4-4fa6-d93f-5f4e742ff49f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 3856587008.00\n",
            "Test RMSE: 62101.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Question 4] Create a model for House Prices"
      ],
      "metadata": {
        "id": "3KCAst_01PeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load dataset (ensure 'train.csv' is in your directory or drive)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/train.csv\")\n",
        "\n",
        "# Select features and target\n",
        "features = ['GrLivArea', 'YearBuilt', 'OverallQual', 'TotalBsmtSF',\n",
        "            '1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'GarageArea', 'WoodDeckSF',\n",
        "            'Fireplaces', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BedroomAbvGr',\n",
        "            'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces']\n",
        "\n",
        "# Drop rows with missing values in the selected features\n",
        "df = df.dropna(subset=features + ['SalePrice'])\n",
        "\n",
        "X = df[features].values\n",
        "y = df['SalePrice'].values.reshape(-1, 1)\n",
        "\n",
        "# Split into train/val/test sets (60% train, 20% val, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25*0.8 = 0.2\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to float32 for TensorFlow\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val = X_val.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_val = y_val.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.005\n",
        "batch_size = 32\n",
        "num_epochs = 200\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "# Define the model using Input() to suppress warning\n",
        "model = Sequential([\n",
        "    Input(shape=(n_features,)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)  # Linear activation for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_val, y_val))\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_loss, test_mse = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\nTest MSE: {test_mse:.2f}\")\n",
        "\n",
        "# Compute RMSE\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "print(f\"Test RMSE: ${test_rmse:,.2f}\")\n",
        "\n",
        "# Example prediction\n",
        "sample_house_data = np.array([[1500, 2003, 7, 1000, 1500, 0, 1000, 400, 100,\n",
        "                               1, 70, 8000, 200, 3, 1, 7, 1]])\n",
        "sample_house_scaled = scaler.transform(sample_house_data.astype(np.float32))\n",
        "\n",
        "predicted_price = model.predict(sample_house_scaled)\n",
        "print(f\"Predicted Price for sample house: ${predicted_price[0][0]:,.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALuGUgB919hW",
        "outputId": "85896fd2-6fe0-4060-9391-9a641b80cb5a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 39097364480.0000 - mse: 39097364480.0000 - val_loss: 35268038656.0000 - val_mse: 35268038656.0000\n",
            "Epoch 2/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 40433647616.0000 - mse: 40433647616.0000 - val_loss: 35118989312.0000 - val_mse: 35118989312.0000\n",
            "Epoch 3/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 37705216000.0000 - mse: 37705216000.0000 - val_loss: 34625261568.0000 - val_mse: 34625261568.0000\n",
            "Epoch 4/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 40683941888.0000 - mse: 40683941888.0000 - val_loss: 33418534912.0000 - val_mse: 33418534912.0000\n",
            "Epoch 5/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 35884494848.0000 - mse: 35884494848.0000 - val_loss: 31246186496.0000 - val_mse: 31246186496.0000\n",
            "Epoch 6/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 34441318400.0000 - mse: 34441318400.0000 - val_loss: 27964467200.0000 - val_mse: 27964467200.0000\n",
            "Epoch 7/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 30765602816.0000 - mse: 30765602816.0000 - val_loss: 23942903808.0000 - val_mse: 23942903808.0000\n",
            "Epoch 8/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24984743936.0000 - mse: 24984743936.0000 - val_loss: 20002459648.0000 - val_mse: 20002459648.0000\n",
            "Epoch 9/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18701377536.0000 - mse: 18701377536.0000 - val_loss: 17097344000.0000 - val_mse: 17097344000.0000\n",
            "Epoch 10/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13518607360.0000 - mse: 13518607360.0000 - val_loss: 15963901952.0000 - val_mse: 15963901952.0000\n",
            "Epoch 11/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 11014279168.0000 - mse: 11014279168.0000 - val_loss: 16016835584.0000 - val_mse: 16016835584.0000\n",
            "Epoch 12/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8583206400.0000 - mse: 8583206400.0000 - val_loss: 16484570112.0000 - val_mse: 16484570112.0000\n",
            "Epoch 13/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9999345664.0000 - mse: 9999345664.0000 - val_loss: 16352184320.0000 - val_mse: 16352184320.0000\n",
            "Epoch 14/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6964929024.0000 - mse: 6964929024.0000 - val_loss: 16125362176.0000 - val_mse: 16125362176.0000\n",
            "Epoch 15/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6372088832.0000 - mse: 6372088832.0000 - val_loss: 15540234240.0000 - val_mse: 15540234240.0000\n",
            "Epoch 16/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6163543552.0000 - mse: 6163543552.0000 - val_loss: 14859919360.0000 - val_mse: 14859919360.0000\n",
            "Epoch 17/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6050912768.0000 - mse: 6050912768.0000 - val_loss: 14444992512.0000 - val_mse: 14444992512.0000\n",
            "Epoch 18/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6028689920.0000 - mse: 6028689920.0000 - val_loss: 13663280128.0000 - val_mse: 13663280128.0000\n",
            "Epoch 19/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6515738624.0000 - mse: 6515738624.0000 - val_loss: 13274577920.0000 - val_mse: 13274577920.0000\n",
            "Epoch 20/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5933321216.0000 - mse: 5933321216.0000 - val_loss: 12865887232.0000 - val_mse: 12865887232.0000\n",
            "Epoch 21/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5115409408.0000 - mse: 5115409408.0000 - val_loss: 12342026240.0000 - val_mse: 12342026240.0000\n",
            "Epoch 22/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4753935872.0000 - mse: 4753935872.0000 - val_loss: 11819591680.0000 - val_mse: 11819591680.0000\n",
            "Epoch 23/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5036567040.0000 - mse: 5036567040.0000 - val_loss: 11383677952.0000 - val_mse: 11383677952.0000\n",
            "Epoch 24/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3645656576.0000 - mse: 3645656576.0000 - val_loss: 11096886272.0000 - val_mse: 11096886272.0000\n",
            "Epoch 25/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4134024192.0000 - mse: 4134024192.0000 - val_loss: 10533735424.0000 - val_mse: 10533735424.0000\n",
            "Epoch 26/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4408522240.0000 - mse: 4408522240.0000 - val_loss: 10136651776.0000 - val_mse: 10136651776.0000\n",
            "Epoch 27/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3459717120.0000 - mse: 3459717120.0000 - val_loss: 9750381568.0000 - val_mse: 9750381568.0000\n",
            "Epoch 28/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3249617408.0000 - mse: 3249617408.0000 - val_loss: 9357377536.0000 - val_mse: 9357377536.0000\n",
            "Epoch 29/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2892780032.0000 - mse: 2892780032.0000 - val_loss: 8970186752.0000 - val_mse: 8970186752.0000\n",
            "Epoch 30/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3284032512.0000 - mse: 3284032512.0000 - val_loss: 8613005312.0000 - val_mse: 8613005312.0000\n",
            "Epoch 31/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2483457024.0000 - mse: 2483457024.0000 - val_loss: 8528844800.0000 - val_mse: 8528844800.0000\n",
            "Epoch 32/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3287575808.0000 - mse: 3287575808.0000 - val_loss: 7906344448.0000 - val_mse: 7906344448.0000\n",
            "Epoch 33/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2496208128.0000 - mse: 2496208128.0000 - val_loss: 7759714816.0000 - val_mse: 7759714816.0000\n",
            "Epoch 34/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2850030592.0000 - mse: 2850030592.0000 - val_loss: 7439632896.0000 - val_mse: 7439632896.0000\n",
            "Epoch 35/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2113705088.0000 - mse: 2113705088.0000 - val_loss: 7321363968.0000 - val_mse: 7321363968.0000\n",
            "Epoch 36/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2453825536.0000 - mse: 2453825536.0000 - val_loss: 6914042880.0000 - val_mse: 6914042880.0000\n",
            "Epoch 37/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2217624576.0000 - mse: 2217624576.0000 - val_loss: 6747428864.0000 - val_mse: 6747428864.0000\n",
            "Epoch 38/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2060918144.0000 - mse: 2060918144.0000 - val_loss: 6589582336.0000 - val_mse: 6589582336.0000\n",
            "Epoch 39/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1794495488.0000 - mse: 1794495488.0000 - val_loss: 6423453184.0000 - val_mse: 6423453184.0000\n",
            "Epoch 40/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1862484480.0000 - mse: 1862484480.0000 - val_loss: 6104017408.0000 - val_mse: 6104017408.0000\n",
            "Epoch 41/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1681033984.0000 - mse: 1681033984.0000 - val_loss: 6100260352.0000 - val_mse: 6100260352.0000\n",
            "Epoch 42/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1578333184.0000 - mse: 1578333184.0000 - val_loss: 5851411456.0000 - val_mse: 5851411456.0000\n",
            "Epoch 43/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1655538176.0000 - mse: 1655538176.0000 - val_loss: 5714321920.0000 - val_mse: 5714321920.0000\n",
            "Epoch 44/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1839116544.0000 - mse: 1839116544.0000 - val_loss: 5518241792.0000 - val_mse: 5518241792.0000\n",
            "Epoch 45/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1679203968.0000 - mse: 1679203968.0000 - val_loss: 5525457920.0000 - val_mse: 5525457920.0000\n",
            "Epoch 46/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1750645504.0000 - mse: 1750645504.0000 - val_loss: 5308580864.0000 - val_mse: 5308580864.0000\n",
            "Epoch 47/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1332437376.0000 - mse: 1332437376.0000 - val_loss: 5303923712.0000 - val_mse: 5303923712.0000\n",
            "Epoch 48/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1700515328.0000 - mse: 1700515328.0000 - val_loss: 5106107904.0000 - val_mse: 5106107904.0000\n",
            "Epoch 49/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1257392000.0000 - mse: 1257392000.0000 - val_loss: 5095339008.0000 - val_mse: 5095339008.0000\n",
            "Epoch 50/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1141460736.0000 - mse: 1141460736.0000 - val_loss: 4999171584.0000 - val_mse: 4999171584.0000\n",
            "Epoch 51/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1336490880.0000 - mse: 1336490880.0000 - val_loss: 4883980288.0000 - val_mse: 4883980288.0000\n",
            "Epoch 52/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1192343296.0000 - mse: 1192343296.0000 - val_loss: 4779662336.0000 - val_mse: 4779662336.0000\n",
            "Epoch 53/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1385270784.0000 - mse: 1385270784.0000 - val_loss: 4612343296.0000 - val_mse: 4612343296.0000\n",
            "Epoch 54/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1856573184.0000 - mse: 1856573184.0000 - val_loss: 4629686272.0000 - val_mse: 4629686272.0000\n",
            "Epoch 55/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1324412416.0000 - mse: 1324412416.0000 - val_loss: 4609104896.0000 - val_mse: 4609104896.0000\n",
            "Epoch 56/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1170930432.0000 - mse: 1170930432.0000 - val_loss: 4435057664.0000 - val_mse: 4435057664.0000\n",
            "Epoch 57/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1071936768.0000 - mse: 1071936768.0000 - val_loss: 4413720576.0000 - val_mse: 4413720576.0000\n",
            "Epoch 58/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1170212352.0000 - mse: 1170212352.0000 - val_loss: 4447058944.0000 - val_mse: 4447058944.0000\n",
            "Epoch 59/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1638754176.0000 - mse: 1638754176.0000 - val_loss: 4169585664.0000 - val_mse: 4169585664.0000\n",
            "Epoch 60/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1273889152.0000 - mse: 1273889152.0000 - val_loss: 4240804608.0000 - val_mse: 4240804608.0000\n",
            "Epoch 61/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1715387520.0000 - mse: 1715387520.0000 - val_loss: 4135896064.0000 - val_mse: 4135896064.0000\n",
            "Epoch 62/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1216715904.0000 - mse: 1216715904.0000 - val_loss: 4090940672.0000 - val_mse: 4090940672.0000\n",
            "Epoch 63/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1536969856.0000 - mse: 1536969856.0000 - val_loss: 4191297792.0000 - val_mse: 4191297792.0000\n",
            "Epoch 64/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1888445952.0000 - mse: 1888445952.0000 - val_loss: 4074426112.0000 - val_mse: 4074426112.0000\n",
            "Epoch 65/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1119329024.0000 - mse: 1119329024.0000 - val_loss: 4177027072.0000 - val_mse: 4177027072.0000\n",
            "Epoch 66/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1188588544.0000 - mse: 1188588544.0000 - val_loss: 3819369984.0000 - val_mse: 3819369984.0000\n",
            "Epoch 67/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1365223040.0000 - mse: 1365223040.0000 - val_loss: 3922446336.0000 - val_mse: 3922446336.0000\n",
            "Epoch 68/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1784432128.0000 - mse: 1784432128.0000 - val_loss: 3920007936.0000 - val_mse: 3920007936.0000\n",
            "Epoch 69/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1030675904.0000 - mse: 1030675904.0000 - val_loss: 3893836800.0000 - val_mse: 3893836800.0000\n",
            "Epoch 70/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1024222208.0000 - mse: 1024222208.0000 - val_loss: 3773741824.0000 - val_mse: 3773741824.0000\n",
            "Epoch 71/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1695923584.0000 - mse: 1695923584.0000 - val_loss: 3790816768.0000 - val_mse: 3790816768.0000\n",
            "Epoch 72/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1717969920.0000 - mse: 1717969920.0000 - val_loss: 3841732608.0000 - val_mse: 3841732608.0000\n",
            "Epoch 73/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 785104576.0000 - mse: 785104576.0000 - val_loss: 3874504960.0000 - val_mse: 3874504960.0000\n",
            "Epoch 74/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 818517632.0000 - mse: 818517632.0000 - val_loss: 3737721856.0000 - val_mse: 3737721856.0000\n",
            "Epoch 75/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 939056896.0000 - mse: 939056896.0000 - val_loss: 3729868032.0000 - val_mse: 3729868032.0000\n",
            "Epoch 76/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1024797504.0000 - mse: 1024797504.0000 - val_loss: 3412535552.0000 - val_mse: 3412535552.0000\n",
            "Epoch 77/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1034526848.0000 - mse: 1034526848.0000 - val_loss: 3583857920.0000 - val_mse: 3583857920.0000\n",
            "Epoch 78/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 879369152.0000 - mse: 879369152.0000 - val_loss: 3540289792.0000 - val_mse: 3540289792.0000\n",
            "Epoch 79/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1169180544.0000 - mse: 1169180544.0000 - val_loss: 3503731968.0000 - val_mse: 3503731968.0000\n",
            "Epoch 80/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1248598784.0000 - mse: 1248598784.0000 - val_loss: 3475734016.0000 - val_mse: 3475734016.0000\n",
            "Epoch 81/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1184993152.0000 - mse: 1184993152.0000 - val_loss: 3513032704.0000 - val_mse: 3513032704.0000\n",
            "Epoch 82/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1293037440.0000 - mse: 1293037440.0000 - val_loss: 3502638080.0000 - val_mse: 3502638080.0000\n",
            "Epoch 83/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 986202048.0000 - mse: 986202048.0000 - val_loss: 3441467904.0000 - val_mse: 3441467904.0000\n",
            "Epoch 84/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 968990464.0000 - mse: 968990464.0000 - val_loss: 3494690560.0000 - val_mse: 3494690560.0000\n",
            "Epoch 85/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 872590208.0000 - mse: 872590208.0000 - val_loss: 3412397568.0000 - val_mse: 3412397568.0000\n",
            "Epoch 86/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 808122688.0000 - mse: 808122688.0000 - val_loss: 3431990784.0000 - val_mse: 3431990784.0000\n",
            "Epoch 87/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1037776000.0000 - mse: 1037776000.0000 - val_loss: 3423492096.0000 - val_mse: 3423492096.0000\n",
            "Epoch 88/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 778865920.0000 - mse: 778865920.0000 - val_loss: 3485171456.0000 - val_mse: 3485171456.0000\n",
            "Epoch 89/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 726446144.0000 - mse: 726446144.0000 - val_loss: 3425285120.0000 - val_mse: 3425285120.0000\n",
            "Epoch 90/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 895165248.0000 - mse: 895165248.0000 - val_loss: 3362077184.0000 - val_mse: 3362077184.0000\n",
            "Epoch 91/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 855510912.0000 - mse: 855510912.0000 - val_loss: 3426298112.0000 - val_mse: 3426298112.0000\n",
            "Epoch 92/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 985600832.0000 - mse: 985600832.0000 - val_loss: 3402867712.0000 - val_mse: 3402867712.0000\n",
            "Epoch 93/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 827815872.0000 - mse: 827815872.0000 - val_loss: 3406704640.0000 - val_mse: 3406704640.0000\n",
            "Epoch 94/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1146059776.0000 - mse: 1146059776.0000 - val_loss: 3367529984.0000 - val_mse: 3367529984.0000\n",
            "Epoch 95/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1219588608.0000 - mse: 1219588608.0000 - val_loss: 3332461056.0000 - val_mse: 3332461056.0000\n",
            "Epoch 96/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 925878144.0000 - mse: 925878144.0000 - val_loss: 3422527232.0000 - val_mse: 3422527232.0000\n",
            "Epoch 97/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1576262912.0000 - mse: 1576262912.0000 - val_loss: 3424341248.0000 - val_mse: 3424341248.0000\n",
            "Epoch 98/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1104958464.0000 - mse: 1104958464.0000 - val_loss: 3367007744.0000 - val_mse: 3367007744.0000\n",
            "Epoch 99/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 960828736.0000 - mse: 960828736.0000 - val_loss: 3280770816.0000 - val_mse: 3280770816.0000\n",
            "Epoch 100/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 870103360.0000 - mse: 870103360.0000 - val_loss: 3501828608.0000 - val_mse: 3501828608.0000\n",
            "Epoch 101/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 946136832.0000 - mse: 946136832.0000 - val_loss: 3109611520.0000 - val_mse: 3109611520.0000\n",
            "Epoch 102/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 914191744.0000 - mse: 914191744.0000 - val_loss: 3205561088.0000 - val_mse: 3205561088.0000\n",
            "Epoch 103/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 809151552.0000 - mse: 809151552.0000 - val_loss: 3383348480.0000 - val_mse: 3383348480.0000\n",
            "Epoch 104/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 798621504.0000 - mse: 798621504.0000 - val_loss: 3211842304.0000 - val_mse: 3211842304.0000\n",
            "Epoch 105/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 709462592.0000 - mse: 709462592.0000 - val_loss: 3240267008.0000 - val_mse: 3240267008.0000\n",
            "Epoch 106/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1122252672.0000 - mse: 1122252672.0000 - val_loss: 3261313024.0000 - val_mse: 3261313024.0000\n",
            "Epoch 107/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 784419456.0000 - mse: 784419456.0000 - val_loss: 3326720000.0000 - val_mse: 3326720000.0000\n",
            "Epoch 108/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1690543616.0000 - mse: 1690543616.0000 - val_loss: 3215658752.0000 - val_mse: 3215658752.0000\n",
            "Epoch 109/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1077246336.0000 - mse: 1077246336.0000 - val_loss: 3241413888.0000 - val_mse: 3241413888.0000\n",
            "Epoch 110/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1117766656.0000 - mse: 1117766656.0000 - val_loss: 3252566016.0000 - val_mse: 3252566016.0000\n",
            "Epoch 111/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 993270464.0000 - mse: 993270464.0000 - val_loss: 3301988608.0000 - val_mse: 3301988608.0000\n",
            "Epoch 112/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 871091328.0000 - mse: 871091328.0000 - val_loss: 3217424896.0000 - val_mse: 3217424896.0000\n",
            "Epoch 113/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1155419264.0000 - mse: 1155419264.0000 - val_loss: 3231419904.0000 - val_mse: 3231419904.0000\n",
            "Epoch 114/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1068717056.0000 - mse: 1068717056.0000 - val_loss: 3210260736.0000 - val_mse: 3210260736.0000\n",
            "Epoch 115/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 903547264.0000 - mse: 903547264.0000 - val_loss: 3284670464.0000 - val_mse: 3284670464.0000\n",
            "Epoch 116/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1465803648.0000 - mse: 1465803648.0000 - val_loss: 3194912000.0000 - val_mse: 3194912000.0000\n",
            "Epoch 117/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 879144704.0000 - mse: 879144704.0000 - val_loss: 3183402752.0000 - val_mse: 3183402752.0000\n",
            "Epoch 118/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 898690240.0000 - mse: 898690240.0000 - val_loss: 3213781504.0000 - val_mse: 3213781504.0000\n",
            "Epoch 119/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 720923584.0000 - mse: 720923584.0000 - val_loss: 3175317248.0000 - val_mse: 3175317248.0000\n",
            "Epoch 120/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1482646912.0000 - mse: 1482646912.0000 - val_loss: 3267967488.0000 - val_mse: 3267967488.0000\n",
            "Epoch 121/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1264366080.0000 - mse: 1264366080.0000 - val_loss: 3211702528.0000 - val_mse: 3211702528.0000\n",
            "Epoch 122/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 621031104.0000 - mse: 621031104.0000 - val_loss: 3214868992.0000 - val_mse: 3214868992.0000\n",
            "Epoch 123/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 874002176.0000 - mse: 874002176.0000 - val_loss: 3160796416.0000 - val_mse: 3160796416.0000\n",
            "Epoch 124/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 831980352.0000 - mse: 831980352.0000 - val_loss: 3150134272.0000 - val_mse: 3150134272.0000\n",
            "Epoch 125/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 853961344.0000 - mse: 853961344.0000 - val_loss: 3330172928.0000 - val_mse: 3330172928.0000\n",
            "Epoch 126/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 850338496.0000 - mse: 850338496.0000 - val_loss: 3161513984.0000 - val_mse: 3161513984.0000\n",
            "Epoch 127/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1092213760.0000 - mse: 1092213760.0000 - val_loss: 3176643072.0000 - val_mse: 3176643072.0000\n",
            "Epoch 128/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1064915136.0000 - mse: 1064915136.0000 - val_loss: 3140046592.0000 - val_mse: 3140046592.0000\n",
            "Epoch 129/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1372809088.0000 - mse: 1372809088.0000 - val_loss: 3215493120.0000 - val_mse: 3215493120.0000\n",
            "Epoch 130/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 956336256.0000 - mse: 956336256.0000 - val_loss: 3171541248.0000 - val_mse: 3171541248.0000\n",
            "Epoch 131/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1015085184.0000 - mse: 1015085184.0000 - val_loss: 3276614400.0000 - val_mse: 3276614400.0000\n",
            "Epoch 132/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 867386624.0000 - mse: 867386624.0000 - val_loss: 3147297280.0000 - val_mse: 3147297280.0000\n",
            "Epoch 133/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 765362880.0000 - mse: 765362880.0000 - val_loss: 3128966400.0000 - val_mse: 3128966400.0000\n",
            "Epoch 134/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 778399104.0000 - mse: 778399104.0000 - val_loss: 3216147712.0000 - val_mse: 3216147712.0000\n",
            "Epoch 135/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1090029824.0000 - mse: 1090029824.0000 - val_loss: 3116510464.0000 - val_mse: 3116510464.0000\n",
            "Epoch 136/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 952257664.0000 - mse: 952257664.0000 - val_loss: 3251907072.0000 - val_mse: 3251907072.0000\n",
            "Epoch 137/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 959201152.0000 - mse: 959201152.0000 - val_loss: 3206540800.0000 - val_mse: 3206540800.0000\n",
            "Epoch 138/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1153694976.0000 - mse: 1153694976.0000 - val_loss: 3221838336.0000 - val_mse: 3221838336.0000\n",
            "Epoch 139/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 677194624.0000 - mse: 677194624.0000 - val_loss: 3254857216.0000 - val_mse: 3254857216.0000\n",
            "Epoch 140/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 716613760.0000 - mse: 716613760.0000 - val_loss: 3270659072.0000 - val_mse: 3270659072.0000\n",
            "Epoch 141/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1116560128.0000 - mse: 1116560128.0000 - val_loss: 3120181760.0000 - val_mse: 3120181760.0000\n",
            "Epoch 142/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1164717312.0000 - mse: 1164717312.0000 - val_loss: 3271679744.0000 - val_mse: 3271679744.0000\n",
            "Epoch 143/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 674445824.0000 - mse: 674445824.0000 - val_loss: 3255684608.0000 - val_mse: 3255684608.0000\n",
            "Epoch 144/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1246000384.0000 - mse: 1246000384.0000 - val_loss: 3198881792.0000 - val_mse: 3198881792.0000\n",
            "Epoch 145/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 714143616.0000 - mse: 714143616.0000 - val_loss: 3243628288.0000 - val_mse: 3243628288.0000\n",
            "Epoch 146/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1115707904.0000 - mse: 1115707904.0000 - val_loss: 3187025920.0000 - val_mse: 3187025920.0000\n",
            "Epoch 147/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 698688896.0000 - mse: 698688896.0000 - val_loss: 3235451392.0000 - val_mse: 3235451392.0000\n",
            "Epoch 148/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1081091200.0000 - mse: 1081091200.0000 - val_loss: 3107722496.0000 - val_mse: 3107722496.0000\n",
            "Epoch 149/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 923750016.0000 - mse: 923750016.0000 - val_loss: 3168394240.0000 - val_mse: 3168394240.0000\n",
            "Epoch 150/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 767529280.0000 - mse: 767529280.0000 - val_loss: 3161254400.0000 - val_mse: 3161254400.0000\n",
            "Epoch 151/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 947165184.0000 - mse: 947165184.0000 - val_loss: 3156121600.0000 - val_mse: 3156121600.0000\n",
            "Epoch 152/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1127740672.0000 - mse: 1127740672.0000 - val_loss: 3197404160.0000 - val_mse: 3197404160.0000\n",
            "Epoch 153/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 713133056.0000 - mse: 713133056.0000 - val_loss: 3104115968.0000 - val_mse: 3104115968.0000\n",
            "Epoch 154/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 970175552.0000 - mse: 970175552.0000 - val_loss: 3275012352.0000 - val_mse: 3275012352.0000\n",
            "Epoch 155/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 744900032.0000 - mse: 744900032.0000 - val_loss: 3357841664.0000 - val_mse: 3357841664.0000\n",
            "Epoch 156/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 739133504.0000 - mse: 739133504.0000 - val_loss: 3071685120.0000 - val_mse: 3071685120.0000\n",
            "Epoch 157/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 894988928.0000 - mse: 894988928.0000 - val_loss: 3174976000.0000 - val_mse: 3174976000.0000\n",
            "Epoch 158/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 913334976.0000 - mse: 913334976.0000 - val_loss: 3113025280.0000 - val_mse: 3113025280.0000\n",
            "Epoch 159/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 870096000.0000 - mse: 870096000.0000 - val_loss: 3185523712.0000 - val_mse: 3185523712.0000\n",
            "Epoch 160/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 837551296.0000 - mse: 837551296.0000 - val_loss: 3165030400.0000 - val_mse: 3165030400.0000\n",
            "Epoch 161/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 768459136.0000 - mse: 768459136.0000 - val_loss: 3259336704.0000 - val_mse: 3259336704.0000\n",
            "Epoch 162/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1403000320.0000 - mse: 1403000320.0000 - val_loss: 3151657216.0000 - val_mse: 3151657216.0000\n",
            "Epoch 163/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 745973504.0000 - mse: 745973504.0000 - val_loss: 3236733184.0000 - val_mse: 3236733184.0000\n",
            "Epoch 164/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1241310976.0000 - mse: 1241310976.0000 - val_loss: 3128783360.0000 - val_mse: 3128783360.0000\n",
            "Epoch 165/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1175627392.0000 - mse: 1175627392.0000 - val_loss: 3147239680.0000 - val_mse: 3147239680.0000\n",
            "Epoch 166/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 819262400.0000 - mse: 819262400.0000 - val_loss: 3220646144.0000 - val_mse: 3220646144.0000\n",
            "Epoch 167/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 686591360.0000 - mse: 686591360.0000 - val_loss: 3132085504.0000 - val_mse: 3132085504.0000\n",
            "Epoch 168/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 720985600.0000 - mse: 720985600.0000 - val_loss: 3172898304.0000 - val_mse: 3172898304.0000\n",
            "Epoch 169/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1310443008.0000 - mse: 1310443008.0000 - val_loss: 3141313024.0000 - val_mse: 3141313024.0000\n",
            "Epoch 170/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 891675392.0000 - mse: 891675392.0000 - val_loss: 3135323904.0000 - val_mse: 3135323904.0000\n",
            "Epoch 171/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 719463104.0000 - mse: 719463104.0000 - val_loss: 3117698048.0000 - val_mse: 3117698048.0000\n",
            "Epoch 172/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 850874880.0000 - mse: 850874880.0000 - val_loss: 3104989440.0000 - val_mse: 3104989440.0000\n",
            "Epoch 173/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 921504768.0000 - mse: 921504768.0000 - val_loss: 3129317376.0000 - val_mse: 3129317376.0000\n",
            "Epoch 174/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 648574656.0000 - mse: 648574656.0000 - val_loss: 3142311424.0000 - val_mse: 3142311424.0000\n",
            "Epoch 175/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 741355072.0000 - mse: 741355072.0000 - val_loss: 3178683904.0000 - val_mse: 3178683904.0000\n",
            "Epoch 176/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 932312640.0000 - mse: 932312640.0000 - val_loss: 3077217280.0000 - val_mse: 3077217280.0000\n",
            "Epoch 177/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 811617152.0000 - mse: 811617152.0000 - val_loss: 3118853376.0000 - val_mse: 3118853376.0000\n",
            "Epoch 178/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 940697792.0000 - mse: 940697792.0000 - val_loss: 3083233536.0000 - val_mse: 3083233536.0000\n",
            "Epoch 179/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1115804544.0000 - mse: 1115804544.0000 - val_loss: 3147248128.0000 - val_mse: 3147248128.0000\n",
            "Epoch 180/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 595668800.0000 - mse: 595668800.0000 - val_loss: 3172909056.0000 - val_mse: 3172909056.0000\n",
            "Epoch 181/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 854505280.0000 - mse: 854505280.0000 - val_loss: 3042073088.0000 - val_mse: 3042073088.0000\n",
            "Epoch 182/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1373886464.0000 - mse: 1373886464.0000 - val_loss: 3238274048.0000 - val_mse: 3238274048.0000\n",
            "Epoch 183/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 852656640.0000 - mse: 852656640.0000 - val_loss: 3080728832.0000 - val_mse: 3080728832.0000\n",
            "Epoch 184/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1150559488.0000 - mse: 1150559488.0000 - val_loss: 3101298176.0000 - val_mse: 3101298176.0000\n",
            "Epoch 185/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 919453952.0000 - mse: 919453952.0000 - val_loss: 3135241728.0000 - val_mse: 3135241728.0000\n",
            "Epoch 186/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 618746048.0000 - mse: 618746048.0000 - val_loss: 3211229184.0000 - val_mse: 3211229184.0000\n",
            "Epoch 187/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 863168448.0000 - mse: 863168448.0000 - val_loss: 3095293696.0000 - val_mse: 3095293696.0000\n",
            "Epoch 188/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 778195008.0000 - mse: 778195008.0000 - val_loss: 3169185792.0000 - val_mse: 3169185792.0000\n",
            "Epoch 189/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1161431808.0000 - mse: 1161431808.0000 - val_loss: 3084802048.0000 - val_mse: 3084802048.0000\n",
            "Epoch 190/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 775699008.0000 - mse: 775699008.0000 - val_loss: 3194339840.0000 - val_mse: 3194339840.0000\n",
            "Epoch 191/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 837095040.0000 - mse: 837095040.0000 - val_loss: 3050640640.0000 - val_mse: 3050640640.0000\n",
            "Epoch 192/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 742201984.0000 - mse: 742201984.0000 - val_loss: 3156266752.0000 - val_mse: 3156266752.0000\n",
            "Epoch 193/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 912118528.0000 - mse: 912118528.0000 - val_loss: 3085353216.0000 - val_mse: 3085353216.0000\n",
            "Epoch 194/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 751934272.0000 - mse: 751934272.0000 - val_loss: 3097871616.0000 - val_mse: 3097871616.0000\n",
            "Epoch 195/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 729171584.0000 - mse: 729171584.0000 - val_loss: 3052632832.0000 - val_mse: 3052632832.0000\n",
            "Epoch 196/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 869600576.0000 - mse: 869600576.0000 - val_loss: 3088596224.0000 - val_mse: 3088596224.0000\n",
            "Epoch 197/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1142136704.0000 - mse: 1142136704.0000 - val_loss: 3166116608.0000 - val_mse: 3166116608.0000\n",
            "Epoch 198/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 779931776.0000 - mse: 779931776.0000 - val_loss: 3162332416.0000 - val_mse: 3162332416.0000\n",
            "Epoch 199/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1122835456.0000 - mse: 1122835456.0000 - val_loss: 3073302528.0000 - val_mse: 3073302528.0000\n",
            "Epoch 200/200\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1486042496.0000 - mse: 1486042496.0000 - val_loss: 3146011904.0000 - val_mse: 3146011904.0000\n",
            "Training finished.\n",
            "Evaluating on test set...\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1416211456.0000 - mse: 1416211456.0000 \n",
            "\n",
            "Test MSE: 1427546112.00\n",
            "Test RMSE: $37,782.88\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
            "Predicted Price for sample house: $210,442.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Question 5] Create an MNIST model"
      ],
      "metadata": {
        "id": "gLyf1NdN45gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load MNIST data using tf.keras.datasets\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], 784).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Create validation set\n",
        "validation_split = 0.1\n",
        "split_index = int(x_train.shape[0] * (1 - validation_split))\n",
        "x_val, y_val = x_train[split_index:], y_train[split_index:]\n",
        "x_train, y_train = x_train[:split_index], y_train[:split_index]\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "n_input = x_train.shape[1]\n",
        "n_hidden1 = 512\n",
        "n_hidden2 = 256\n",
        "n_classes = num_classes\n",
        "\n",
        "# Define the model using Keras Sequential API with Input layer\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(n_input,)),\n",
        "    tf.keras.layers.Dense(n_hidden1, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_hidden2, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes)  # No softmax since using from_logits=True\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_val, y_val))\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Evaluating on test set...\")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Predict on a sample image\n",
        "sample_image = x_test[0].reshape(1, n_input)\n",
        "predicted_logits = model.predict(sample_image)\n",
        "predicted_class = np.argmax(predicted_logits)\n",
        "\n",
        "# True class\n",
        "true_class = np.argmax(y_test[0])\n",
        "\n",
        "# Optionally apply softmax to get probabilities\n",
        "predicted_probs = tf.nn.softmax(predicted_logits).numpy()[0]\n",
        "\n",
        "print(f\"\\nSample Image Prediction:\")\n",
        "print(f\"  Predicted Logits: {predicted_logits[0]}\")\n",
        "print(f\"  Predicted Probabilities (softmax): {predicted_probs}\")\n",
        "print(f\"  Predicted Class: {predicted_class}\")\n",
        "print(f\"  True Class: {true_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChoUvFcD3YGW",
        "outputId": "99e89005-60be-4ed7-e798-2bc3abb906bd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.8947 - loss: 0.3424 - val_accuracy: 0.9663 - val_loss: 0.1039\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.9743 - loss: 0.0806 - val_accuracy: 0.9770 - val_loss: 0.0762\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - accuracy: 0.9831 - loss: 0.0522 - val_accuracy: 0.9798 - val_loss: 0.0732\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - accuracy: 0.9873 - loss: 0.0391 - val_accuracy: 0.9760 - val_loss: 0.0793\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9904 - loss: 0.0287 - val_accuracy: 0.9818 - val_loss: 0.0768\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - accuracy: 0.9922 - loss: 0.0227 - val_accuracy: 0.9800 - val_loss: 0.0860\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9934 - loss: 0.0200 - val_accuracy: 0.9787 - val_loss: 0.0980\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9946 - loss: 0.0177 - val_accuracy: 0.9788 - val_loss: 0.1030\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.9943 - loss: 0.0169 - val_accuracy: 0.9787 - val_loss: 0.1015\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9946 - loss: 0.0156 - val_accuracy: 0.9793 - val_loss: 0.0975\n",
            "Training finished.\n",
            "Evaluating on test set...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9752 - loss: 0.1190\n",
            "\n",
            "Test Accuracy: 0.9788\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\n",
            "Sample Image Prediction:\n",
            "  Predicted Logits: [-13.618185  -10.530514   -7.2194266   2.036842  -22.617386  -10.362293\n",
            " -23.231625   19.607712  -10.992364    1.0496519]\n",
            "  Predicted Probabilities (softmax): [3.7168538e-15 8.1495744e-14 2.2341976e-12 2.3392055e-08 4.5906384e-19\n",
            " 9.6425560e-14 2.4837770e-19 1.0000000e+00 5.1351884e-14 8.7163983e-09]\n",
            "  Predicted Class: 7\n",
            "  True Class: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mhm9dH-X6Q1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}