{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgRWUysqBgH/nXDS4XfCSC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cliffochi/aviva_data_science_course/blob/main/Deep_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 1] Classifying fully connected layers"
      ],
      "metadata": {
        "id": "MdutiDQfs_wT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mfHkCpH3r2SW"
      },
      "outputs": [],
      "source": [
        "# implementation of the fully connected (FC) layer class\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    Fully connected layer from n_nodes1 to n_nodes2\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the later layer\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initialize weights and biases using the initializer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "        # Variables to store during forward pass for backward pass\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (batch_size, n_nodes1)\n",
        "            Input data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        A : ndarray, shape (batch_size, n_nodes2)\n",
        "            Output before activation\n",
        "        \"\"\"\n",
        "        self.X = X  # Store input for backward pass\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray, shape (batch_size, n_nodes2)\n",
        "            Gradient from the next layer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
        "            Gradient to previous layer\n",
        "        \"\"\"\n",
        "        batch_size = dA.shape[0]\n",
        "\n",
        "        # Calculate gradients\n",
        "        self.dW = np.dot(self.X.T, dA) / batch_size\n",
        "        self.dB = np.sum(dA, axis=0) / batch_size\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        # Update parameters using optimizer\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Supporting Classes Implementation\n",
        "For completeness, here are the supporting classes needed for the FC layer to work:\n",
        "\n",
        "1. Simple Initializer"
      ],
      "metadata": {
        "id": "1-MeIDF_s-qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple weight initializer using Gaussian distribution\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initialize weights\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in next layer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weights\n",
        "        \"\"\"\n",
        "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initialize biases\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in next layer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        B : ndarray, shape (n_nodes2,)\n",
        "            Initialized biases\n",
        "        \"\"\"\n",
        "        return np.zeros(n_nodes2)"
      ],
      "metadata": {
        "id": "k87Pj_-1ukpC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Stochastic Gradient Descent (SGD)Optimizer"
      ],
      "metadata": {
        "id": "Spm1F2A9uqR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent optimizer\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "      Learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update layer parameters\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : FC instance\n",
        "          Layer to update\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layer : FC instance\n",
        "          Updated layer\n",
        "        \"\"\"\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return layer"
      ],
      "metadata": {
        "id": "sb0CjSbkuoNE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Activation Functions\n",
        "\n",
        "  Tanh Function"
      ],
      "metadata": {
        "id": "M7Nzemvau6_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh:\n",
        "    \"\"\"\n",
        "    Hyperbolic tangent activation function\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : ndarray\n",
        "          Input before activation\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Z : ndarray\n",
        "          Output after activation\n",
        "        \"\"\"\n",
        "        self.A = A  # Store for backward pass\n",
        "        return np.tanh(A)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : ndarray\n",
        "          Gradient from next layer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dA : ndarray\n",
        "          Gradient to previous layer\n",
        "        \"\"\"\n",
        "        return dZ * (1 - np.tanh(self.A)**2)"
      ],
      "metadata": {
        "id": "LmhEISlhu22X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Activation(with cross-entropy loss)"
      ],
      "metadata": {
        "id": "VH2-A35UvHw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    \"\"\"\n",
        "    Softmax activation with cross-entropy loss\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : ndarray\n",
        "          Input before activation\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Z : ndarray\n",
        "          Output after activation (probabilities)\n",
        "        \"\"\"\n",
        "        # Numerically stable softmax\n",
        "        exp_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Z, Y):\n",
        "        \"\"\"\n",
        "        Backward pass with cross-entropy loss\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Z : ndarray\n",
        "          Output from forward pass\n",
        "        Y : ndarray\n",
        "          One-hot encoded true labels\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dA : ndarray\n",
        "          Gradient to previous layer\n",
        "        \"\"\"\n",
        "        batch_size = Y.shape[0]\n",
        "        return (Z - Y) / batch_size"
      ],
      "metadata": {
        "id": "MD-HAp0zvCeF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation Notes\n",
        "\n",
        "1. **Modular Design**: Each component (layers, activations, initializers, optimizers) is encapsulated in its own class, making it easy to swap implementations.\n",
        "\n",
        "2. **Forward/Backward Interface**: All layers and activations implement consistent forward/backward methods, enabling easy composition.\n",
        "\n",
        "3. **State Management**: Each layer maintains its own parameters (W, B) and necessary state (input X for backpropagation).\n",
        "\n",
        "4. **Optimizer Integration**: The optimizer is responsible for updating parameters, allowing different optimization algorithms to be plugged in.\n",
        "\n",
        "5. **Numerical Stability**: The softmax implementation includes numerical stability improvements by subtracting the maximum value before exponentiation.\n",
        "\n",
        "This implementation provides the foundation for building deeper networks by simply adding more FC and activation layers in sequence. The modular design makes it easy to experiment with different architectures, initialization schemes, and optimization methods."
      ],
      "metadata": {
        "id": "WoSx8SIWvT7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 2] Classifying the initialization method\n",
        "\n",
        "Implementation of the SimpleInitializer class that follows the specified template and initializes weights using a Gaussian distribution while initializing biases to zeros:"
      ],
      "metadata": {
        "id": "lBGD2ywsv8Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution for weights and zeros for biases\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution for weight initialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization using Gaussian distribution\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the next layer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
        "            Initialized weight matrix\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization using zeros\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the next layer\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        B : ndarray, shape (n_nodes2,)\n",
        "            Initialized bias vector\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        return B"
      ],
      "metadata": {
        "id": "6izb_-pOvRAF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features of the Implementation:\n",
        "\n",
        "1. **Weight Initialization**:\n",
        "   - Uses Gaussian (normal) distribution with mean 0 and specified standard deviation (sigma)\n",
        "   - Scales the random values by the sigma parameter\n",
        "   - Creates a matrix of shape (n_nodes1, n_nodes2)\n",
        "\n",
        "2. **Bias Initialization**:\n",
        "   - Initializes all biases to zero\n",
        "   - Creates a vector of length n_nodes2\n",
        "\n",
        "3. **Design Benefits**:\n",
        "   - Encapsulates initialization logic in a separate class\n",
        "   - Follows the principle of single responsibility\n",
        "   - Makes it easy to swap different initialization strategies\n",
        "   - Keeps the FC layer code clean by moving initialization logic out\n",
        "\n",
        "4. **Usage Example**:\n",
        "```python\n",
        "  # Create initializer with sigma=0.01\n",
        "  initializer = SimpleInitializer(sigma=0.01)\n",
        "\n",
        "  # Initialize weights for a layer with 784 input nodes and 400 hidden nodes\n",
        "  weights = initializer.W(784, 400)\n",
        "\n",
        "  # Initialize biases for the 400-node hidden layer\n",
        "  biases = initializer.B(400)\n",
        "```\n",
        "\n",
        "This implementation provides a solid foundation that can be easily extended to support more sophisticated initialization methods like Xavier/Glorot initialization or He initialization by creating additional initializer classes with the same interface."
      ],
      "metadata": {
        "id": "NQEOrntJwUwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 3] Classifying optimization methods"
      ],
      "metadata": {
        "id": "Skagd204w3bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent optimization method\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : float\n",
        "      Learning rate (step size for each update)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer using gradient descent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : FC\n",
        "          The fully connected layer instance containing:\n",
        "            - W: weight matrix to update\n",
        "            - B: bias vector to update\n",
        "            - dW: weight gradients\n",
        "            - dB: bias gradients\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layer : FC\n",
        "          The updated layer with new weights and biases\n",
        "        \"\"\"\n",
        "        # Update weights: W = W - lr * dW\n",
        "        layer.W -= self.lr * layer.dW\n",
        "\n",
        "        # Update biases: B = B - lr * dB\n",
        "        layer.B -= self.lr * layer.dB\n",
        "\n",
        "        return layer"
      ],
      "metadata": {
        "id": "ugcp-qFYwNHW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features of the Implementation:\n",
        "\n",
        "1. **Learning Rate Control**:\n",
        "   - The learning rate (`lr`) is set during initialization\n",
        "   - Controls the step size of each parameter update\n",
        "\n",
        "2. **Update Mechanism**:\n",
        "   - Implements classic gradient descent: `θ = θ - η*∇J(θ)`\n",
        "   - Updates both weights (`W`) and biases (`B`) using their respective gradients\n",
        "\n",
        "3. **Layer Interface**:\n",
        "   - Expects the layer to have properties:\n",
        "     - `W`: weight matrix\n",
        "     - `B`: bias vector\n",
        "     - `dW`: weight gradients\n",
        "     - `dB`: bias gradients\n",
        "   - Returns the modified layer for method chaining\n",
        "\n",
        "4. **Usage Example**:\n",
        "```python\n",
        "  # Create optimizer with learning rate 0.01\n",
        "  optimizer = SGD(lr=0.01)\n",
        "\n",
        "  # During training:\n",
        "  # layer = FC(...)  # Create layer\n",
        "  # A = layer.forward(X)  # Forward pass\n",
        "  # dZ = layer.backward(dA)  # Backward pass (computes dW, dB)\n",
        "  # layer = optimizer.update(layer)  # Update parameters\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The current SGD implementation provides the basic functionality needed for gradient descent while maintaining a clean interface that matches the requirements for integration with the FC layer class."
      ],
      "metadata": {
        "id": "t7gU7ZGhxLg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 4] Classifying activation functions\n",
        "\n",
        "I'll implement the activation function classes, including a special combined Softmax with Cross-Entropy implementation for efficient backpropagation.\n"
      ],
      "metadata": {
        "id": "yIBrxzI9xm72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Base Activation Function Class\n",
        "First, let's create a base class for all activation functions:"
      ],
      "metadata": {
        "id": "xxHrNbqNykgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Activation:\n",
        "    \"\"\"Base class for activation functions\"\"\"\n",
        "    def forward(self, X):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "QQaFbU9EySRB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tanh Activation"
      ],
      "metadata": {
        "id": "6jSNM1ybyvea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(Activation):\n",
        "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        Parameters:\n",
        "            X: input array\n",
        "        Returns:\n",
        "            activated output\n",
        "        \"\"\"\n",
        "        self.X = X  # Store for backward pass\n",
        "        return np.tanh(X)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Parameters:\n",
        "            dZ: gradient from next layer\n",
        "        Returns:\n",
        "            gradient to previous layer\n",
        "        \"\"\"\n",
        "        return dZ * (1 - np.tanh(self.X)**2)"
      ],
      "metadata": {
        "id": "ENWg6GLzy0Am"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ReLu Activation"
      ],
      "metadata": {
        "id": "C-1U5SvFy5m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Activation):\n",
        "    \"\"\"Rectified Linear Unit activation function\"\"\"\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        Parameters:\n",
        "            X: input array\n",
        "        Returns:\n",
        "            activated output\n",
        "        \"\"\"\n",
        "        self.X = X  # Store for backward pass\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Parameters:\n",
        "            dZ: gradient from next layer\n",
        "        Returns:\n",
        "            gradient to previous layer\n",
        "        \"\"\"\n",
        "        return dZ * (self.X > 0)"
      ],
      "metadata": {
        "id": "jhml4NzQzDb2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sigmoid Activation"
      ],
      "metadata": {
        "id": "sXwpnbmPzFng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Activation):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        Parameters:\n",
        "            X: input array\n",
        "        Returns:\n",
        "            activated output\n",
        "        \"\"\"\n",
        "        self.output = 1 / (1 + np.exp(-X))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        Parameters:\n",
        "            dZ: gradient from next layer\n",
        "        Returns:\n",
        "            gradient to previous layer\n",
        "        \"\"\"\n",
        "        return dZ * self.output * (1 - self.output)"
      ],
      "metadata": {
        "id": "mLmJKA1XzLSA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sofmax with Entropy (Combined)"
      ],
      "metadata": {
        "id": "M4xBnjhqzQ27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxWithCrossEntropy(Activation):\n",
        "    \"\"\"\n",
        "    Softmax activation combined with cross-entropy loss\n",
        "    for more efficient backpropagation\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass with numerical stability\n",
        "        Parameters:\n",
        "            X: input array\n",
        "        Returns:\n",
        "            softmax probabilities\n",
        "        \"\"\"\n",
        "        # Numerically stable softmax\n",
        "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, Y):\n",
        "        \"\"\"\n",
        "        Backward pass combining softmax and cross-entropy\n",
        "        Parameters:\n",
        "            Y: one-hot encoded true labels\n",
        "        Returns:\n",
        "            gradient to previous layer\n",
        "        \"\"\"\n",
        "        batch_size = Y.shape[0]\n",
        "        return (self.output - Y) / batch_size"
      ],
      "metadata": {
        "id": "uCWbhWVczVfO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "1. **Standard Activations**:\n",
        "   - Implemented Tanh, ReLU, and Sigmoid with proper forward/backward passes\n",
        "   - Each stores necessary information during forward pass for efficient backpropagation\n",
        "\n",
        "2. **Softmax-CrossEntropy Combination**:\n",
        "   - Special combined implementation that simplifies the gradient calculation\n",
        "   - More numerically stable implementation of softmax\n",
        "   - Returns ∂L/∂Z directly during backward pass (rather than ∂L/∂A)\n",
        "\n",
        "3. **Numerical Stability**:\n",
        "   - Softmax implementation subtracts max value before exponentiation\n",
        "   - Prevents overflow/underflow issues\n",
        "\n",
        "4. **Consistent Interface**:\n",
        "   - All activations follow the same interface pattern\n",
        "   - Makes them interchangeable in network architectures\n",
        "\n",
        "## Usage Example:\n",
        "\n",
        "```python\n",
        "  # Using standard activation\n",
        "  relu = ReLU()\n",
        "  A = relu.forward(Z)  # Forward pass\n",
        "  dZ = relu.backward(dA)  # Backward pass\n",
        "\n",
        "  # Using softmax-crossentropy\n",
        "  softmax_ce = SoftmaxWithCrossEntropy()\n",
        "  output = softmax_ce.forward(final_layer_output)  # Get probabilities\n",
        "  loss = -np.mean(np.log(output[np.arange(batch_size), y_true]))  # Calculate loss\n",
        "  dZ = softmax_ce.backward(y_one_hot)  # Get gradient\n",
        "```\n",
        "\n",
        "This implementation provides a clean, modular way to handle different activation functions while optimizing the special case of softmax with cross-entropy loss for more efficient training."
      ],
      "metadata": {
        "id": "tIQ7kAo-x1l3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 5] ReLU class creation"
      ],
      "metadata": {
        "id": "v5uPGCQrztgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) Activation Function\n",
        "\n",
        "    ReLU is defined as:\n",
        "        f(x) = max(0, x)\n",
        "\n",
        "    The derivative for backpropagation is:\n",
        "        f'(x) = 1 if x > 0 else 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.X = None  # To store input during forward pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass of ReLU activation\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy.ndarray\n",
        "            Input array of any shape\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Output after applying ReLU activation\n",
        "        \"\"\"\n",
        "        self.X = X  # Store input for backward pass\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Backward pass of ReLU activation\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dZ : numpy.ndarray\n",
        "            Gradient from the next layer\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Gradient to be propagated to previous layer\n",
        "        \"\"\"\n",
        "        # Create mask where X > 0\n",
        "        mask = (self.X > 0).astype(float)\n",
        "        return dZ * mask"
      ],
      "metadata": {
        "id": "MAhK6GTtxDqJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features:\n",
        "\n",
        "1. **Forward Pass**:\n",
        "   - Implements the ReLU function: f(x) = max(0, x)\n",
        "   - Uses `np.maximum` for efficient array operations\n",
        "   - Stores input X for use in backward pass\n",
        "\n",
        "2. **Backward Pass**:\n",
        "   - Computes gradient as 1 where input was positive, 0 otherwise\n",
        "   - Uses stored input to create a binary mask\n",
        "   - Multiplies incoming gradient (dZ) by this mask\n",
        "\n",
        "3. **Numerical Stability**:\n",
        "   - Avoids issues with vanishing gradients (unlike sigmoid/tanh)\n",
        "   - Simple computation with no exponential operations\n",
        "\n",
        "4. **Efficiency**:\n",
        "   - Uses vectorized operations for fast computation\n",
        "   - Minimal memory overhead (only stores input)\n",
        "\n",
        "## Usage Example:\n",
        "\n",
        "```python\n",
        "  # Create ReLU activation\n",
        "  relu = ReLU()\n",
        "\n",
        "  # Forward pass example\n",
        "  X = np.array([[-1, 0.5, 2], [0, -0.3, 1]])\n",
        "  output = relu.forward(X)\n",
        "  # output = [[0, 0.5, 2], [0, 0, 1]]\n",
        "\n",
        "  # Backward pass example\n",
        "  dZ = np.array([[0.1, -0.2, 0.3], [-0.4, 0.5, -0.6]])\n",
        "  dX = relu.backward(dZ)\n",
        "  # dX will be [[0, -0.2, 0.3], [0, 0, -0.6]] if original X was positive at those positions\n",
        "```\n",
        "\n",
        "## Implementation Notes:\n",
        "\n",
        "1. The derivative at x=0 is technically undefined, but we follow the common practice of treating it as 0.\n",
        "\n",
        "2. The implementation handles both positive and negative inputs correctly:\n",
        "   - Positive inputs pass through unchanged during forward pass\n",
        "   - Negative inputs become 0 during forward pass\n",
        "   - Only positive inputs propagate gradients during backward pass\n",
        "\n",
        "3. The class maintains minimal state (only storing the input X) to enable proper backpropagation while keeping memory usage low.\n",
        "\n",
        "4. The implementation works with arrays of any shape, making it suitable for all layers in a neural network.\n",
        "\n",
        "This ReLU implementation provides the standard functionality needed for modern neural networks, with the added benefits of numerical stability and computational efficiency compared to sigmoid/tanh activations."
      ],
      "metadata": {
        "id": "k8KHIYtgz9z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 6] Initial value of weight\n",
        "\n",
        "####Xavier and He Initializers Implementation\n",
        "Here are the implementations of the Xavier/Glorot and He initializers for neural network weights:\n",
        "\n",
        "####1.Xavier/Glorot Initializer\n"
      ],
      "metadata": {
        "id": "-uD5AzO40P8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier/Glorot initialization for weights\n",
        "    (Good for sigmoid/tanh activations)\n",
        "\n",
        "    Standard deviation σ = 1/√n\n",
        "    where n is number of input nodes\n",
        "\n",
        "    Reference:\n",
        "    Glorot & Bengio, 2010\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initialize weights with Xavier/Glorot method\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_nodes1 : int\n",
        "            Number of nodes in previous layer\n",
        "        n_nodes2 : int\n",
        "            Number of nodes in next layer\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Initialized weight matrix\n",
        "        \"\"\"\n",
        "        sigma = 1 / np.sqrt(n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initialize biases as zeros\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_nodes2 : int\n",
        "            Number of nodes in next layer\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Zero-initialized bias vector\n",
        "        \"\"\"\n",
        "        return np.zeros(n_nodes2)"
      ],
      "metadata": {
        "id": "BY3POq9_zvrT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. He initializer"
      ],
      "metadata": {
        "id": "dRSPz8km1WyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    He initialization for weights\n",
        "    (Good for ReLU activations)\n",
        "\n",
        "    Standard deviation σ = √(2/n)\n",
        "    where n is number of input nodes\n",
        "\n",
        "    Reference:\n",
        "    He et al., 2015\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initialize weights with He method\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_nodes1 : int\n",
        "            Number of nodes in previous layer\n",
        "        n_nodes2 : int\n",
        "            Number of nodes in next layer\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Initialized weight matrix\n",
        "        \"\"\"\n",
        "        sigma = np.sqrt(2 / n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initialize biases as zeros\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_nodes2 : int\n",
        "            Number of nodes in next layer\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Zero-initialized bias vector\n",
        "        \"\"\"\n",
        "        return np.zeros(n_nodes2)"
      ],
      "metadata": {
        "id": "c_kKo0Jb1FSK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features:\n",
        "\n",
        "1. **Xavier/Glorot Initialization**:\n",
        "   - Designed for sigmoid/tanh activations\n",
        "   - σ = 1/√n where n is input dimension\n",
        "   - Maintains variance of activations across layers\n",
        "\n",
        "2. **He Initialization**:\n",
        "   - Designed for ReLU activations\n",
        "   - σ = √(2/n) where n is input dimension\n",
        "   - Accounts for ReLU's zeroing of half the activations\n",
        "\n",
        "3. **Common Interface**:\n",
        "   - Both implement W() and B() methods\n",
        "   - Can be used interchangeably with the FC layer class\n",
        "   - Follow same pattern as SimpleInitializer\n",
        "\n",
        "4. **Bias Initialization**:\n",
        "   - Both initialize biases to zero (common practice)\n",
        "\n",
        "## Usage Example:\n",
        "\n",
        "```python\n",
        "  # For a network with tanh activations\n",
        "  xavier_init = XavierInitializer()\n",
        "  fc_layer1 = FC(784, 256, xavier_init, optimizer)\n",
        "\n",
        "  # For a network with ReLU activations\n",
        "  he_init = HeInitializer()\n",
        "  fc_layer2 = FC(256, 128, he_init, optimizer)\n",
        "```\n",
        "\n",
        "## Mathematical Justification:\n",
        "\n",
        "1. **Xavier Initialization**:\n",
        "   - Variance should be 1/n to maintain activation magnitudes\n",
        "   - Derived by considering the variance of the sum of n inputs\n",
        "\n",
        "2. **He Initialization**:\n",
        "   - Accounts for ReLU killing half the activations\n",
        "   - Uses √2/n to compensate for the lost variance\n",
        "   - Empirical results show better performance with ReLU\n",
        "\n",
        "These initializers help prevent vanishing/exploding gradients in deep networks by maintaining appropriate activation variances across layers."
      ],
      "metadata": {
        "id": "3sOQyOz31jrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 7] Optimization method\n",
        "####AdaGrad Optimizer Implementation"
      ],
      "metadata": {
        "id": "iWX4N1nr17as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad (Adaptive Gradient) optimization method\n",
        "\n",
        "    Adapts learning rates individually for each parameter by dividing\n",
        "    by the square root of accumulated squared gradients.\n",
        "\n",
        "    Reference:\n",
        "    Duchi et al., 2011\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, epsilon=1e-8):\n",
        "        \"\"\"\n",
        "        Initialize AdaGrad optimizer\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        lr : float\n",
        "            Base learning rate (default: 0.01)\n",
        "        epsilon : float\n",
        "            Small constant for numerical stability (default: 1e-8)\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.h_W = None  # Accumulated squared gradients for weights\n",
        "        self.h_B = None  # Accumulated squared gradients for biases\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update layer parameters using AdaGrad\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        layer : FC\n",
        "            The fully connected layer instance containing:\n",
        "                - W: weight matrix\n",
        "                - B: bias vector\n",
        "                - dW: weight gradients\n",
        "                - dB: bias gradients\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        FC\n",
        "            The updated layer\n",
        "        \"\"\"\n",
        "        # Initialize accumulated gradients if first run\n",
        "        if self.h_W is None:\n",
        "            self.h_W = np.zeros_like(layer.W)\n",
        "            self.h_B = np.zeros_like(layer.B)\n",
        "\n",
        "        # Accumulate squared gradients\n",
        "        self.h_W += layer.dW ** 2\n",
        "        self.h_B += layer.dB ** 2\n",
        "\n",
        "        # Update weights with adaptive learning rate\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h_W) + self.epsilon)\n",
        "\n",
        "        # Update biases with adaptive learning rate\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.h_B) + self.epsilon)\n",
        "\n",
        "        return layer"
      ],
      "metadata": {
        "id": "07zGCCkz1cmC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features:\n",
        "\n",
        "1. **Per-Parameter Learning Rates**:\n",
        "   - Maintains separate learning rates for each parameter\n",
        "   - Adapts based on historical gradient information\n",
        "\n",
        "2. **Gradient Accumulation**:\n",
        "   - Tracks sum of squared gradients (h_W and h_B)\n",
        "   - Automatically initializes on first update\n",
        "\n",
        "3. **Adaptive Updates**:\n",
        "   - Learning rate is divided by √(sum of squared gradients)\n",
        "   - Parameters with large gradients get smaller updates\n",
        "   - Parameters with small gradients get larger updates\n",
        "\n",
        "4. **Numerical Stability**:\n",
        "   - Adds small epsilon (1e-8) to prevent division by zero\n",
        "   - Standard practice in adaptive methods\n",
        "\n",
        "5. **Hyperparameters**:\n",
        "   - `lr`: Base learning rate (typical default: 0.01)\n",
        "   - `epsilon`: Small constant (typical default: 1e-8)\n",
        "\n",
        "## Mathematical Formulation:\n",
        "\n",
        "For weights W at time t:\n",
        "1. Accumulate squared gradients:\n",
        "   ```math\n",
        "   h_W^{(t)} = h_W^{(t-1)} + (\\frac{\\partial L}{\\partial W})^2\n",
        "   ```\n",
        "2. Update rule:\n",
        "   ```math\n",
        "   W^{(t+1)} = W^{(t)} - \\frac{\\alpha}{\\sqrt{h_W^{(t)}} + \\epsilon} \\cdot \\frac{\\partial L}{\\partial W}\n",
        "   ```\n",
        "\n",
        "Same applies for biases B.\n",
        "\n",
        "## Usage Example:\n",
        "\n",
        "```python\n",
        "  # Create AdaGrad optimizer with learning rate 0.01\n",
        "  optimizer = AdaGrad(lr=0.01)\n",
        "\n",
        "  # During training:\n",
        "  # layer = FC(...)  # Create layer\n",
        "  # A = layer.forward(X)  # Forward pass\n",
        "  # dZ = layer.backward(dA)  # Backward pass (computes dW, dB)\n",
        "  # layer = optimizer.update(layer)  # Update parameters with AdaGrad\n",
        "```\n",
        "\n",
        "## Advantages over SGD:\n",
        "\n",
        "1. **Automatic Learning Rate Tuning**:\n",
        "   - No need to manually decay learning rate\n",
        "   - Automatically adapts to gradient landscape\n",
        "\n",
        "2. **Better for Sparse Gradients**:\n",
        "   - Works well when some features are rarely updated\n",
        "   - Common in natural language processing and recommendation systems\n",
        "\n",
        "3. **Theoretical Guarantees**:\n",
        "   - Proven convergence for convex problems\n",
        "   - Works well in practice for non-convex neural networks\n",
        "\n",
        "Note: While AdaGrad is an important foundational adaptive method, modern deep learning often uses more sophisticated variants like RMSprop or Adam which address AdaGrad's tendency to make learning rates too small too quickly."
      ],
      "metadata": {
        "id": "9GTMs65E2uWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[Problem 8] Class completion\n",
        "Complete the Scratch Deep Neural Netrowk Classifier class that can be trained and estimated with any configuration.\n",
        "\n",
        "####Scratch Deep Neural Network Classifier\n",
        "Here's the complete implementation of the modular deep neural network classifier:"
      ],
      "metadata": {
        "id": "vb4w90_c3HDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class ScratchDeepNeuralNetworkClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Modular Deep Neural Network Classifier implemented from scratch\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    hidden_layer_sizes : tuple\n",
        "        Number of nodes in each hidden layer\n",
        "    activation : str\n",
        "        Activation function ('relu', 'tanh', 'sigmoid')\n",
        "    initializer : str\n",
        "        Weight initializer ('xavier', 'he', 'simple')\n",
        "    optimizer : str\n",
        "        Optimization method ('sgd', 'adagrad')\n",
        "    learning_rate : float\n",
        "        Learning rate for optimization\n",
        "    sigma : float\n",
        "        Standard deviation for simple initializer\n",
        "    epochs : int\n",
        "        Number of training epochs\n",
        "    batch_size : int\n",
        "        Size of mini-batches\n",
        "    verbose : bool\n",
        "        Whether to print training progress\n",
        "    epsilon : float\n",
        "        Small constant for numerical stability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_layer_sizes=(400,), activation='relu',\n",
        "                 initializer='he', optimizer='sgd', learning_rate=0.01,\n",
        "                 sigma=0.01, epochs=10, batch_size=32, verbose=True, epsilon=1e-8):\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation = activation\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.sigma = sigma\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.epsilon = epsilon\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "\n",
        "    def _initialize_network(self, n_features, n_classes):\n",
        "        \"\"\"Initialize network architecture with given parameters\"\"\"\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "\n",
        "        # Select initializer\n",
        "        if self.initializer == 'xavier':\n",
        "            initializer = XavierInitializer()\n",
        "        elif self.initializer == 'he':\n",
        "            initializer = HeInitializer()\n",
        "        else:\n",
        "            initializer = SimpleInitializer(self.sigma)\n",
        "\n",
        "        # Select optimizer\n",
        "        if self.optimizer == 'adagrad':\n",
        "            optimizer = AdaGrad(self.learning_rate, self.epsilon)\n",
        "        else:\n",
        "            optimizer = SGD(self.learning_rate)\n",
        "\n",
        "        # Select activation\n",
        "        activation_class = {\n",
        "            'relu': ReLU,\n",
        "            'tanh': Tanh,\n",
        "            'sigmoid': Sigmoid\n",
        "        }.get(self.activation, ReLU)\n",
        "\n",
        "        # Build hidden layers\n",
        "        n_nodes_prev = n_features\n",
        "        for n_nodes in self.hidden_layer_sizes:\n",
        "            self.layers.append(FC(n_nodes_prev, n_nodes, initializer, optimizer))\n",
        "            self.activations.append(activation_class())\n",
        "            n_nodes_prev = n_nodes\n",
        "\n",
        "        # Add output layer\n",
        "        self.layers.append(FC(n_nodes_prev, n_classes, initializer, optimizer))\n",
        "        self.activations.append(SoftmaxWithCrossEntropy())\n",
        "\n",
        "    def _forward_propagation(self, X):\n",
        "        \"\"\"Perform forward pass through all layers\"\"\"\n",
        "        A = X\n",
        "        for layer, activation in zip(self.layers, self.activations):\n",
        "            Z = layer.forward(A)\n",
        "            A = activation.forward(Z)\n",
        "        return A\n",
        "\n",
        "    def _backward_propagation(self, X, y):\n",
        "        \"\"\"Perform backward pass through all layers\"\"\"\n",
        "        # Forward pass to store activations\n",
        "        self._forward_propagation(X)\n",
        "\n",
        "        # Backward pass\n",
        "        dA = self.activations[-1].backward(y)\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            dZ = self.layers[i].backward(dA)\n",
        "            if i > 0:  # Don't need to backpropagate beyond first layer\n",
        "                dA = self.activations[i-1].backward(dZ)\n",
        "\n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"Compute cross-entropy loss\"\"\"\n",
        "        y_pred = self._forward_propagation(X)\n",
        "        return -np.mean(np.log(y_pred[np.arange(len(y)), np.argmax(y, axis=1)]))\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train the deep neural network\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : numpy.ndarray\n",
        "            Training features\n",
        "        y : numpy.ndarray\n",
        "            Training labels (one-hot encoded)\n",
        "        X_val : numpy.ndarray, optional\n",
        "            Validation features\n",
        "        y_val : numpy.ndarray, optional\n",
        "            Validation labels (one-hot encoded)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = y.shape[1]\n",
        "\n",
        "        self._initialize_network(n_features, n_classes)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Shuffle and create mini-batches\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for i in range(0, n_samples, self.batch_size):\n",
        "                X_batch = X_shuffled[i:i+self.batch_size]\n",
        "                y_batch = y_shuffled[i:i+self.batch_size]\n",
        "\n",
        "                # Forward and backward pass\n",
        "                self._backward_propagation(X_batch, y_batch)\n",
        "\n",
        "            # Calculate and store losses\n",
        "            train_loss = self._compute_loss(X, y)\n",
        "            self.train_loss_history.append(train_loss)\n",
        "\n",
        "            if X_val is not None:\n",
        "                val_loss = self._compute_loss(X_val, y_val)\n",
        "                self.val_loss_history.append(val_loss)\n",
        "\n",
        "            # Print training progress\n",
        "            if self.verbose:\n",
        "                msg = f\"Epoch {epoch+1}/{self.epochs}, Train Loss: {train_loss:.4f}\"\n",
        "                if X_val is not None:\n",
        "                    msg += f\", Val Loss: {val_loss:.4f}\"\n",
        "                print(msg)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make class predictions\"\"\"\n",
        "        y_pred = self._forward_propagation(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Make probability predictions\"\"\"\n",
        "        return self._forward_propagation(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"Calculate accuracy score\"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "        return accuracy_score(np.argmax(y, axis=1), y_pred)\n",
        "\n",
        "    def plot_learning_curve(self):\n",
        "        \"\"\"Plot training and validation loss curves\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, self.epochs+1), self.train_loss_history, label='Training Loss')\n",
        "        if self.val_loss_history:\n",
        "            plt.plot(range(1, self.epochs+1), self.val_loss_history, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Learning Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "_Ns-QoLy2pDp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Features:\n",
        "\n",
        "1. **Modular Architecture**:\n",
        "   - Supports arbitrary number of hidden layers\n",
        "   - Configurable activation functions (ReLU, tanh, sigmoid)\n",
        "   - Multiple initialization methods (Xavier, He, Simple)\n",
        "   - Different optimizers (SGD, AdaGrad)\n",
        "\n",
        "2. **Training Capabilities**:\n",
        "   - Mini-batch gradient descent\n",
        "   - Learning curve tracking\n",
        "   - Validation monitoring\n",
        "   - Early stopping capability (can be added)\n",
        "\n",
        "3. **Scikit-learn Compatibility**:\n",
        "   - Inherits from BaseEstimator and ClassifierMixin\n",
        "   - Implements standard scikit-learn interface (fit, predict, score)\n",
        "\n",
        "4. **Visualization**:\n",
        "   - Built-in learning curve plotting\n",
        "   - Training progress monitoring\n",
        "\n",
        "5. **Extensibility**:\n",
        "   - Easy to add new activation functions\n",
        "   - Simple to implement additional optimizers\n",
        "   - Straightforward to incorporate new layer types\n",
        "\n",
        "## Usage Example:\n",
        "\n",
        "```python\n",
        "  # Example usage:\n",
        "  from sklearn.datasets import make_classification\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  # Create synthetic data\n",
        "  X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=10)\n",
        "  y = OneHotEncoder(sparse=False).fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "  # Split data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  # Create and train network\n",
        "  dnn = ScratchDeepNeuralNetworkClassifier(\n",
        "      hidden_layer_sizes=(64, 32),\n",
        "      activation='relu',\n",
        "      initializer='he',\n",
        "      optimizer='adagrad',\n",
        "      learning_rate=0.01,\n",
        "      epochs=50,\n",
        "      batch_size=32,\n",
        "      verbose=True\n",
        "  )\n",
        "\n",
        "  dnn.fit(X_train, y_train, X_test, y_test)\n",
        "\n",
        "  # Evaluate\n",
        "  print(f\"Test Accuracy: {dnn.score(X_test, y_test):.4f}\")\n",
        "\n",
        "  # Plot learning curve\n",
        "  dnn.plot_learning_curve()\n",
        "```\n",
        "\n",
        "This implementation provides a complete, modular deep neural network that can be configured with different architectures and training parameters while maintaining clean, object-oriented design principles."
      ],
      "metadata": {
        "id": "i_QQjjp-3uL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Verification\n",
        "###[Problem 9] Learning and estimation\n",
        "Create several networks with varying numbers of layers and activation functions. Then, train and estimate the MNIST data and calculate the Accuracy.\n",
        "\n",
        "####Network Architecture Comparison on MNIST\n",
        "Let's evaluate different network configurations on the MNIST dataset to compare their performance.\n",
        "\n",
        "Implementation"
      ],
      "metadata": {
        "id": "gEUnCa9w4DY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad (Adaptive Gradient) optimization method\n",
        "\n",
        "    Adapts learning rates individually for each parameter by dividing\n",
        "    by the square root of accumulated squared gradients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, epsilon=1e-8):\n",
        "        \"\"\"\n",
        "        Initialize AdaGrad optimizer\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        lr : float\n",
        "            Base learning rate (default: 0.01)\n",
        "        epsilon : float\n",
        "            Small constant for numerical stability (default: 1e-8)\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.h = {}  # Dictionary to store accumulated gradients for each layer\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update layer parameters using AdaGrad\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        layer : FC\n",
        "            The fully connected layer instance containing:\n",
        "                - W: weight matrix\n",
        "                - B: bias vector\n",
        "                - dW: weight gradients\n",
        "                - dB: bias gradients\n",
        "                - layer_id: unique identifier for the layer\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        FC\n",
        "            The updated layer\n",
        "        \"\"\"\n",
        "        # Initialize accumulated gradients if first run for this layer\n",
        "        if not hasattr(layer, 'layer_id'):\n",
        "            layer.layer_id = id(layer)  # Create unique identifier for the layer\n",
        "\n",
        "        if layer.layer_id not in self.h:\n",
        "            self.h[layer.layer_id] = {\n",
        "                'h_W': np.zeros_like(layer.W),\n",
        "                'h_B': np.zeros_like(layer.B)\n",
        "            }\n",
        "\n",
        "        # Get references to this layer's accumulators\n",
        "        h_W = self.h[layer.layer_id]['h_W']\n",
        "        h_B = self.h[layer.layer_id]['h_B']\n",
        "\n",
        "        # Accumulate squared gradients\n",
        "        h_W += layer.dW ** 2\n",
        "        h_B += layer.dB ** 2\n",
        "\n",
        "        # Update weights with adaptive learning rate\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(h_W) + self.epsilon)\n",
        "\n",
        "        # Update biases with adaptive learning rate\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(h_B) + self.epsilon)\n",
        "\n",
        "        return layer"
      ],
      "metadata": {
        "id": "WTDpWV7kDUsk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load MNIST data\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X = mnist.data.astype(np.float32) / 255.0\n",
        "y = mnist.target.astype(np.int32)\n",
        "\n",
        "# One-hot encode labels\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "y_onehot = enc.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split into train, validation and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=10000, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000, random_state=42)\n",
        "\n",
        "# Define network configurations to test\n",
        "configurations = [\n",
        "    {\n",
        "        'name': 'Shallow ReLU (1 hidden)',\n",
        "        'hidden_layer_sizes': (400,),\n",
        "        'activation': 'relu',\n",
        "        'initializer': 'he',\n",
        "        'optimizer': 'sgd'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Deep ReLU (3 hidden)',\n",
        "        'hidden_layer_sizes': (400, 200, 100),\n",
        "        'activation': 'relu',\n",
        "        'initializer': 'he',\n",
        "        'optimizer': 'sgd'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Shallow Tanh (1 hidden)',\n",
        "        'hidden_layer_sizes': (400,),\n",
        "        'activation': 'tanh',\n",
        "        'initializer': 'xavier',\n",
        "        'optimizer': 'sgd'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Deep Tanh (3 hidden)',\n",
        "        'hidden_layer_sizes': (400, 200, 100),\n",
        "        'activation': 'tanh',\n",
        "        'initializer': 'xavier',\n",
        "        'optimizer': 'sgd'\n",
        "    },\n",
        "    {\n",
        "        'name': 'AdaGrad ReLU (2 hidden)',\n",
        "        'hidden_layer_sizes': (400, 200),\n",
        "        'activation': 'relu',\n",
        "        'initializer': 'he',\n",
        "        'optimizer': 'adagrad'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Train and evaluate each configuration\n",
        "results = []\n",
        "for config in configurations:\n",
        "    print(f\"\\nTraining {config['name']}...\")\n",
        "\n",
        "    # Create network\n",
        "    dnn = ScratchDeepNeuralNetworkClassifier(\n",
        "        hidden_layer_sizes=config['hidden_layer_sizes'],\n",
        "        activation=config['activation'],\n",
        "        initializer=config['initializer'],\n",
        "        optimizer=config['optimizer'],\n",
        "        learning_rate=0.01,\n",
        "        epochs=15,\n",
        "        batch_size=32,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    dnn.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Evaluate\n",
        "    train_acc = dnn.score(X_train, y_train)\n",
        "    val_acc = dnn.score(X_val, y_val)\n",
        "    test_acc = dnn.score(X_test, y_test)\n",
        "\n",
        "    results.append({\n",
        "        'name': config['name'],\n",
        "        'train_acc': train_acc,\n",
        "        'val_acc': val_acc,\n",
        "        'test_acc': test_acc\n",
        "    })\n",
        "\n",
        "    print(f\"{config['name']} - Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n=== Final Results ===\")\n",
        "for result in results:\n",
        "    print(f\"{result['name']}:\")\n",
        "    print(f\"  Train Accuracy: {result['train_acc']:.4f}\")\n",
        "    print(f\"  Val Accuracy:   {result['val_acc']:.4f}\")\n",
        "    print(f\"  Test Accuracy:  {result['test_acc']:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5kUT9fR3pNg",
        "outputId": "c307e169-3dba-4716-fdd0-d1d72ae5397a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Shallow ReLU (1 hidden)...\n",
            "Epoch 1/15, Train Loss: 1.8461, Val Loss: 1.8529\n",
            "Epoch 2/15, Train Loss: 1.4531, Val Loss: 1.4617\n",
            "Epoch 3/15, Train Loss: 1.1885, Val Loss: 1.1976\n",
            "Epoch 4/15, Train Loss: 1.0090, Val Loss: 1.0181\n",
            "Epoch 5/15, Train Loss: 0.8838, Val Loss: 0.8924\n",
            "Epoch 6/15, Train Loss: 0.7932, Val Loss: 0.8015\n",
            "Epoch 7/15, Train Loss: 0.7251, Val Loss: 0.7329\n",
            "Epoch 8/15, Train Loss: 0.6725, Val Loss: 0.6798\n",
            "Epoch 9/15, Train Loss: 0.6307, Val Loss: 0.6378\n",
            "Epoch 10/15, Train Loss: 0.5967, Val Loss: 0.6034\n",
            "Epoch 11/15, Train Loss: 0.5685, Val Loss: 0.5749\n",
            "Epoch 12/15, Train Loss: 0.5447, Val Loss: 0.5509\n",
            "Epoch 13/15, Train Loss: 0.5244, Val Loss: 0.5304\n",
            "Epoch 14/15, Train Loss: 0.5068, Val Loss: 0.5125\n",
            "Epoch 15/15, Train Loss: 0.4914, Val Loss: 0.4970\n",
            "Shallow ReLU (1 hidden) - Test Accuracy: 0.8752\n",
            "\n",
            "Training Deep ReLU (3 hidden)...\n",
            "Epoch 1/15, Train Loss: 1.8539, Val Loss: 1.8582\n",
            "Epoch 2/15, Train Loss: 1.3786, Val Loss: 1.3867\n",
            "Epoch 3/15, Train Loss: 1.0196, Val Loss: 1.0282\n",
            "Epoch 4/15, Train Loss: 0.8019, Val Loss: 0.8102\n",
            "Epoch 5/15, Train Loss: 0.6705, Val Loss: 0.6789\n",
            "Epoch 6/15, Train Loss: 0.5862, Val Loss: 0.5942\n",
            "Epoch 7/15, Train Loss: 0.5274, Val Loss: 0.5358\n",
            "Epoch 8/15, Train Loss: 0.4846, Val Loss: 0.4932\n",
            "Epoch 9/15, Train Loss: 0.4521, Val Loss: 0.4604\n",
            "Epoch 10/15, Train Loss: 0.4263, Val Loss: 0.4346\n",
            "Epoch 11/15, Train Loss: 0.4058, Val Loss: 0.4144\n",
            "Epoch 12/15, Train Loss: 0.3885, Val Loss: 0.3970\n",
            "Epoch 13/15, Train Loss: 0.3742, Val Loss: 0.3824\n",
            "Epoch 14/15, Train Loss: 0.3625, Val Loss: 0.3714\n",
            "Epoch 15/15, Train Loss: 0.3509, Val Loss: 0.3590\n",
            "Deep ReLU (3 hidden) - Test Accuracy: 0.9015\n",
            "\n",
            "Training Shallow Tanh (1 hidden)...\n",
            "Epoch 1/15, Train Loss: 1.7637, Val Loss: 1.7665\n",
            "Epoch 2/15, Train Loss: 1.4107, Val Loss: 1.4146\n",
            "Epoch 3/15, Train Loss: 1.1758, Val Loss: 1.1803\n",
            "Epoch 4/15, Train Loss: 1.0162, Val Loss: 1.0207\n",
            "Epoch 5/15, Train Loss: 0.9035, Val Loss: 0.9080\n",
            "Epoch 6/15, Train Loss: 0.8209, Val Loss: 0.8251\n",
            "Epoch 7/15, Train Loss: 0.7580, Val Loss: 0.7621\n",
            "Epoch 8/15, Train Loss: 0.7087, Val Loss: 0.7126\n",
            "Epoch 9/15, Train Loss: 0.6690, Val Loss: 0.6727\n",
            "Epoch 10/15, Train Loss: 0.6363, Val Loss: 0.6399\n",
            "Epoch 11/15, Train Loss: 0.6090, Val Loss: 0.6124\n",
            "Epoch 12/15, Train Loss: 0.5857, Val Loss: 0.5890\n",
            "Epoch 13/15, Train Loss: 0.5657, Val Loss: 0.5689\n",
            "Epoch 14/15, Train Loss: 0.5483, Val Loss: 0.5513\n",
            "Epoch 15/15, Train Loss: 0.5329, Val Loss: 0.5358\n",
            "Shallow Tanh (1 hidden) - Test Accuracy: 0.8654\n",
            "\n",
            "Training Deep Tanh (3 hidden)...\n",
            "Epoch 1/15, Train Loss: 1.7948, Val Loss: 1.7990\n",
            "Epoch 2/15, Train Loss: 1.4177, Val Loss: 1.4234\n",
            "Epoch 3/15, Train Loss: 1.1488, Val Loss: 1.1548\n",
            "Epoch 4/15, Train Loss: 0.9673, Val Loss: 0.9732\n",
            "Epoch 5/15, Train Loss: 0.8432, Val Loss: 0.8489\n",
            "Epoch 6/15, Train Loss: 0.7546, Val Loss: 0.7600\n",
            "Epoch 7/15, Train Loss: 0.6885, Val Loss: 0.6938\n",
            "Epoch 8/15, Train Loss: 0.6375, Val Loss: 0.6425\n",
            "Epoch 9/15, Train Loss: 0.5969, Val Loss: 0.6018\n",
            "Epoch 10/15, Train Loss: 0.5638, Val Loss: 0.5684\n",
            "Epoch 11/15, Train Loss: 0.5364, Val Loss: 0.5408\n",
            "Epoch 12/15, Train Loss: 0.5132, Val Loss: 0.5176\n",
            "Epoch 13/15, Train Loss: 0.4935, Val Loss: 0.4977\n",
            "Epoch 14/15, Train Loss: 0.4764, Val Loss: 0.4805\n",
            "Epoch 15/15, Train Loss: 0.4616, Val Loss: 0.4655\n",
            "Deep Tanh (3 hidden) - Test Accuracy: 0.8800\n",
            "\n",
            "Training AdaGrad ReLU (2 hidden)...\n",
            "Epoch 1/15, Train Loss: 0.0914, Val Loss: 0.1103\n",
            "Epoch 2/15, Train Loss: 0.0764, Val Loss: 0.1084\n",
            "Epoch 3/15, Train Loss: 0.0421, Val Loss: 0.0773\n",
            "Epoch 4/15, Train Loss: 0.0349, Val Loss: 0.0789\n",
            "Epoch 5/15, Train Loss: 0.0223, Val Loss: 0.0680\n",
            "Epoch 6/15, Train Loss: 0.0180, Val Loss: 0.0666\n",
            "Epoch 7/15, Train Loss: 0.0144, Val Loss: 0.0657\n",
            "Epoch 8/15, Train Loss: 0.0118, Val Loss: 0.0652\n",
            "Epoch 9/15, Train Loss: 0.0090, Val Loss: 0.0641\n",
            "Epoch 10/15, Train Loss: 0.0074, Val Loss: 0.0637\n",
            "Epoch 11/15, Train Loss: 0.0062, Val Loss: 0.0639\n",
            "Epoch 12/15, Train Loss: 0.0055, Val Loss: 0.0646\n",
            "Epoch 13/15, Train Loss: 0.0046, Val Loss: 0.0651\n",
            "Epoch 14/15, Train Loss: 0.0040, Val Loss: 0.0647\n",
            "Epoch 15/15, Train Loss: 0.0036, Val Loss: 0.0656\n",
            "AdaGrad ReLU (2 hidden) - Test Accuracy: 0.9801\n",
            "\n",
            "=== Final Results ===\n",
            "Shallow ReLU (1 hidden):\n",
            "  Train Accuracy: 0.8795\n",
            "  Val Accuracy:   0.8745\n",
            "  Test Accuracy:  0.8752\n",
            "\n",
            "Deep ReLU (3 hidden):\n",
            "  Train Accuracy: 0.9035\n",
            "  Val Accuracy:   0.9004\n",
            "  Test Accuracy:  0.9015\n",
            "\n",
            "Shallow Tanh (1 hidden):\n",
            "  Train Accuracy: 0.8676\n",
            "  Val Accuracy:   0.8687\n",
            "  Test Accuracy:  0.8654\n",
            "\n",
            "Deep Tanh (3 hidden):\n",
            "  Train Accuracy: 0.8820\n",
            "  Val Accuracy:   0.8807\n",
            "  Test Accuracy:  0.8800\n",
            "\n",
            "AdaGrad ReLU (2 hidden):\n",
            "  Train Accuracy: 0.9999\n",
            "  Val Accuracy:   0.9806\n",
            "  Test Accuracy:  0.9801\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expected Results Analysis\n",
        "\n",
        "Based on typical MNIST performance with these architectures, we would expect:\n",
        "\n",
        "1. **Shallow ReLU (1 hidden)**:\n",
        "   - Moderate performance (∼97-98% accuracy)\n",
        "   - Fast training but may underfit slightly\n",
        "\n",
        "2. **Deep ReLU (3 hidden)**:\n",
        "   - Better performance (∼98-98.5% accuracy)\n",
        "   - Slower training but better feature learning\n",
        "\n",
        "3. **Shallow Tanh (1 hidden)**:\n",
        "   - Similar to shallow ReLU but slightly worse (∼96-97%)\n",
        "   - May require more careful initialization\n",
        "\n",
        "4. **Deep Tanh (3 hidden)**:\n",
        "   - Potentially good performance (∼97-98%)\n",
        "   - May suffer from vanishing gradients\n",
        "\n",
        "5. **AdaGrad ReLU (2 hidden)**:\n",
        "   - Competitive performance (∼98%+)\n",
        "   - More stable training than SGD\n",
        "\n",
        "## Key Observations:\n",
        "\n",
        "1. **ReLU vs Tanh**:\n",
        "   - ReLU networks typically outperform tanh on MNIST\n",
        "   - Tanh may require more tuning of learning rates\n",
        "\n",
        "2. **Depth Impact**:\n",
        "   - Deeper networks generally perform better but take longer to train\n",
        "   - Very deep networks may need additional techniques (batch norm, skip connections)\n",
        "\n",
        "3. **Optimizer Comparison**:\n",
        "   - AdaGrad often provides more stable training than SGD\n",
        "   - SGD may require learning rate scheduling for best results\n",
        "\n",
        "4. **Initialization Importance**:\n",
        "   - Proper initialization (He for ReLU, Xavier for tanh) is crucial\n",
        "   - Bad initialization can lead to poor convergence\n",
        "\n",
        "## Recommendations:\n",
        "\n",
        "1. For best accuracy:\n",
        "   - Use ReLU activation with He initialization\n",
        "   - 2-3 hidden layers (400-200-100 architecture)\n",
        "   - Consider Adam optimizer (could be implemented next)\n",
        "\n",
        "2. For faster training:\n",
        "   - Use single hidden layer with ReLU\n",
        "   - Larger batch sizes (64-128)\n",
        "\n",
        "3. For stability:\n",
        "   - Use adaptive optimizers (AdaGrad, RMSprop)\n",
        "   - Add batch normalization between layers\n",
        "\n",
        "This evaluation framework allows systematic comparison of different architectural choices and helps identify the best configuration for a given problem."
      ],
      "metadata": {
        "id": "XO0Og_L-4zw3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7s0iItpI4xKM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}