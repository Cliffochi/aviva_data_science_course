{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtMQuIXR+A//F91qNmvmak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cliffochi/aviva_data_science_course/blob/main/Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Machine translation execution and code reading\n",
        "The following sample code does a short English to French translation. Run it to see the results."
      ],
      "metadata": {
        "id": "ySup2JZySmJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Title: Character-level recurrent sequence-to-sequence model\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2017/09/29\n",
        "Last modified: 2023/11/22\n",
        "Description: Character-level recurrent sequence-to-sequence model.\n",
        "Accelerator: GPU\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Download the data\n",
        "data_path = keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        "    cache_dir='.'\n",
        ")\n",
        "dirpath = Path(data_path).parent.absolute()\n",
        "data_file_path = os.path.join(dirpath, 'fra-eng', 'fra.txt')\n",
        "\n",
        "# Configuration\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "\n",
        "# Prepare the data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "\n",
        "# Bonus: Print first line for verification\n",
        "print(\"Sample line from data file:\", lines[0])\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) >= 2:\n",
        "        input_text, target_text = parts[0], parts[1]\n",
        "        # Use \"\\t\" as start sequence character for target, \"\\n\" as end sequence character\n",
        "        target_text = \"\\t\" + target_text + \"\\n\"\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for char in input_text:\n",
        "            if char not in input_characters:\n",
        "                input_characters.add(char)\n",
        "        for char in target_text:\n",
        "            if char not in target_characters:\n",
        "                target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "if not input_texts:\n",
        "    print(\"Error: No valid input sentences found in the data file.\")\n",
        "else:\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    print(\"Number of samples:\", len(input_texts))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text):\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "    # Build the model\n",
        "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Train the model\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.2,\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model.save(\"s2s_model.keras\")\n",
        "\n",
        "    # Run inference\n",
        "    model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]  # input_2\n",
        "    decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.layers[3]\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h_dec, state_c_dec]\n",
        "    decoder_dense = model.layers[4]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "    def decode_sequence(input_seq):\n",
        "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = decoder_model.predict(\n",
        "                [target_seq] + states_value, verbose=0\n",
        "            )\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "            if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "            target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "            states_value = [h, c]\n",
        "        return decoded_sentence\n",
        "\n",
        "    for seq_index in range(20):\n",
        "        input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "        decoded_sentence = decode_sequence(input_seq)\n",
        "        print(\"-\")\n",
        "        print(\"Input sentence:\", input_texts[seq_index])\n",
        "        print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHWoNdoLcRON",
        "outputId": "f4099841-7ed9-4311-e395-db40ac033036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample line from data file: Go.\tVa !\n",
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 93\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 59\n",
            "Epoch 1/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.6928 - loss: 1.6183 - val_accuracy: 0.6968 - val_loss: 1.1544\n",
            "Epoch 2/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7341 - loss: 1.0054 - val_accuracy: 0.7086 - val_loss: 1.0375\n",
            "Epoch 3/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7536 - loss: 0.8940 - val_accuracy: 0.7332 - val_loss: 0.9398\n",
            "Epoch 4/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7750 - loss: 0.8022 - val_accuracy: 0.7494 - val_loss: 0.8768\n",
            "Epoch 5/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7870 - loss: 0.7492 - val_accuracy: 0.7725 - val_loss: 0.7817\n",
            "Epoch 6/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8075 - loss: 0.6645 - val_accuracy: 0.7804 - val_loss: 0.7475\n",
            "Epoch 7/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8140 - loss: 0.6353 - val_accuracy: 0.7918 - val_loss: 0.7068\n",
            "Epoch 8/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8214 - loss: 0.6076 - val_accuracy: 0.7943 - val_loss: 0.6886\n",
            "Epoch 9/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8289 - loss: 0.5844 - val_accuracy: 0.8044 - val_loss: 0.6623\n",
            "Epoch 10/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8342 - loss: 0.5670 - val_accuracy: 0.8121 - val_loss: 0.6424\n",
            "Epoch 11/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8389 - loss: 0.5501 - val_accuracy: 0.8181 - val_loss: 0.6223\n",
            "Epoch 12/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8451 - loss: 0.5296 - val_accuracy: 0.8223 - val_loss: 0.6067\n",
            "Epoch 13/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8486 - loss: 0.5161 - val_accuracy: 0.8245 - val_loss: 0.5936\n",
            "Epoch 14/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8520 - loss: 0.5032 - val_accuracy: 0.8290 - val_loss: 0.5844\n",
            "Epoch 15/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8566 - loss: 0.4885 - val_accuracy: 0.8315 - val_loss: 0.5740\n",
            "Epoch 16/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8591 - loss: 0.4783 - val_accuracy: 0.8336 - val_loss: 0.5648\n",
            "Epoch 17/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8621 - loss: 0.4666 - val_accuracy: 0.8364 - val_loss: 0.5595\n",
            "Epoch 18/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8638 - loss: 0.4602 - val_accuracy: 0.8402 - val_loss: 0.5428\n",
            "Epoch 19/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8662 - loss: 0.4514 - val_accuracy: 0.8402 - val_loss: 0.5431\n",
            "Epoch 20/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8695 - loss: 0.4402 - val_accuracy: 0.8442 - val_loss: 0.5294\n",
            "Epoch 21/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8708 - loss: 0.4329 - val_accuracy: 0.8440 - val_loss: 0.5280\n",
            "Epoch 22/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8729 - loss: 0.4248 - val_accuracy: 0.8478 - val_loss: 0.5196\n",
            "Epoch 23/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8755 - loss: 0.4176 - val_accuracy: 0.8483 - val_loss: 0.5151\n",
            "Epoch 24/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8774 - loss: 0.4095 - val_accuracy: 0.8483 - val_loss: 0.5149\n",
            "Epoch 25/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8789 - loss: 0.4057 - val_accuracy: 0.8497 - val_loss: 0.5072\n",
            "Epoch 26/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8801 - loss: 0.3976 - val_accuracy: 0.8516 - val_loss: 0.5036\n",
            "Epoch 27/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8819 - loss: 0.3945 - val_accuracy: 0.8530 - val_loss: 0.4984\n",
            "Epoch 28/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8842 - loss: 0.3886 - val_accuracy: 0.8547 - val_loss: 0.4934\n",
            "Epoch 29/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8869 - loss: 0.3786 - val_accuracy: 0.8558 - val_loss: 0.4921\n",
            "Epoch 30/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8867 - loss: 0.3782 - val_accuracy: 0.8564 - val_loss: 0.4905\n",
            "Epoch 31/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8899 - loss: 0.3683 - val_accuracy: 0.8576 - val_loss: 0.4835\n",
            "Epoch 32/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8900 - loss: 0.3669 - val_accuracy: 0.8601 - val_loss: 0.4804\n",
            "Epoch 33/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8916 - loss: 0.3604 - val_accuracy: 0.8589 - val_loss: 0.4794\n",
            "Epoch 34/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8935 - loss: 0.3552 - val_accuracy: 0.8593 - val_loss: 0.4814\n",
            "Epoch 35/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8948 - loss: 0.3511 - val_accuracy: 0.8611 - val_loss: 0.4759\n",
            "Epoch 36/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8961 - loss: 0.3463 - val_accuracy: 0.8610 - val_loss: 0.4759\n",
            "Epoch 37/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8977 - loss: 0.3404 - val_accuracy: 0.8599 - val_loss: 0.4772\n",
            "Epoch 38/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8982 - loss: 0.3385 - val_accuracy: 0.8607 - val_loss: 0.4797\n",
            "Epoch 39/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8999 - loss: 0.3333 - val_accuracy: 0.8633 - val_loss: 0.4691\n",
            "Epoch 40/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9012 - loss: 0.3293 - val_accuracy: 0.8627 - val_loss: 0.4751\n",
            "Epoch 41/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9037 - loss: 0.3226 - val_accuracy: 0.8645 - val_loss: 0.4684\n",
            "Epoch 42/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9047 - loss: 0.3184 - val_accuracy: 0.8648 - val_loss: 0.4696\n",
            "Epoch 43/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9049 - loss: 0.3151 - val_accuracy: 0.8651 - val_loss: 0.4680\n",
            "Epoch 44/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9063 - loss: 0.3111 - val_accuracy: 0.8659 - val_loss: 0.4674\n",
            "Epoch 45/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9070 - loss: 0.3096 - val_accuracy: 0.8654 - val_loss: 0.4699\n",
            "Epoch 46/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9081 - loss: 0.3042 - val_accuracy: 0.8659 - val_loss: 0.4658\n",
            "Epoch 47/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9089 - loss: 0.3023 - val_accuracy: 0.8657 - val_loss: 0.4672\n",
            "Epoch 48/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9118 - loss: 0.2953 - val_accuracy: 0.8675 - val_loss: 0.4674\n",
            "Epoch 49/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9115 - loss: 0.2944 - val_accuracy: 0.8661 - val_loss: 0.4688\n",
            "Epoch 50/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9131 - loss: 0.2897 - val_accuracy: 0.8666 - val_loss: 0.4690\n",
            "Epoch 51/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9145 - loss: 0.2854 - val_accuracy: 0.8657 - val_loss: 0.4737\n",
            "Epoch 52/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9150 - loss: 0.2823 - val_accuracy: 0.8669 - val_loss: 0.4715\n",
            "Epoch 53/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9158 - loss: 0.2783 - val_accuracy: 0.8685 - val_loss: 0.4659\n",
            "Epoch 54/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9169 - loss: 0.2754 - val_accuracy: 0.8682 - val_loss: 0.4674\n",
            "Epoch 55/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9188 - loss: 0.2717 - val_accuracy: 0.8669 - val_loss: 0.4707\n",
            "Epoch 56/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9195 - loss: 0.2680 - val_accuracy: 0.8681 - val_loss: 0.4707\n",
            "Epoch 57/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9196 - loss: 0.2663 - val_accuracy: 0.8678 - val_loss: 0.4738\n",
            "Epoch 58/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9211 - loss: 0.2618 - val_accuracy: 0.8672 - val_loss: 0.4730\n",
            "Epoch 59/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9214 - loss: 0.2610 - val_accuracy: 0.8691 - val_loss: 0.4706\n",
            "Epoch 60/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9232 - loss: 0.2556 - val_accuracy: 0.8678 - val_loss: 0.4751\n",
            "Epoch 61/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9237 - loss: 0.2520 - val_accuracy: 0.8681 - val_loss: 0.4788\n",
            "Epoch 62/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9243 - loss: 0.2516 - val_accuracy: 0.8679 - val_loss: 0.4807\n",
            "Epoch 63/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9258 - loss: 0.2461 - val_accuracy: 0.8686 - val_loss: 0.4771\n",
            "Epoch 64/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9258 - loss: 0.2447 - val_accuracy: 0.8694 - val_loss: 0.4771\n",
            "Epoch 65/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9270 - loss: 0.2415 - val_accuracy: 0.8690 - val_loss: 0.4776\n",
            "Epoch 66/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9278 - loss: 0.2381 - val_accuracy: 0.8678 - val_loss: 0.4861\n",
            "Epoch 67/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9293 - loss: 0.2353 - val_accuracy: 0.8688 - val_loss: 0.4808\n",
            "Epoch 68/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9293 - loss: 0.2344 - val_accuracy: 0.8677 - val_loss: 0.4870\n",
            "Epoch 69/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9306 - loss: 0.2302 - val_accuracy: 0.8686 - val_loss: 0.4830\n",
            "Epoch 70/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9305 - loss: 0.2292 - val_accuracy: 0.8682 - val_loss: 0.4894\n",
            "Epoch 71/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9315 - loss: 0.2265 - val_accuracy: 0.8690 - val_loss: 0.4863\n",
            "Epoch 72/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9318 - loss: 0.2242 - val_accuracy: 0.8681 - val_loss: 0.4923\n",
            "Epoch 73/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9331 - loss: 0.2206 - val_accuracy: 0.8689 - val_loss: 0.4911\n",
            "Epoch 74/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9341 - loss: 0.2175 - val_accuracy: 0.8686 - val_loss: 0.4939\n",
            "Epoch 75/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9345 - loss: 0.2170 - val_accuracy: 0.8679 - val_loss: 0.4953\n",
            "Epoch 76/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9363 - loss: 0.2119 - val_accuracy: 0.8681 - val_loss: 0.4990\n",
            "Epoch 77/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9360 - loss: 0.2116 - val_accuracy: 0.8684 - val_loss: 0.4966\n",
            "Epoch 78/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9365 - loss: 0.2085 - val_accuracy: 0.8685 - val_loss: 0.5000\n",
            "Epoch 79/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9381 - loss: 0.2046 - val_accuracy: 0.8688 - val_loss: 0.4980\n",
            "Epoch 80/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.2050 - val_accuracy: 0.8687 - val_loss: 0.5050\n",
            "Epoch 81/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9387 - loss: 0.2022 - val_accuracy: 0.8672 - val_loss: 0.5082\n",
            "Epoch 82/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9398 - loss: 0.1983 - val_accuracy: 0.8679 - val_loss: 0.5119\n",
            "Epoch 83/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9401 - loss: 0.1964 - val_accuracy: 0.8678 - val_loss: 0.5114\n",
            "Epoch 84/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9401 - loss: 0.1976 - val_accuracy: 0.8681 - val_loss: 0.5110\n",
            "Epoch 85/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9415 - loss: 0.1929 - val_accuracy: 0.8671 - val_loss: 0.5140\n",
            "Epoch 86/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9421 - loss: 0.1906 - val_accuracy: 0.8670 - val_loss: 0.5215\n",
            "Epoch 87/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9438 - loss: 0.1864 - val_accuracy: 0.8680 - val_loss: 0.5197\n",
            "Epoch 88/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9431 - loss: 0.1871 - val_accuracy: 0.8680 - val_loss: 0.5193\n",
            "Epoch 89/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9443 - loss: 0.1838 - val_accuracy: 0.8675 - val_loss: 0.5256\n",
            "Epoch 90/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9447 - loss: 0.1816 - val_accuracy: 0.8681 - val_loss: 0.5238\n",
            "Epoch 91/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9454 - loss: 0.1810 - val_accuracy: 0.8696 - val_loss: 0.5225\n",
            "Epoch 92/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9457 - loss: 0.1788 - val_accuracy: 0.8671 - val_loss: 0.5364\n",
            "Epoch 93/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9466 - loss: 0.1770 - val_accuracy: 0.8687 - val_loss: 0.5340\n",
            "Epoch 94/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9469 - loss: 0.1752 - val_accuracy: 0.8679 - val_loss: 0.5353\n",
            "Epoch 95/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9470 - loss: 0.1738 - val_accuracy: 0.8683 - val_loss: 0.5394\n",
            "Epoch 96/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9480 - loss: 0.1712 - val_accuracy: 0.8677 - val_loss: 0.5414\n",
            "Epoch 97/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9484 - loss: 0.1693 - val_accuracy: 0.8681 - val_loss: 0.5401\n",
            "Epoch 98/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9491 - loss: 0.1669 - val_accuracy: 0.8678 - val_loss: 0.5436\n",
            "Epoch 99/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9494 - loss: 0.1650 - val_accuracy: 0.8682 - val_loss: 0.5426\n",
            "Epoch 100/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9500 - loss: 0.1633 - val_accuracy: 0.8685 - val_loss: 0.5456\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Pars !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Trais de cremint.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Trais de cremint.\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Qui l'a eclonsé ?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Tous le nous a viter !\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: L'aime !\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Aidez-nous !\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Soussez-vous !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Attrapez-le.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Attrapez-le.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Attrapez-le.\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends-toi !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends-toi !\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Sonne le commenc.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Sonne le commenc.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Sonne le commenc.\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Aide-moi !\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Aide-moi !\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Je vois une lien.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Code Explanation by Section**\n",
        "\n",
        "**Lines 51–55: Importing libraries**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "```\n",
        "\n",
        "These lines import essential libraries for numerical operations and deep learning using TensorFlow/Keras. LSTM is used to build the sequence-to-sequence model.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 57–62: Hyperparameter settings**\n",
        "\n",
        "```python\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "data_path = keras.utils.get_file(\n",
        "    \"fra-eng/fra.txt\", \"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\", extract=True\n",
        ")\n",
        "```\n",
        "\n",
        "Defines parameters such as:\n",
        "\n",
        "* `batch_size`: Number of samples per training batch\n",
        "* `epochs`: Number of iterations over the full dataset\n",
        "* `latent_dim`: Hidden layer size of LSTM\n",
        "* `num_samples`: Number of sentence pairs to use\n",
        "* `data_path`: Downloads and extracts English-French translation dataset\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 64–95: Reading and preprocessing the data**\n",
        "\n",
        "```python\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "# Read and clean data...\n",
        "```\n",
        "\n",
        "* Loads the dataset and creates parallel lists for input (English) and target (French) sentences.\n",
        "* Also builds character sets for tokenization.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 97–110: Sort characters and assign indices**\n",
        "\n",
        "```python\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "```\n",
        "\n",
        "* Assigns a unique integer ID to each character in both languages.\n",
        "* Useful for vectorization.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 112–125: Vectorize input and output**\n",
        "\n",
        "```python\n",
        "encoder_input_data = np.zeros(...)\n",
        "decoder_input_data = np.zeros(...)\n",
        "decoder_target_data = np.zeros(...)\n",
        "# Populate the arrays with one-hot encodings\n",
        "```\n",
        "\n",
        "* Converts text into one-hot encoded arrays to feed into the model.\n",
        "* Handles input for encoder and decoder.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 127–137: Build the encoder model**\n",
        "\n",
        "```python\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "```\n",
        "\n",
        "* The encoder processes input sequences and returns internal LSTM states.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 139–151: Build the decoder model**\n",
        "\n",
        "```python\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "```\n",
        "\n",
        "* Decoder uses the encoder's internal states to generate output sequences.\n",
        "* Applies a dense softmax layer for final output prediction.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 153–155: Define and compile the model**\n",
        "\n",
        "```python\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "* Wraps the encoder-decoder into a full model and compiles it with categorical loss and accuracy metric.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 157–159: Train the model**\n",
        "\n",
        "```python\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "```\n",
        "\n",
        "* Begins training the model with the input and target data.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 161–174: Define inference models (for translation)**\n",
        "\n",
        "* Separates encoder and decoder models for inference (translating new sentences).\n",
        "* Necessary because in inference, you generate one token at a time.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 176–196: Decode sequence function**\n",
        "\n",
        "* Translates input sentences using the trained model and inference loop.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 198–202: Display sample translations**\n",
        "\n",
        "* Tests the model on a few input sequences and prints the predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## Character-Level Tokenization with `CountVectorizer`\n",
        "\n",
        "When using `sklearn.feature_extraction.text.CountVectorizer`, you can specify how text is tokenized using the `analyzer` argument.\n",
        "\n",
        "### 🔸 `analyzer='char'`\n",
        "\n",
        "* **Character n-grams** across words.\n",
        "* E.g., `\"This movie\"` → tokens like `'T'`, `'Th'`, `'his'`, `'is '`, `'s m'`, `' mo'`, etc.\n",
        "\n",
        "### 🔸 `analyzer='char_wb'`\n",
        "\n",
        "* **Character n-grams** only *within* word boundaries.\n",
        "* Avoids capturing patterns like `'s m'` (from `This movie`) which cross word boundaries.\n",
        "\n",
        "Use case:\n",
        "\n",
        "* **char**: Useful for language modeling and machine translation\n",
        "* **char\\_wb**: Better for morphology-sensitive tasks (e.g., spelling)\n",
        "\n",
        "**Docs:**\n",
        "[CountVectorizer – scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1yArGwQTSqal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Running a pre-trained model for image captioning"
      ],
      "metadata": {
        "id": "ZpS5UXRidUp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RNJp8iqbSHWE",
        "outputId": "b1a8ca22-b103-4874-b4ea-a3e9f879eaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# set up the environment i.e. install dependencies\n",
        "!pip install torch torchvision pillow numpy matplotlib nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Download NLTK Word Tokenizer (required for text preprocessing):"
      ],
      "metadata": {
        "id": "OC4DnQ-CdxUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_xxrNQOdxBF",
        "outputId": "8ac61f28-d420-4624-e0d7-73876604608c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yunjey/pytorch-tutorial.git\n",
        "!cd pytorch-tutorial/tutorials/03-advanced/image_captioning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5SjJaLudrGN",
        "outputId": "527e399a-6f32-4b25-8b3a-4a8c93bb88ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-tutorial'...\n",
            "remote: Enumerating objects: 917, done.\u001b[K\n",
            "remote: Total 917 (delta 0), reused 0 (delta 0), pack-reused 917 (from 1)\u001b[K\n",
            "Receiving objects: 100% (917/917), 12.80 MiB | 40.46 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''2. Using gdown:\n",
        "Upload to Google Drive: Upload your file to Google Drive and make it shareable with \"anyone with the link.\"\n",
        "Copy File ID: Extract the file ID from the shareable link.\n",
        "Download in Colab: Use the gdown command in a Colab cell:\n",
        "Code\n",
        "\n",
        "Replace <your_file_id> with the actual file ID. This is often faster than direct downloads.\n",
        "'''\n",
        "!gdown 1Wmq6aKkItmTufvachL9mFeMCT-3-g2qH\n",
        "!gdown 1iegY6ZVt1dm8cYeHu7CA2QYupJY6kDiC"
      ],
      "metadata": {
        "id": "ICKnCvJ2sVIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38b1f8d-b85e-4afe-c459-7215df93a2d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Wmq6aKkItmTufvachL9mFeMCT-3-g2qH\n",
            "From (redirected): https://drive.google.com/uc?id=1Wmq6aKkItmTufvachL9mFeMCT-3-g2qH&confirm=t&uuid=13a69996-ac62-4719-9526-5d759c6f3397\n",
            "To: /content/encoder-5-3000.pkl\n",
            "100% 235M/235M [00:02<00:00, 81.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iegY6ZVt1dm8cYeHu7CA2QYupJY6kDiC\n",
            "To: /content/decoder-5-3000.pkl\n",
            "100% 36.9M/36.9M [00:00<00:00, 80.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/pytorch-tutorial/data .\n",
        "#!cp -r /content/pytorch-tutorial/models ."
      ],
      "metadata": {
        "id": "LR6r8v8ifTmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Construct the full paths to the model files\n",
        "encoder_path = '/content/encoder-5-3000.pkl'\n",
        "decoder_path = '/content/decoder-5-3000.pkl'\n",
        "\n",
        "# Load the models\n",
        "encoder = torch.load(encoder_path, map_location='cpu')  # or 'cuda'\n",
        "decoder = torch.load(decoder_path, map_location='cpu')\n",
        "\n",
        "print(\"Encoder keys:\", encoder.keys())  # Should show model weights\n",
        "print(\"Decoder keys:\", decoder.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQJsJlUpiDb",
        "outputId": "4138a25f-694a-4315-b165-95e9861c936d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder keys: odict_keys(['resnet.0.weight', 'resnet.1.weight', 'resnet.1.bias', 'resnet.1.running_mean', 'resnet.1.running_var', 'resnet.4.0.conv1.weight', 'resnet.4.0.bn1.weight', 'resnet.4.0.bn1.bias', 'resnet.4.0.bn1.running_mean', 'resnet.4.0.bn1.running_var', 'resnet.4.0.conv2.weight', 'resnet.4.0.bn2.weight', 'resnet.4.0.bn2.bias', 'resnet.4.0.bn2.running_mean', 'resnet.4.0.bn2.running_var', 'resnet.4.0.conv3.weight', 'resnet.4.0.bn3.weight', 'resnet.4.0.bn3.bias', 'resnet.4.0.bn3.running_mean', 'resnet.4.0.bn3.running_var', 'resnet.4.0.downsample.0.weight', 'resnet.4.0.downsample.1.weight', 'resnet.4.0.downsample.1.bias', 'resnet.4.0.downsample.1.running_mean', 'resnet.4.0.downsample.1.running_var', 'resnet.4.1.conv1.weight', 'resnet.4.1.bn1.weight', 'resnet.4.1.bn1.bias', 'resnet.4.1.bn1.running_mean', 'resnet.4.1.bn1.running_var', 'resnet.4.1.conv2.weight', 'resnet.4.1.bn2.weight', 'resnet.4.1.bn2.bias', 'resnet.4.1.bn2.running_mean', 'resnet.4.1.bn2.running_var', 'resnet.4.1.conv3.weight', 'resnet.4.1.bn3.weight', 'resnet.4.1.bn3.bias', 'resnet.4.1.bn3.running_mean', 'resnet.4.1.bn3.running_var', 'resnet.4.2.conv1.weight', 'resnet.4.2.bn1.weight', 'resnet.4.2.bn1.bias', 'resnet.4.2.bn1.running_mean', 'resnet.4.2.bn1.running_var', 'resnet.4.2.conv2.weight', 'resnet.4.2.bn2.weight', 'resnet.4.2.bn2.bias', 'resnet.4.2.bn2.running_mean', 'resnet.4.2.bn2.running_var', 'resnet.4.2.conv3.weight', 'resnet.4.2.bn3.weight', 'resnet.4.2.bn3.bias', 'resnet.4.2.bn3.running_mean', 'resnet.4.2.bn3.running_var', 'resnet.5.0.conv1.weight', 'resnet.5.0.bn1.weight', 'resnet.5.0.bn1.bias', 'resnet.5.0.bn1.running_mean', 'resnet.5.0.bn1.running_var', 'resnet.5.0.conv2.weight', 'resnet.5.0.bn2.weight', 'resnet.5.0.bn2.bias', 'resnet.5.0.bn2.running_mean', 'resnet.5.0.bn2.running_var', 'resnet.5.0.conv3.weight', 'resnet.5.0.bn3.weight', 'resnet.5.0.bn3.bias', 'resnet.5.0.bn3.running_mean', 'resnet.5.0.bn3.running_var', 'resnet.5.0.downsample.0.weight', 'resnet.5.0.downsample.1.weight', 'resnet.5.0.downsample.1.bias', 'resnet.5.0.downsample.1.running_mean', 'resnet.5.0.downsample.1.running_var', 'resnet.5.1.conv1.weight', 'resnet.5.1.bn1.weight', 'resnet.5.1.bn1.bias', 'resnet.5.1.bn1.running_mean', 'resnet.5.1.bn1.running_var', 'resnet.5.1.conv2.weight', 'resnet.5.1.bn2.weight', 'resnet.5.1.bn2.bias', 'resnet.5.1.bn2.running_mean', 'resnet.5.1.bn2.running_var', 'resnet.5.1.conv3.weight', 'resnet.5.1.bn3.weight', 'resnet.5.1.bn3.bias', 'resnet.5.1.bn3.running_mean', 'resnet.5.1.bn3.running_var', 'resnet.5.2.conv1.weight', 'resnet.5.2.bn1.weight', 'resnet.5.2.bn1.bias', 'resnet.5.2.bn1.running_mean', 'resnet.5.2.bn1.running_var', 'resnet.5.2.conv2.weight', 'resnet.5.2.bn2.weight', 'resnet.5.2.bn2.bias', 'resnet.5.2.bn2.running_mean', 'resnet.5.2.bn2.running_var', 'resnet.5.2.conv3.weight', 'resnet.5.2.bn3.weight', 'resnet.5.2.bn3.bias', 'resnet.5.2.bn3.running_mean', 'resnet.5.2.bn3.running_var', 'resnet.5.3.conv1.weight', 'resnet.5.3.bn1.weight', 'resnet.5.3.bn1.bias', 'resnet.5.3.bn1.running_mean', 'resnet.5.3.bn1.running_var', 'resnet.5.3.conv2.weight', 'resnet.5.3.bn2.weight', 'resnet.5.3.bn2.bias', 'resnet.5.3.bn2.running_mean', 'resnet.5.3.bn2.running_var', 'resnet.5.3.conv3.weight', 'resnet.5.3.bn3.weight', 'resnet.5.3.bn3.bias', 'resnet.5.3.bn3.running_mean', 'resnet.5.3.bn3.running_var', 'resnet.5.4.conv1.weight', 'resnet.5.4.bn1.weight', 'resnet.5.4.bn1.bias', 'resnet.5.4.bn1.running_mean', 'resnet.5.4.bn1.running_var', 'resnet.5.4.conv2.weight', 'resnet.5.4.bn2.weight', 'resnet.5.4.bn2.bias', 'resnet.5.4.bn2.running_mean', 'resnet.5.4.bn2.running_var', 'resnet.5.4.conv3.weight', 'resnet.5.4.bn3.weight', 'resnet.5.4.bn3.bias', 'resnet.5.4.bn3.running_mean', 'resnet.5.4.bn3.running_var', 'resnet.5.5.conv1.weight', 'resnet.5.5.bn1.weight', 'resnet.5.5.bn1.bias', 'resnet.5.5.bn1.running_mean', 'resnet.5.5.bn1.running_var', 'resnet.5.5.conv2.weight', 'resnet.5.5.bn2.weight', 'resnet.5.5.bn2.bias', 'resnet.5.5.bn2.running_mean', 'resnet.5.5.bn2.running_var', 'resnet.5.5.conv3.weight', 'resnet.5.5.bn3.weight', 'resnet.5.5.bn3.bias', 'resnet.5.5.bn3.running_mean', 'resnet.5.5.bn3.running_var', 'resnet.5.6.conv1.weight', 'resnet.5.6.bn1.weight', 'resnet.5.6.bn1.bias', 'resnet.5.6.bn1.running_mean', 'resnet.5.6.bn1.running_var', 'resnet.5.6.conv2.weight', 'resnet.5.6.bn2.weight', 'resnet.5.6.bn2.bias', 'resnet.5.6.bn2.running_mean', 'resnet.5.6.bn2.running_var', 'resnet.5.6.conv3.weight', 'resnet.5.6.bn3.weight', 'resnet.5.6.bn3.bias', 'resnet.5.6.bn3.running_mean', 'resnet.5.6.bn3.running_var', 'resnet.5.7.conv1.weight', 'resnet.5.7.bn1.weight', 'resnet.5.7.bn1.bias', 'resnet.5.7.bn1.running_mean', 'resnet.5.7.bn1.running_var', 'resnet.5.7.conv2.weight', 'resnet.5.7.bn2.weight', 'resnet.5.7.bn2.bias', 'resnet.5.7.bn2.running_mean', 'resnet.5.7.bn2.running_var', 'resnet.5.7.conv3.weight', 'resnet.5.7.bn3.weight', 'resnet.5.7.bn3.bias', 'resnet.5.7.bn3.running_mean', 'resnet.5.7.bn3.running_var', 'resnet.6.0.conv1.weight', 'resnet.6.0.bn1.weight', 'resnet.6.0.bn1.bias', 'resnet.6.0.bn1.running_mean', 'resnet.6.0.bn1.running_var', 'resnet.6.0.conv2.weight', 'resnet.6.0.bn2.weight', 'resnet.6.0.bn2.bias', 'resnet.6.0.bn2.running_mean', 'resnet.6.0.bn2.running_var', 'resnet.6.0.conv3.weight', 'resnet.6.0.bn3.weight', 'resnet.6.0.bn3.bias', 'resnet.6.0.bn3.running_mean', 'resnet.6.0.bn3.running_var', 'resnet.6.0.downsample.0.weight', 'resnet.6.0.downsample.1.weight', 'resnet.6.0.downsample.1.bias', 'resnet.6.0.downsample.1.running_mean', 'resnet.6.0.downsample.1.running_var', 'resnet.6.1.conv1.weight', 'resnet.6.1.bn1.weight', 'resnet.6.1.bn1.bias', 'resnet.6.1.bn1.running_mean', 'resnet.6.1.bn1.running_var', 'resnet.6.1.conv2.weight', 'resnet.6.1.bn2.weight', 'resnet.6.1.bn2.bias', 'resnet.6.1.bn2.running_mean', 'resnet.6.1.bn2.running_var', 'resnet.6.1.conv3.weight', 'resnet.6.1.bn3.weight', 'resnet.6.1.bn3.bias', 'resnet.6.1.bn3.running_mean', 'resnet.6.1.bn3.running_var', 'resnet.6.2.conv1.weight', 'resnet.6.2.bn1.weight', 'resnet.6.2.bn1.bias', 'resnet.6.2.bn1.running_mean', 'resnet.6.2.bn1.running_var', 'resnet.6.2.conv2.weight', 'resnet.6.2.bn2.weight', 'resnet.6.2.bn2.bias', 'resnet.6.2.bn2.running_mean', 'resnet.6.2.bn2.running_var', 'resnet.6.2.conv3.weight', 'resnet.6.2.bn3.weight', 'resnet.6.2.bn3.bias', 'resnet.6.2.bn3.running_mean', 'resnet.6.2.bn3.running_var', 'resnet.6.3.conv1.weight', 'resnet.6.3.bn1.weight', 'resnet.6.3.bn1.bias', 'resnet.6.3.bn1.running_mean', 'resnet.6.3.bn1.running_var', 'resnet.6.3.conv2.weight', 'resnet.6.3.bn2.weight', 'resnet.6.3.bn2.bias', 'resnet.6.3.bn2.running_mean', 'resnet.6.3.bn2.running_var', 'resnet.6.3.conv3.weight', 'resnet.6.3.bn3.weight', 'resnet.6.3.bn3.bias', 'resnet.6.3.bn3.running_mean', 'resnet.6.3.bn3.running_var', 'resnet.6.4.conv1.weight', 'resnet.6.4.bn1.weight', 'resnet.6.4.bn1.bias', 'resnet.6.4.bn1.running_mean', 'resnet.6.4.bn1.running_var', 'resnet.6.4.conv2.weight', 'resnet.6.4.bn2.weight', 'resnet.6.4.bn2.bias', 'resnet.6.4.bn2.running_mean', 'resnet.6.4.bn2.running_var', 'resnet.6.4.conv3.weight', 'resnet.6.4.bn3.weight', 'resnet.6.4.bn3.bias', 'resnet.6.4.bn3.running_mean', 'resnet.6.4.bn3.running_var', 'resnet.6.5.conv1.weight', 'resnet.6.5.bn1.weight', 'resnet.6.5.bn1.bias', 'resnet.6.5.bn1.running_mean', 'resnet.6.5.bn1.running_var', 'resnet.6.5.conv2.weight', 'resnet.6.5.bn2.weight', 'resnet.6.5.bn2.bias', 'resnet.6.5.bn2.running_mean', 'resnet.6.5.bn2.running_var', 'resnet.6.5.conv3.weight', 'resnet.6.5.bn3.weight', 'resnet.6.5.bn3.bias', 'resnet.6.5.bn3.running_mean', 'resnet.6.5.bn3.running_var', 'resnet.6.6.conv1.weight', 'resnet.6.6.bn1.weight', 'resnet.6.6.bn1.bias', 'resnet.6.6.bn1.running_mean', 'resnet.6.6.bn1.running_var', 'resnet.6.6.conv2.weight', 'resnet.6.6.bn2.weight', 'resnet.6.6.bn2.bias', 'resnet.6.6.bn2.running_mean', 'resnet.6.6.bn2.running_var', 'resnet.6.6.conv3.weight', 'resnet.6.6.bn3.weight', 'resnet.6.6.bn3.bias', 'resnet.6.6.bn3.running_mean', 'resnet.6.6.bn3.running_var', 'resnet.6.7.conv1.weight', 'resnet.6.7.bn1.weight', 'resnet.6.7.bn1.bias', 'resnet.6.7.bn1.running_mean', 'resnet.6.7.bn1.running_var', 'resnet.6.7.conv2.weight', 'resnet.6.7.bn2.weight', 'resnet.6.7.bn2.bias', 'resnet.6.7.bn2.running_mean', 'resnet.6.7.bn2.running_var', 'resnet.6.7.conv3.weight', 'resnet.6.7.bn3.weight', 'resnet.6.7.bn3.bias', 'resnet.6.7.bn3.running_mean', 'resnet.6.7.bn3.running_var', 'resnet.6.8.conv1.weight', 'resnet.6.8.bn1.weight', 'resnet.6.8.bn1.bias', 'resnet.6.8.bn1.running_mean', 'resnet.6.8.bn1.running_var', 'resnet.6.8.conv2.weight', 'resnet.6.8.bn2.weight', 'resnet.6.8.bn2.bias', 'resnet.6.8.bn2.running_mean', 'resnet.6.8.bn2.running_var', 'resnet.6.8.conv3.weight', 'resnet.6.8.bn3.weight', 'resnet.6.8.bn3.bias', 'resnet.6.8.bn3.running_mean', 'resnet.6.8.bn3.running_var', 'resnet.6.9.conv1.weight', 'resnet.6.9.bn1.weight', 'resnet.6.9.bn1.bias', 'resnet.6.9.bn1.running_mean', 'resnet.6.9.bn1.running_var', 'resnet.6.9.conv2.weight', 'resnet.6.9.bn2.weight', 'resnet.6.9.bn2.bias', 'resnet.6.9.bn2.running_mean', 'resnet.6.9.bn2.running_var', 'resnet.6.9.conv3.weight', 'resnet.6.9.bn3.weight', 'resnet.6.9.bn3.bias', 'resnet.6.9.bn3.running_mean', 'resnet.6.9.bn3.running_var', 'resnet.6.10.conv1.weight', 'resnet.6.10.bn1.weight', 'resnet.6.10.bn1.bias', 'resnet.6.10.bn1.running_mean', 'resnet.6.10.bn1.running_var', 'resnet.6.10.conv2.weight', 'resnet.6.10.bn2.weight', 'resnet.6.10.bn2.bias', 'resnet.6.10.bn2.running_mean', 'resnet.6.10.bn2.running_var', 'resnet.6.10.conv3.weight', 'resnet.6.10.bn3.weight', 'resnet.6.10.bn3.bias', 'resnet.6.10.bn3.running_mean', 'resnet.6.10.bn3.running_var', 'resnet.6.11.conv1.weight', 'resnet.6.11.bn1.weight', 'resnet.6.11.bn1.bias', 'resnet.6.11.bn1.running_mean', 'resnet.6.11.bn1.running_var', 'resnet.6.11.conv2.weight', 'resnet.6.11.bn2.weight', 'resnet.6.11.bn2.bias', 'resnet.6.11.bn2.running_mean', 'resnet.6.11.bn2.running_var', 'resnet.6.11.conv3.weight', 'resnet.6.11.bn3.weight', 'resnet.6.11.bn3.bias', 'resnet.6.11.bn3.running_mean', 'resnet.6.11.bn3.running_var', 'resnet.6.12.conv1.weight', 'resnet.6.12.bn1.weight', 'resnet.6.12.bn1.bias', 'resnet.6.12.bn1.running_mean', 'resnet.6.12.bn1.running_var', 'resnet.6.12.conv2.weight', 'resnet.6.12.bn2.weight', 'resnet.6.12.bn2.bias', 'resnet.6.12.bn2.running_mean', 'resnet.6.12.bn2.running_var', 'resnet.6.12.conv3.weight', 'resnet.6.12.bn3.weight', 'resnet.6.12.bn3.bias', 'resnet.6.12.bn3.running_mean', 'resnet.6.12.bn3.running_var', 'resnet.6.13.conv1.weight', 'resnet.6.13.bn1.weight', 'resnet.6.13.bn1.bias', 'resnet.6.13.bn1.running_mean', 'resnet.6.13.bn1.running_var', 'resnet.6.13.conv2.weight', 'resnet.6.13.bn2.weight', 'resnet.6.13.bn2.bias', 'resnet.6.13.bn2.running_mean', 'resnet.6.13.bn2.running_var', 'resnet.6.13.conv3.weight', 'resnet.6.13.bn3.weight', 'resnet.6.13.bn3.bias', 'resnet.6.13.bn3.running_mean', 'resnet.6.13.bn3.running_var', 'resnet.6.14.conv1.weight', 'resnet.6.14.bn1.weight', 'resnet.6.14.bn1.bias', 'resnet.6.14.bn1.running_mean', 'resnet.6.14.bn1.running_var', 'resnet.6.14.conv2.weight', 'resnet.6.14.bn2.weight', 'resnet.6.14.bn2.bias', 'resnet.6.14.bn2.running_mean', 'resnet.6.14.bn2.running_var', 'resnet.6.14.conv3.weight', 'resnet.6.14.bn3.weight', 'resnet.6.14.bn3.bias', 'resnet.6.14.bn3.running_mean', 'resnet.6.14.bn3.running_var', 'resnet.6.15.conv1.weight', 'resnet.6.15.bn1.weight', 'resnet.6.15.bn1.bias', 'resnet.6.15.bn1.running_mean', 'resnet.6.15.bn1.running_var', 'resnet.6.15.conv2.weight', 'resnet.6.15.bn2.weight', 'resnet.6.15.bn2.bias', 'resnet.6.15.bn2.running_mean', 'resnet.6.15.bn2.running_var', 'resnet.6.15.conv3.weight', 'resnet.6.15.bn3.weight', 'resnet.6.15.bn3.bias', 'resnet.6.15.bn3.running_mean', 'resnet.6.15.bn3.running_var', 'resnet.6.16.conv1.weight', 'resnet.6.16.bn1.weight', 'resnet.6.16.bn1.bias', 'resnet.6.16.bn1.running_mean', 'resnet.6.16.bn1.running_var', 'resnet.6.16.conv2.weight', 'resnet.6.16.bn2.weight', 'resnet.6.16.bn2.bias', 'resnet.6.16.bn2.running_mean', 'resnet.6.16.bn2.running_var', 'resnet.6.16.conv3.weight', 'resnet.6.16.bn3.weight', 'resnet.6.16.bn3.bias', 'resnet.6.16.bn3.running_mean', 'resnet.6.16.bn3.running_var', 'resnet.6.17.conv1.weight', 'resnet.6.17.bn1.weight', 'resnet.6.17.bn1.bias', 'resnet.6.17.bn1.running_mean', 'resnet.6.17.bn1.running_var', 'resnet.6.17.conv2.weight', 'resnet.6.17.bn2.weight', 'resnet.6.17.bn2.bias', 'resnet.6.17.bn2.running_mean', 'resnet.6.17.bn2.running_var', 'resnet.6.17.conv3.weight', 'resnet.6.17.bn3.weight', 'resnet.6.17.bn3.bias', 'resnet.6.17.bn3.running_mean', 'resnet.6.17.bn3.running_var', 'resnet.6.18.conv1.weight', 'resnet.6.18.bn1.weight', 'resnet.6.18.bn1.bias', 'resnet.6.18.bn1.running_mean', 'resnet.6.18.bn1.running_var', 'resnet.6.18.conv2.weight', 'resnet.6.18.bn2.weight', 'resnet.6.18.bn2.bias', 'resnet.6.18.bn2.running_mean', 'resnet.6.18.bn2.running_var', 'resnet.6.18.conv3.weight', 'resnet.6.18.bn3.weight', 'resnet.6.18.bn3.bias', 'resnet.6.18.bn3.running_mean', 'resnet.6.18.bn3.running_var', 'resnet.6.19.conv1.weight', 'resnet.6.19.bn1.weight', 'resnet.6.19.bn1.bias', 'resnet.6.19.bn1.running_mean', 'resnet.6.19.bn1.running_var', 'resnet.6.19.conv2.weight', 'resnet.6.19.bn2.weight', 'resnet.6.19.bn2.bias', 'resnet.6.19.bn2.running_mean', 'resnet.6.19.bn2.running_var', 'resnet.6.19.conv3.weight', 'resnet.6.19.bn3.weight', 'resnet.6.19.bn3.bias', 'resnet.6.19.bn3.running_mean', 'resnet.6.19.bn3.running_var', 'resnet.6.20.conv1.weight', 'resnet.6.20.bn1.weight', 'resnet.6.20.bn1.bias', 'resnet.6.20.bn1.running_mean', 'resnet.6.20.bn1.running_var', 'resnet.6.20.conv2.weight', 'resnet.6.20.bn2.weight', 'resnet.6.20.bn2.bias', 'resnet.6.20.bn2.running_mean', 'resnet.6.20.bn2.running_var', 'resnet.6.20.conv3.weight', 'resnet.6.20.bn3.weight', 'resnet.6.20.bn3.bias', 'resnet.6.20.bn3.running_mean', 'resnet.6.20.bn3.running_var', 'resnet.6.21.conv1.weight', 'resnet.6.21.bn1.weight', 'resnet.6.21.bn1.bias', 'resnet.6.21.bn1.running_mean', 'resnet.6.21.bn1.running_var', 'resnet.6.21.conv2.weight', 'resnet.6.21.bn2.weight', 'resnet.6.21.bn2.bias', 'resnet.6.21.bn2.running_mean', 'resnet.6.21.bn2.running_var', 'resnet.6.21.conv3.weight', 'resnet.6.21.bn3.weight', 'resnet.6.21.bn3.bias', 'resnet.6.21.bn3.running_mean', 'resnet.6.21.bn3.running_var', 'resnet.6.22.conv1.weight', 'resnet.6.22.bn1.weight', 'resnet.6.22.bn1.bias', 'resnet.6.22.bn1.running_mean', 'resnet.6.22.bn1.running_var', 'resnet.6.22.conv2.weight', 'resnet.6.22.bn2.weight', 'resnet.6.22.bn2.bias', 'resnet.6.22.bn2.running_mean', 'resnet.6.22.bn2.running_var', 'resnet.6.22.conv3.weight', 'resnet.6.22.bn3.weight', 'resnet.6.22.bn3.bias', 'resnet.6.22.bn3.running_mean', 'resnet.6.22.bn3.running_var', 'resnet.6.23.conv1.weight', 'resnet.6.23.bn1.weight', 'resnet.6.23.bn1.bias', 'resnet.6.23.bn1.running_mean', 'resnet.6.23.bn1.running_var', 'resnet.6.23.conv2.weight', 'resnet.6.23.bn2.weight', 'resnet.6.23.bn2.bias', 'resnet.6.23.bn2.running_mean', 'resnet.6.23.bn2.running_var', 'resnet.6.23.conv3.weight', 'resnet.6.23.bn3.weight', 'resnet.6.23.bn3.bias', 'resnet.6.23.bn3.running_mean', 'resnet.6.23.bn3.running_var', 'resnet.6.24.conv1.weight', 'resnet.6.24.bn1.weight', 'resnet.6.24.bn1.bias', 'resnet.6.24.bn1.running_mean', 'resnet.6.24.bn1.running_var', 'resnet.6.24.conv2.weight', 'resnet.6.24.bn2.weight', 'resnet.6.24.bn2.bias', 'resnet.6.24.bn2.running_mean', 'resnet.6.24.bn2.running_var', 'resnet.6.24.conv3.weight', 'resnet.6.24.bn3.weight', 'resnet.6.24.bn3.bias', 'resnet.6.24.bn3.running_mean', 'resnet.6.24.bn3.running_var', 'resnet.6.25.conv1.weight', 'resnet.6.25.bn1.weight', 'resnet.6.25.bn1.bias', 'resnet.6.25.bn1.running_mean', 'resnet.6.25.bn1.running_var', 'resnet.6.25.conv2.weight', 'resnet.6.25.bn2.weight', 'resnet.6.25.bn2.bias', 'resnet.6.25.bn2.running_mean', 'resnet.6.25.bn2.running_var', 'resnet.6.25.conv3.weight', 'resnet.6.25.bn3.weight', 'resnet.6.25.bn3.bias', 'resnet.6.25.bn3.running_mean', 'resnet.6.25.bn3.running_var', 'resnet.6.26.conv1.weight', 'resnet.6.26.bn1.weight', 'resnet.6.26.bn1.bias', 'resnet.6.26.bn1.running_mean', 'resnet.6.26.bn1.running_var', 'resnet.6.26.conv2.weight', 'resnet.6.26.bn2.weight', 'resnet.6.26.bn2.bias', 'resnet.6.26.bn2.running_mean', 'resnet.6.26.bn2.running_var', 'resnet.6.26.conv3.weight', 'resnet.6.26.bn3.weight', 'resnet.6.26.bn3.bias', 'resnet.6.26.bn3.running_mean', 'resnet.6.26.bn3.running_var', 'resnet.6.27.conv1.weight', 'resnet.6.27.bn1.weight', 'resnet.6.27.bn1.bias', 'resnet.6.27.bn1.running_mean', 'resnet.6.27.bn1.running_var', 'resnet.6.27.conv2.weight', 'resnet.6.27.bn2.weight', 'resnet.6.27.bn2.bias', 'resnet.6.27.bn2.running_mean', 'resnet.6.27.bn2.running_var', 'resnet.6.27.conv3.weight', 'resnet.6.27.bn3.weight', 'resnet.6.27.bn3.bias', 'resnet.6.27.bn3.running_mean', 'resnet.6.27.bn3.running_var', 'resnet.6.28.conv1.weight', 'resnet.6.28.bn1.weight', 'resnet.6.28.bn1.bias', 'resnet.6.28.bn1.running_mean', 'resnet.6.28.bn1.running_var', 'resnet.6.28.conv2.weight', 'resnet.6.28.bn2.weight', 'resnet.6.28.bn2.bias', 'resnet.6.28.bn2.running_mean', 'resnet.6.28.bn2.running_var', 'resnet.6.28.conv3.weight', 'resnet.6.28.bn3.weight', 'resnet.6.28.bn3.bias', 'resnet.6.28.bn3.running_mean', 'resnet.6.28.bn3.running_var', 'resnet.6.29.conv1.weight', 'resnet.6.29.bn1.weight', 'resnet.6.29.bn1.bias', 'resnet.6.29.bn1.running_mean', 'resnet.6.29.bn1.running_var', 'resnet.6.29.conv2.weight', 'resnet.6.29.bn2.weight', 'resnet.6.29.bn2.bias', 'resnet.6.29.bn2.running_mean', 'resnet.6.29.bn2.running_var', 'resnet.6.29.conv3.weight', 'resnet.6.29.bn3.weight', 'resnet.6.29.bn3.bias', 'resnet.6.29.bn3.running_mean', 'resnet.6.29.bn3.running_var', 'resnet.6.30.conv1.weight', 'resnet.6.30.bn1.weight', 'resnet.6.30.bn1.bias', 'resnet.6.30.bn1.running_mean', 'resnet.6.30.bn1.running_var', 'resnet.6.30.conv2.weight', 'resnet.6.30.bn2.weight', 'resnet.6.30.bn2.bias', 'resnet.6.30.bn2.running_mean', 'resnet.6.30.bn2.running_var', 'resnet.6.30.conv3.weight', 'resnet.6.30.bn3.weight', 'resnet.6.30.bn3.bias', 'resnet.6.30.bn3.running_mean', 'resnet.6.30.bn3.running_var', 'resnet.6.31.conv1.weight', 'resnet.6.31.bn1.weight', 'resnet.6.31.bn1.bias', 'resnet.6.31.bn1.running_mean', 'resnet.6.31.bn1.running_var', 'resnet.6.31.conv2.weight', 'resnet.6.31.bn2.weight', 'resnet.6.31.bn2.bias', 'resnet.6.31.bn2.running_mean', 'resnet.6.31.bn2.running_var', 'resnet.6.31.conv3.weight', 'resnet.6.31.bn3.weight', 'resnet.6.31.bn3.bias', 'resnet.6.31.bn3.running_mean', 'resnet.6.31.bn3.running_var', 'resnet.6.32.conv1.weight', 'resnet.6.32.bn1.weight', 'resnet.6.32.bn1.bias', 'resnet.6.32.bn1.running_mean', 'resnet.6.32.bn1.running_var', 'resnet.6.32.conv2.weight', 'resnet.6.32.bn2.weight', 'resnet.6.32.bn2.bias', 'resnet.6.32.bn2.running_mean', 'resnet.6.32.bn2.running_var', 'resnet.6.32.conv3.weight', 'resnet.6.32.bn3.weight', 'resnet.6.32.bn3.bias', 'resnet.6.32.bn3.running_mean', 'resnet.6.32.bn3.running_var', 'resnet.6.33.conv1.weight', 'resnet.6.33.bn1.weight', 'resnet.6.33.bn1.bias', 'resnet.6.33.bn1.running_mean', 'resnet.6.33.bn1.running_var', 'resnet.6.33.conv2.weight', 'resnet.6.33.bn2.weight', 'resnet.6.33.bn2.bias', 'resnet.6.33.bn2.running_mean', 'resnet.6.33.bn2.running_var', 'resnet.6.33.conv3.weight', 'resnet.6.33.bn3.weight', 'resnet.6.33.bn3.bias', 'resnet.6.33.bn3.running_mean', 'resnet.6.33.bn3.running_var', 'resnet.6.34.conv1.weight', 'resnet.6.34.bn1.weight', 'resnet.6.34.bn1.bias', 'resnet.6.34.bn1.running_mean', 'resnet.6.34.bn1.running_var', 'resnet.6.34.conv2.weight', 'resnet.6.34.bn2.weight', 'resnet.6.34.bn2.bias', 'resnet.6.34.bn2.running_mean', 'resnet.6.34.bn2.running_var', 'resnet.6.34.conv3.weight', 'resnet.6.34.bn3.weight', 'resnet.6.34.bn3.bias', 'resnet.6.34.bn3.running_mean', 'resnet.6.34.bn3.running_var', 'resnet.6.35.conv1.weight', 'resnet.6.35.bn1.weight', 'resnet.6.35.bn1.bias', 'resnet.6.35.bn1.running_mean', 'resnet.6.35.bn1.running_var', 'resnet.6.35.conv2.weight', 'resnet.6.35.bn2.weight', 'resnet.6.35.bn2.bias', 'resnet.6.35.bn2.running_mean', 'resnet.6.35.bn2.running_var', 'resnet.6.35.conv3.weight', 'resnet.6.35.bn3.weight', 'resnet.6.35.bn3.bias', 'resnet.6.35.bn3.running_mean', 'resnet.6.35.bn3.running_var', 'resnet.7.0.conv1.weight', 'resnet.7.0.bn1.weight', 'resnet.7.0.bn1.bias', 'resnet.7.0.bn1.running_mean', 'resnet.7.0.bn1.running_var', 'resnet.7.0.conv2.weight', 'resnet.7.0.bn2.weight', 'resnet.7.0.bn2.bias', 'resnet.7.0.bn2.running_mean', 'resnet.7.0.bn2.running_var', 'resnet.7.0.conv3.weight', 'resnet.7.0.bn3.weight', 'resnet.7.0.bn3.bias', 'resnet.7.0.bn3.running_mean', 'resnet.7.0.bn3.running_var', 'resnet.7.0.downsample.0.weight', 'resnet.7.0.downsample.1.weight', 'resnet.7.0.downsample.1.bias', 'resnet.7.0.downsample.1.running_mean', 'resnet.7.0.downsample.1.running_var', 'resnet.7.1.conv1.weight', 'resnet.7.1.bn1.weight', 'resnet.7.1.bn1.bias', 'resnet.7.1.bn1.running_mean', 'resnet.7.1.bn1.running_var', 'resnet.7.1.conv2.weight', 'resnet.7.1.bn2.weight', 'resnet.7.1.bn2.bias', 'resnet.7.1.bn2.running_mean', 'resnet.7.1.bn2.running_var', 'resnet.7.1.conv3.weight', 'resnet.7.1.bn3.weight', 'resnet.7.1.bn3.bias', 'resnet.7.1.bn3.running_mean', 'resnet.7.1.bn3.running_var', 'resnet.7.2.conv1.weight', 'resnet.7.2.bn1.weight', 'resnet.7.2.bn1.bias', 'resnet.7.2.bn1.running_mean', 'resnet.7.2.bn1.running_var', 'resnet.7.2.conv2.weight', 'resnet.7.2.bn2.weight', 'resnet.7.2.bn2.bias', 'resnet.7.2.bn2.running_mean', 'resnet.7.2.bn2.running_var', 'resnet.7.2.conv3.weight', 'resnet.7.2.bn3.weight', 'resnet.7.2.bn3.bias', 'resnet.7.2.bn3.running_mean', 'resnet.7.2.bn3.running_var', 'linear.weight', 'linear.bias', 'bn.weight', 'bn.bias', 'bn.running_mean', 'bn.running_var'])\n",
            "Decoder keys: odict_keys(['embed.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'linear.weight', 'linear.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnQ22FkEuoru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df0d91bd",
        "outputId": "18781c81-d7ac-404c-c1d9-b79399e45533"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Path to the pytorch-tutorial directory\n",
        "pytorch_tutorial_path = '/content/pytorch-tutorial/tutorials/03-advanced/image_captioning'\n",
        "\n",
        "# Add the pytorch-tutorial directory to the system path to import necessary modules\n",
        "sys.path.insert(0, pytorch_tutorial_path)\n",
        "\n",
        "# Change the current working directory to the pytorch-tutorial directory\n",
        "os.chdir(pytorch_tutorial_path)\n",
        "\n",
        "# Import the necessary file that defines the Vocabulary class\n",
        "# Assuming the Vocabulary class is in a file named 'build_vocab.py' or similar\n",
        "# You might need to adjust the import based on the actual file name in the repository\n",
        "try:\n",
        "    from build_vocab import Vocabulary\n",
        "except ImportError:\n",
        "    # If the above import fails, try importing from data_loader\n",
        "    try:\n",
        "        from data_loader import Vocabulary\n",
        "    except ImportError:\n",
        "        print(\"Could not find the 'Vocabulary' class definition. Please check the file name.\")\n",
        "        # It's important to exit or handle this error if the class isn't found\n",
        "        # For now, we'll just print a message and continue, which might lead to further errors.\n",
        "        # A better approach might be to raise an error or stop execution.\n",
        "        pass\n",
        "\n",
        "\n",
        "# Path to the vocabulary file\n",
        "vocab_path = os.path.join('/content/pytorch-tutorial/data', 'vocab.pkl')\n",
        "\n",
        "# Load the vocabulary wrapper\n",
        "# Add error handling for file not found just in case\n",
        "try:\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "    # You can inspect some words in the vocabulary if needed\n",
        "    # print(vocab.idx2word[:10])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Vocabulary file not found at {vocab_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the vocabulary: {e}\")\n",
        "\n",
        "# It's generally good practice to change back to the original directory if needed\n",
        "# os.chdir('/content') # Uncomment if you need to change back"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/decoder-5-3000.pkl /content/encoder-5-3000.pkl /content/pytorch-tutorial/tutorials/03-advanced/image_captioning/models/"
      ],
      "metadata": {
        "id": "hhj0x5KSv54j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flower image - my_image.jpg\n",
        "!python sample.py --image my_image.jpg > sample_predictions.txt"
      ],
      "metadata": {
        "id": "k9IkDt-YwQ0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433ef2de-04c3-4c60-f6f1-c76ca6b3fca7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate what to do if you want to run it with Keras\n",
        "I have tried to implement it in PyTorch, but please investigate what steps I should take if I want to run it in Keras. In particular, please mention how to make the trained weights in PyTorch usable in Keras."
      ],
      "metadata": {
        "id": "3Vzfq5gsERtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running **image captioning** in **Keras** instead of PyTorch involves several challenges, especially when it comes to using **pre-trained PyTorch weights in Keras**. PyTorch and Keras (TensorFlow backend) use different model definitions, serialization formats, and internal layer representations.\n",
        "\n",
        "---\n",
        "### Summary of Steps\n",
        "\n",
        "#### 1. **Find or Build a Keras Image Captioning Model**\n",
        "\n",
        "Since Keras doesn’t have an official end-to-end image captioning implementation with pre-trained weights like Yunjey’s PyTorch version, you have two choices:\n",
        "\n",
        "* **Option A: Build from scratch in Keras**\n",
        "* **Option B: Convert PyTorch model + weights to Keras** (complex and error-prone)\n",
        "\n",
        "---\n",
        "\n",
        "### Option A: Build Image Captioning Model in Keras\n",
        "\n",
        "#### **High-level architecture**:\n",
        "\n",
        "1. **Encoder**: Pre-trained CNN (e.g., InceptionV3 or ResNet50)\n",
        "2. **Decoder**: RNN (typically LSTM) with an attention mechanism\n",
        "3. **Output**: Word-by-word caption generation\n",
        "\n",
        "#### Resources:\n",
        "\n",
        "* **Keras example** (without pre-trained decoder weights):\n",
        "  [Image captioning with visual attention (Keras)](https://keras.io/examples/vision/image_captioning/)\n",
        "\n",
        "#### Key Steps:\n",
        "\n",
        "1. Use **InceptionV3 / ResNet50** as CNN encoder\n",
        "2. Extract features from the image using the CNN\n",
        "3. Use an **LSTM decoder** with attention to generate the caption\n",
        "4. Train or fine-tune on a dataset like COCO or Flickr8k\n",
        "\n",
        "---\n",
        "\n",
        "### Option B: Convert PyTorch Weights to Keras\n",
        "\n",
        "This requires deep familiarity with both frameworks.\n",
        "\n",
        "### Conversion is difficult because:\n",
        "\n",
        "* PyTorch and Keras store weights **differently**\n",
        "* Layer names and architectures **don’t match 1:1**\n",
        "* No official tool exists for **automatic weight translation**\n",
        "\n",
        "###  Possible Workarounds:\n",
        "\n",
        "1. **Manually port weights** layer-by-layer:\n",
        "\n",
        "   * Export PyTorch weights (`.pt` or `.pkl`)\n",
        "   * Convert them to NumPy\n",
        "   * Load them into matching Keras layers using `set_weights()`\n",
        "   * Painstaking and error-prone\n",
        "\n",
        "2. **Use ONNX as an intermediate**:\n",
        "\n",
        "   * Convert PyTorch → ONNX\n",
        "\n",
        "   * Try ONNX → TensorFlow (via `onnx-tf`)\n",
        "\n",
        "   * Then load model into Keras\n",
        "\n",
        "   > But ONNX → Keras conversion is very fragile for custom models.\n",
        "\n",
        "### Example conversion path:\n",
        "\n",
        "```bash\n",
        "# Convert PyTorch to ONNX\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
        "\n",
        "# Convert ONNX to TensorFlow\n",
        "onnx-tf convert -i model.onnx -o tf_model\n",
        "\n",
        "# Attempt to load TF model in Keras\n",
        "tf.keras.models.load_model('tf_model')\n",
        "```\n",
        "\n",
        "This might work for **simple feed-forward models**, but often fails with **custom models like encoder-decoder architectures with attention**.\n",
        "\n",
        "---\n",
        "\n",
        "### Recommended Path\n",
        "\n",
        "If aiming to work in **Keras**, the **best approach** is:\n",
        "\n",
        "#### Use this official example from Keras:\n",
        "\n",
        " [Image Captioning with Visual Attention (Keras)](https://keras.io/examples/vision/image_captioning/)\n",
        "\n",
        "This implementation:\n",
        "\n",
        "* Uses TensorFlow/Keras\n",
        "* Extracts image features with InceptionV3\n",
        "* Uses a custom LSTM decoder with Bahdanau attention\n",
        "* Can be fine-tuned or extended\n",
        "\n",
        "### If we want to reuse PyTorch-trained captions or vocabulary:\n",
        "\n",
        "* Export the vocabulary (word2idx) as a JSON or pickle\n",
        "* Load it into your Keras model\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Task            | PyTorch             | Keras                        | Notes                                                |\n",
        "| --------------- | ------------------- | ---------------------------- | ---------------------------------------------------- |\n",
        "| Encoder CNN     | Pre-trained ResNet  | Pre-trained InceptionV3      | Replaceable                                          |\n",
        "| Decoder RNN     | Custom LSTM         | LSTM w/ attention            | Must build anew in Keras                             |\n",
        "| Weight transfer | Pickle/pt           | HDF5                         | Manual conversion or retraining required             |\n",
        "| Model loading   | `torch.load`        | `tf.keras.models.load_model` | Incompatible formats                                 |\n",
        "| Best option     | Use PyTorch version | Use Keras example            | Start fresh in Keras if you want to stay in TF/Keras |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-JTPHlpZEyPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(Advanced assignment) Code reading and rewriting\n",
        "The model part is written in [model.py], but please think about how to write this model in Keras and write the actual code. At this time, the machine translated sample code will be helpful."
      ],
      "metadata": {
        "id": "M9p23OYrGnKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll implement:\n",
        "\n",
        "1. **EncoderCNN** using a pre-trained **ResNet152**\n",
        "2. **DecoderRNN** using **Embedding + LSTM**\n",
        "3. Greedy caption sampling\n",
        "\n",
        "---\n",
        "\n",
        "#### Architecture Notes\n",
        "\n",
        "| PyTorch                             | Keras                                                 |\n",
        "| ----------------------------------- | ----------------------------------------------------- |\n",
        "| `models.resnet152(pretrained=True)` | `tf.keras.applications.ResNet152(weights='imagenet')` |\n",
        "| `nn.Linear(...)`                    | `Dense(...)`                                          |\n",
        "| `nn.LSTM(...)`                      | `tf.keras.layers.LSTM(...)`                           |\n",
        "| `Embedding(vocab_size, embed_dim)`  | `tf.keras.layers.Embedding(...)`                      |\n",
        "| `sample()`                          | Greedy decoding in Keras with `predict()` and loops   |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rXgMLKGcG1uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Keras Implementation of `model.py`\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "\n",
        "\n",
        "class EncoderCNN(tf.keras.Model):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        # Load ResNet152 without the top layer\n",
        "        base_model = ResNet152(include_top=False, weights='imagenet', pooling='avg')\n",
        "        base_model.trainable = False  # Freeze base model\n",
        "        self.resnet = base_model\n",
        "        self.fc = layers.Dense(embed_size)\n",
        "        self.bn = layers.BatchNormalization(momentum=0.01)\n",
        "\n",
        "    def call(self, images):\n",
        "        x = preprocess_input(images)  # ResNet preprocessing\n",
        "        features = self.resnet(x)\n",
        "        features = self.fc(features)\n",
        "        features = self.bn(features)\n",
        "        return features  # shape: (batch_size, embed_size)\n",
        "\n",
        "\n",
        "class DecoderRNN(tf.keras.Model):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, max_seq_length=20):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = layers.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "        self.dense = layers.Dense(vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def call(self, features, captions, lengths):\n",
        "        # Remove the last token (e.g. <end>) during training\n",
        "        captions_input = captions[:, :-1]\n",
        "        embeddings = self.embed(captions_input)\n",
        "\n",
        "        # Prepend image features as the first \"word\"\n",
        "        features = tf.expand_dims(features, 1)\n",
        "        inputs = tf.concat([features, embeddings], axis=1)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        outputs, _, _ = self.lstm(inputs)\n",
        "        logits = self.dense(outputs)\n",
        "        return logits  # shape: (batch_size, caption_len, vocab_size)\n",
        "\n",
        "    def sample(self, features, start_token, end_token):\n",
        "        # Greedy decoding loop\n",
        "        input_word = tf.expand_dims([start_token], 0)  # shape: (1, 1)\n",
        "        caption = []\n",
        "\n",
        "        state_h, state_c = None, None\n",
        "        inputs = tf.expand_dims(features, 1)\n",
        "\n",
        "        for _ in range(self.max_seq_length):\n",
        "            if state_h is None:\n",
        "                output, state_h, state_c = self.lstm(inputs)\n",
        "            else:\n",
        "                output, state_h, state_c = self.lstm(inputs, initial_state=[state_h, state_c])\n",
        "\n",
        "            logits = self.dense(output)  # (1, 1, vocab_size)\n",
        "            predicted_id = tf.argmax(logits[0, 0]).numpy()\n",
        "            caption.append(predicted_id)\n",
        "\n",
        "            if predicted_id == end_token:\n",
        "                break\n",
        "\n",
        "            inputs = tf.expand_dims(self.embed([predicted_id]), 1)  # shape: (1, 1, embed_size)\n",
        "\n",
        "        return caption"
      ],
      "metadata": {
        "id": "aE8KLtwXF0Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Usage\n",
        "\n",
        "Here's how you'd use the models after building and compiling:"
      ],
      "metadata": {
        "id": "d9gxvreOHljF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "EMBED_SIZE = 256\n",
        "HIDDEN_SIZE = 512\n",
        "VOCAB_SIZE = len(word2idx)\n",
        "MAX_SEQ_LEN = 20\n",
        "\n",
        "# Instantiate models\n",
        "encoder = EncoderCNN(embed_size=EMBED_SIZE)\n",
        "decoder = DecoderRNN(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE, max_seq_length=MAX_SEQ_LEN)\n",
        "\n",
        "# Load and preprocess image\n",
        "img = tf.keras.preprocessing.image.load_img(\"my_image.jpg\", target_size=(224, 224))\n",
        "img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "img = tf.expand_dims(img, 0)  # batch dimension\n",
        "\n",
        "# Extract features\n",
        "features = encoder(img)\n",
        "\n",
        "# Generate caption\n",
        "start_token = word2idx['<start>']\n",
        "end_token = word2idx['<end>']\n",
        "generated_ids = decoder.sample(features, start_token, end_token)\n",
        "\n",
        "# Convert tokens to words\n",
        "caption = [idx2word[i] for i in generated_ids]\n",
        "print(' '.join(caption))\n"
      ],
      "metadata": {
        "id": "_LwK8I2IHwHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART 1: Machine Translation – Translating Between Japanese and English\n",
        "\n",
        "#### General Steps\n",
        "\n",
        "When building a **machine translation system** (e.g., Japanese ⇄ English), here’s what you need:\n",
        "\n",
        "1. **Data**\n",
        "\n",
        "   * Parallel corpora: sentence pairs in Japanese and English (e.g., [JParaCrawl](https://opus.nlpl.eu/JParaCrawl.php), [Tatoeba](https://tatoeba.org/))\n",
        "   * Preprocessing: tokenization, subword units (e.g., SentencePiece or Byte-Pair Encoding)\n",
        "\n",
        "2. **Model Choices**\n",
        "\n",
        "   * **Seq2Seq with Attention**\n",
        "   * **Transformer models** (e.g., T5, MarianMT, mBART, mT5)\n",
        "\n",
        "3. **Training**\n",
        "\n",
        "   * Loss: Cross-entropy loss over vocabulary\n",
        "   * Metrics: BLEU, METEOR, or COMET scores\n",
        "\n",
        "4. **Inference**\n",
        "\n",
        "   * Greedy decoding / Beam Search\n",
        "   * Token to string conversion\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Methods of Machine Translation\n",
        "\n",
        "#### 1. **Attention Mechanism**\n",
        "\n",
        "* Introduced in [Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)\n",
        "* Learns where to focus in the source sentence when generating each word in the target sentence.\n",
        "* Led to significantly improved performance over vanilla Seq2Seq models.\n",
        "\n",
        "#### 2. **Transformer Model (Vaswani et al., 2017)**\n",
        "\n",
        "* Uses **multi-head self-attention** instead of RNNs.\n",
        "* Faster training (parallelizable), better long-range dependency modeling.\n",
        "* Foundation of modern translation models: BERT, GPT, T5, etc.\n",
        "\n",
        "#### 3. **Pre-trained Multilingual Models**\n",
        "\n",
        "* **MarianMT**: Trained on many language pairs using the Transformer architecture.\n",
        "* **mBART**: Pre-trained on denoising multilingual text; fine-tuned for translation tasks.\n",
        "* **mT5**: A multilingual variant of the T5 model by Google.\n",
        "* These support zero-shot and few-shot translation across many languages.\n",
        "\n",
        "---\n",
        "\n",
        "### Evolutionary Approaches (Beyond Transformers)\n",
        "\n",
        "Some **experimental or hybrid techniques** being researched:\n",
        "\n",
        "* **Neuroevolution of Augmenting Topologies (NEAT)** for evolving encoder-decoder architectures\n",
        "* **Reinforcement learning-based translation** where BLEU or human ratings are the reward\n",
        "* **Meta-learning** for low-resource language pairs\n",
        "* **Multimodal translation** (text + image or speech → text translation)\n",
        "\n",
        "---\n",
        "\n",
        "### PART 2: Generating Images from Text (Opposite of Image Captioning)\n",
        "\n",
        "This falls under **text-to-image synthesis**, and has advanced rapidly in recent years.\n",
        "\n",
        "### Key Technologies\n",
        "\n",
        "| Model                                      | Description                                                                                  |\n",
        "| ------------------------------------------ | -------------------------------------------------------------------------------------------- |\n",
        "| **GANs (Generative Adversarial Networks)** | Early models like StackGAN, AttnGAN used GANs with text embeddings.                          |\n",
        "| **VQ-VAE**                                 | Vector-quantized autoencoders used in early DALL·E.                                          |\n",
        "| **CLIP + Diffusion**                       | Used by modern SOTA (e.g., DALL·E 2, Stable Diffusion). CLIP connects text and image spaces. |\n",
        "| **Transformer-based Models**               | DALL·E and CogView use autoregressive transformers for image generation.                     |\n",
        "\n",
        "---\n",
        "\n",
        "### Modern Text-to-Image Models\n",
        "\n",
        "| Model                | Publisher    | Highlights                                                                         |\n",
        "| -------------------- | ------------ | ---------------------------------------------------------------------------------- |\n",
        "| **DALL·E 2 / 3**     | OpenAI       | Uses CLIP + diffusion model. High-quality and coherent images.                     |\n",
        "| **Stable Diffusion** | Stability AI | Open-source, supports custom models and fine-tuning.                               |\n",
        "| **Midjourney**       | Independent  | Artistic, stylized images. Prompt-based.                                           |\n",
        "| **Imagen**           | Google       | State-of-the-art text-to-image with unprecedented realism (not publicly released). |\n",
        "| **DeepFloyd IF**     | DeepFloyd    | Modular diffusion-based image generation model.                                    |\n",
        "\n",
        "---\n",
        "\n",
        "### Workflow Overview\n",
        "\n",
        "1. **Input**: Text prompt (e.g., “A robot reading a book under a cherry blossom tree”)\n",
        "2. **Text encoder**: Convert prompt into embeddings (CLIP, T5, etc.)\n",
        "3. **Image generator**: Use diffusion or autoregressive model to generate an image\n",
        "4. **Upsampling**: Increase resolution using super-resolution models\n",
        "\n",
        "---\n",
        "\n",
        "### Practical Toolkits\n",
        "\n",
        "* `diffusers` library by HuggingFace (for Stable Diffusion, DeepFloyd IF)\n",
        "* OpenAI’s DALL·E API\n",
        "* RunwayML (no-code tools)\n",
        "\n",
        "---\n",
        "\n",
        "### Connecting the Dots\n",
        "\n",
        "| Captioning        | Translation         | Text-to-Image           |\n",
        "| ----------------- | ------------------- | ----------------------- |\n",
        "| Image → Text      | Text → Text         | Text → Image            |\n",
        "| CNN + RNN         | Transformer         | Transformer + Diffusion |\n",
        "| e.g., Show & Tell | e.g., MarianMT, mT5 | e.g., DALL·E, SD        |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PTuQWDasH5uC"
      }
    }
  ]
}