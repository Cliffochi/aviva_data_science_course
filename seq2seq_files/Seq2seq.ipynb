{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Machine translation execution and code reading\n",
        "The following sample code does a short English to French translation."
      ],
      "metadata": {
        "id": "ySup2JZySmJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Title: Character-level recurrent sequence-to-sequence model\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2017/09/29\n",
        "Last modified: 2023/11/22\n",
        "Description: Character-level recurrent sequence-to-sequence model.\n",
        "Accelerator: GPU\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Download the data\n",
        "data_path = keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        "    cache_dir='.'\n",
        ")\n",
        "dirpath = Path(data_path).parent.absolute()\n",
        "data_file_path = os.path.join(dirpath, 'fra-eng', 'fra.txt')\n",
        "\n",
        "# Configuration\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "\n",
        "# Prepare the data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "\n",
        "# Bonus: Print first line for verification\n",
        "print(\"Sample line from data file:\", lines[0])\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) >= 2:\n",
        "        input_text, target_text = parts[0], parts[1]\n",
        "        # Use \"\\t\" as start sequence character for target, \"\\n\" as end sequence character\n",
        "        target_text = \"\\t\" + target_text + \"\\n\"\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "        for char in input_text:\n",
        "            if char not in input_characters:\n",
        "                input_characters.add(char)\n",
        "        for char in target_text:\n",
        "            if char not in target_characters:\n",
        "                target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "if not input_texts:\n",
        "    print(\"Error: No valid input sentences found in the data file.\")\n",
        "else:\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "    print(\"Number of samples:\", len(input_texts))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text):\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "    # Build the model\n",
        "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Train the model\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.2,\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model.save(\"s2s_model.keras\")\n",
        "\n",
        "    # Run inference\n",
        "    model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]  # input_2\n",
        "    decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.layers[3]\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h_dec, state_c_dec]\n",
        "    decoder_dense = model.layers[4]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "    def decode_sequence(input_seq):\n",
        "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        while not stop_condition:\n",
        "            output_tokens, h, c = decoder_model.predict(\n",
        "                [target_seq] + states_value, verbose=0\n",
        "            )\n",
        "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "            if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "                stop_condition = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "            target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "            states_value = [h, c]\n",
        "        return decoded_sentence\n",
        "\n",
        "    for seq_index in range(20):\n",
        "        input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "        decoded_sentence = decode_sequence(input_seq)\n",
        "        print(\"-\")\n",
        "        print(\"Input sentence:\", input_texts[seq_index])\n",
        "        print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHWoNdoLcRON",
        "outputId": "c3f35737-828a-4f82-ec4e-1cbe5b5835ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample line from data file: Go.\tVa !\n",
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 93\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 59\n",
            "Epoch 1/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.6910 - loss: 1.6353 - val_accuracy: 0.6964 - val_loss: 1.1517\n",
            "Epoch 2/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.7325 - loss: 1.0175 - val_accuracy: 0.6962 - val_loss: 1.1024\n",
            "Epoch 3/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7511 - loss: 0.9073 - val_accuracy: 0.7436 - val_loss: 0.9389\n",
            "Epoch 4/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7790 - loss: 0.8036 - val_accuracy: 0.7571 - val_loss: 0.8492\n",
            "Epoch 5/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.7844 - loss: 0.7556 - val_accuracy: 0.7708 - val_loss: 0.7879\n",
            "Epoch 6/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8059 - loss: 0.6698 - val_accuracy: 0.7769 - val_loss: 0.7588\n",
            "Epoch 7/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8130 - loss: 0.6381 - val_accuracy: 0.7868 - val_loss: 0.7211\n",
            "Epoch 8/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8199 - loss: 0.6142 - val_accuracy: 0.7965 - val_loss: 0.6913\n",
            "Epoch 9/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8264 - loss: 0.5916 - val_accuracy: 0.8005 - val_loss: 0.6713\n",
            "Epoch 10/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8315 - loss: 0.5724 - val_accuracy: 0.8100 - val_loss: 0.6523\n",
            "Epoch 11/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8383 - loss: 0.5491 - val_accuracy: 0.8132 - val_loss: 0.6317\n",
            "Epoch 12/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8434 - loss: 0.5315 - val_accuracy: 0.8199 - val_loss: 0.6210\n",
            "Epoch 13/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8496 - loss: 0.5147 - val_accuracy: 0.8260 - val_loss: 0.6022\n",
            "Epoch 14/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8516 - loss: 0.5060 - val_accuracy: 0.8282 - val_loss: 0.5875\n",
            "Epoch 15/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8547 - loss: 0.4950 - val_accuracy: 0.8310 - val_loss: 0.5751\n",
            "Epoch 16/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8577 - loss: 0.4838 - val_accuracy: 0.8330 - val_loss: 0.5675\n",
            "Epoch 17/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8602 - loss: 0.4726 - val_accuracy: 0.8373 - val_loss: 0.5562\n",
            "Epoch 18/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8628 - loss: 0.4620 - val_accuracy: 0.8400 - val_loss: 0.5447\n",
            "Epoch 19/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8665 - loss: 0.4519 - val_accuracy: 0.8424 - val_loss: 0.5390\n",
            "Epoch 20/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8667 - loss: 0.4489 - val_accuracy: 0.8424 - val_loss: 0.5348\n",
            "Epoch 21/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8719 - loss: 0.4334 - val_accuracy: 0.8446 - val_loss: 0.5312\n",
            "Epoch 22/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8734 - loss: 0.4293 - val_accuracy: 0.8463 - val_loss: 0.5227\n",
            "Epoch 23/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8745 - loss: 0.4219 - val_accuracy: 0.8478 - val_loss: 0.5209\n",
            "Epoch 24/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8770 - loss: 0.4147 - val_accuracy: 0.8481 - val_loss: 0.5178\n",
            "Epoch 25/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8781 - loss: 0.4094 - val_accuracy: 0.8529 - val_loss: 0.5054\n",
            "Epoch 26/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8814 - loss: 0.3983 - val_accuracy: 0.8502 - val_loss: 0.5110\n",
            "Epoch 27/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8826 - loss: 0.3953 - val_accuracy: 0.8540 - val_loss: 0.4988\n",
            "Epoch 28/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8851 - loss: 0.3856 - val_accuracy: 0.8547 - val_loss: 0.4965\n",
            "Epoch 29/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8866 - loss: 0.3823 - val_accuracy: 0.8545 - val_loss: 0.4947\n",
            "Epoch 30/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8880 - loss: 0.3748 - val_accuracy: 0.8570 - val_loss: 0.4901\n",
            "Epoch 31/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8887 - loss: 0.3741 - val_accuracy: 0.8576 - val_loss: 0.4885\n",
            "Epoch 32/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8916 - loss: 0.3645 - val_accuracy: 0.8584 - val_loss: 0.4869\n",
            "Epoch 33/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8922 - loss: 0.3611 - val_accuracy: 0.8581 - val_loss: 0.4871\n",
            "Epoch 34/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8929 - loss: 0.3583 - val_accuracy: 0.8599 - val_loss: 0.4793\n",
            "Epoch 35/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8951 - loss: 0.3505 - val_accuracy: 0.8607 - val_loss: 0.4811\n",
            "Epoch 36/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8966 - loss: 0.3476 - val_accuracy: 0.8612 - val_loss: 0.4773\n",
            "Epoch 37/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8981 - loss: 0.3404 - val_accuracy: 0.8612 - val_loss: 0.4780\n",
            "Epoch 38/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8993 - loss: 0.3370 - val_accuracy: 0.8630 - val_loss: 0.4731\n",
            "Epoch 39/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9000 - loss: 0.3340 - val_accuracy: 0.8634 - val_loss: 0.4719\n",
            "Epoch 40/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9022 - loss: 0.3277 - val_accuracy: 0.8635 - val_loss: 0.4706\n",
            "Epoch 41/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9030 - loss: 0.3236 - val_accuracy: 0.8632 - val_loss: 0.4718\n",
            "Epoch 42/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9037 - loss: 0.3212 - val_accuracy: 0.8641 - val_loss: 0.4713\n",
            "Epoch 43/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9043 - loss: 0.3187 - val_accuracy: 0.8641 - val_loss: 0.4726\n",
            "Epoch 44/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9059 - loss: 0.3135 - val_accuracy: 0.8648 - val_loss: 0.4716\n",
            "Epoch 45/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9079 - loss: 0.3077 - val_accuracy: 0.8652 - val_loss: 0.4683\n",
            "Epoch 46/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9085 - loss: 0.3047 - val_accuracy: 0.8649 - val_loss: 0.4715\n",
            "Epoch 47/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9103 - loss: 0.2999 - val_accuracy: 0.8644 - val_loss: 0.4732\n",
            "Epoch 48/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9107 - loss: 0.2980 - val_accuracy: 0.8644 - val_loss: 0.4733\n",
            "Epoch 49/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9127 - loss: 0.2912 - val_accuracy: 0.8657 - val_loss: 0.4705\n",
            "Epoch 50/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9122 - loss: 0.2917 - val_accuracy: 0.8664 - val_loss: 0.4677\n",
            "Epoch 51/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9145 - loss: 0.2844 - val_accuracy: 0.8665 - val_loss: 0.4700\n",
            "Epoch 52/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9154 - loss: 0.2809 - val_accuracy: 0.8674 - val_loss: 0.4682\n",
            "Epoch 53/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9163 - loss: 0.2787 - val_accuracy: 0.8666 - val_loss: 0.4731\n",
            "Epoch 54/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9169 - loss: 0.2759 - val_accuracy: 0.8672 - val_loss: 0.4709\n",
            "Epoch 55/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9187 - loss: 0.2706 - val_accuracy: 0.8664 - val_loss: 0.4733\n",
            "Epoch 56/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9192 - loss: 0.2691 - val_accuracy: 0.8684 - val_loss: 0.4696\n",
            "Epoch 57/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9206 - loss: 0.2641 - val_accuracy: 0.8677 - val_loss: 0.4715\n",
            "Epoch 58/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9215 - loss: 0.2597 - val_accuracy: 0.8678 - val_loss: 0.4757\n",
            "Epoch 59/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9220 - loss: 0.2598 - val_accuracy: 0.8684 - val_loss: 0.4745\n",
            "Epoch 60/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9225 - loss: 0.2572 - val_accuracy: 0.8688 - val_loss: 0.4763\n",
            "Epoch 61/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9247 - loss: 0.2502 - val_accuracy: 0.8683 - val_loss: 0.4758\n",
            "Epoch 62/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9252 - loss: 0.2494 - val_accuracy: 0.8683 - val_loss: 0.4796\n",
            "Epoch 63/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9253 - loss: 0.2466 - val_accuracy: 0.8690 - val_loss: 0.4775\n",
            "Epoch 64/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9271 - loss: 0.2414 - val_accuracy: 0.8687 - val_loss: 0.4777\n",
            "Epoch 65/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9271 - loss: 0.2414 - val_accuracy: 0.8686 - val_loss: 0.4819\n",
            "Epoch 66/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9286 - loss: 0.2374 - val_accuracy: 0.8689 - val_loss: 0.4840\n",
            "Epoch 67/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9296 - loss: 0.2346 - val_accuracy: 0.8686 - val_loss: 0.4830\n",
            "Epoch 68/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9293 - loss: 0.2340 - val_accuracy: 0.8681 - val_loss: 0.4858\n",
            "Epoch 69/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9303 - loss: 0.2300 - val_accuracy: 0.8694 - val_loss: 0.4851\n",
            "Epoch 70/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9312 - loss: 0.2288 - val_accuracy: 0.8682 - val_loss: 0.4897\n",
            "Epoch 71/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9329 - loss: 0.2228 - val_accuracy: 0.8680 - val_loss: 0.4916\n",
            "Epoch 72/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9327 - loss: 0.2232 - val_accuracy: 0.8691 - val_loss: 0.4917\n",
            "Epoch 73/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9338 - loss: 0.2188 - val_accuracy: 0.8681 - val_loss: 0.4935\n",
            "Epoch 74/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9342 - loss: 0.2180 - val_accuracy: 0.8685 - val_loss: 0.4972\n",
            "Epoch 75/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9355 - loss: 0.2131 - val_accuracy: 0.8682 - val_loss: 0.4993\n",
            "Epoch 76/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9355 - loss: 0.2125 - val_accuracy: 0.8687 - val_loss: 0.5001\n",
            "Epoch 77/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9362 - loss: 0.2109 - val_accuracy: 0.8691 - val_loss: 0.5001\n",
            "Epoch 78/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9370 - loss: 0.2092 - val_accuracy: 0.8687 - val_loss: 0.5050\n",
            "Epoch 79/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9379 - loss: 0.2051 - val_accuracy: 0.8682 - val_loss: 0.5073\n",
            "Epoch 80/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9384 - loss: 0.2036 - val_accuracy: 0.8688 - val_loss: 0.5076\n",
            "Epoch 81/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9389 - loss: 0.2018 - val_accuracy: 0.8678 - val_loss: 0.5071\n",
            "Epoch 82/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9401 - loss: 0.1996 - val_accuracy: 0.8675 - val_loss: 0.5111\n",
            "Epoch 83/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9407 - loss: 0.1951 - val_accuracy: 0.8685 - val_loss: 0.5132\n",
            "Epoch 84/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9414 - loss: 0.1931 - val_accuracy: 0.8686 - val_loss: 0.5141\n",
            "Epoch 85/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9418 - loss: 0.1911 - val_accuracy: 0.8681 - val_loss: 0.5151\n",
            "Epoch 86/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9424 - loss: 0.1900 - val_accuracy: 0.8683 - val_loss: 0.5210\n",
            "Epoch 87/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9432 - loss: 0.1872 - val_accuracy: 0.8684 - val_loss: 0.5186\n",
            "Epoch 88/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9437 - loss: 0.1850 - val_accuracy: 0.8680 - val_loss: 0.5261\n",
            "Epoch 89/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9439 - loss: 0.1843 - val_accuracy: 0.8685 - val_loss: 0.5252\n",
            "Epoch 90/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9444 - loss: 0.1829 - val_accuracy: 0.8682 - val_loss: 0.5264\n",
            "Epoch 91/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9451 - loss: 0.1792 - val_accuracy: 0.8688 - val_loss: 0.5258\n",
            "Epoch 92/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9462 - loss: 0.1775 - val_accuracy: 0.8679 - val_loss: 0.5326\n",
            "Epoch 93/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9460 - loss: 0.1771 - val_accuracy: 0.8682 - val_loss: 0.5307\n",
            "Epoch 94/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9476 - loss: 0.1727 - val_accuracy: 0.8691 - val_loss: 0.5360\n",
            "Epoch 95/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9479 - loss: 0.1724 - val_accuracy: 0.8675 - val_loss: 0.5413\n",
            "Epoch 96/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9480 - loss: 0.1701 - val_accuracy: 0.8685 - val_loss: 0.5377\n",
            "Epoch 97/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9484 - loss: 0.1688 - val_accuracy: 0.8691 - val_loss: 0.5395\n",
            "Epoch 98/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9492 - loss: 0.1666 - val_accuracy: 0.8684 - val_loss: 0.5429\n",
            "Epoch 99/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9495 - loss: 0.1657 - val_accuracy: 0.8696 - val_loss: 0.5429\n",
            "Epoch 100/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9500 - loss: 0.1636 - val_accuracy: 0.8683 - val_loss: 0.5506\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Va t'uc fris !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Malle marcher.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Passiez-me.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Passiez-me.\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Qu'est-ce que l'amour ?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Attendez à l'impotie à Tom !\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Reculez.\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Appellez-moi.\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Emproce.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrêtez !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrêtez !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Arrêtez !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attendez !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attendez !\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Vas-y !\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Vas-y !\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Vas-y !\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Appellez-me !\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Appellez-me !\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Je l'ai confient.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The official Keras sample code is running an implementation that is doing a short English to - French conversion."
      ],
      "metadata": {
        "id": "zLAWF50emdL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Code Explanation by Section**\n",
        "\n",
        "**Lines 51–55: Importing libraries**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "```\n",
        "\n",
        "These lines import essential libraries for numerical operations and deep learning using TensorFlow/Keras. LSTM is used to build the sequence-to-sequence model.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 57–62: Hyperparameter settings**\n",
        "\n",
        "```python\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "data_path = keras.utils.get_file(\n",
        "    \"fra-eng/fra.txt\", \"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\", extract=True\n",
        ")\n",
        "```\n",
        "\n",
        "Defines parameters such as:\n",
        "\n",
        "* `batch_size`: Number of samples per training batch\n",
        "* `epochs`: Number of iterations over the full dataset\n",
        "* `latent_dim`: Hidden layer size of LSTM\n",
        "* `num_samples`: Number of sentence pairs to use\n",
        "* `data_path`: Downloads and extracts English-French translation dataset\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 64–95: Reading and preprocessing the data**\n",
        "\n",
        "```python\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "# Read and clean data...\n",
        "```\n",
        "\n",
        "* Loads the dataset and creates parallel lists for input (English) and target (French) sentences.\n",
        "* Also builds character sets for tokenization.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 97–110: Sort characters and assign indices**\n",
        "\n",
        "```python\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "```\n",
        "\n",
        "* Assigns a unique integer ID to each character in both languages.\n",
        "* Useful for vectorization.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 112–125: Vectorize input and output**\n",
        "\n",
        "```python\n",
        "encoder_input_data = np.zeros(...)\n",
        "decoder_input_data = np.zeros(...)\n",
        "decoder_target_data = np.zeros(...)\n",
        "# Populate the arrays with one-hot encodings\n",
        "```\n",
        "\n",
        "* Converts text into one-hot encoded arrays to feed into the model.\n",
        "* Handles input for encoder and decoder.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 127–137: Build the encoder model**\n",
        "\n",
        "```python\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "```\n",
        "\n",
        "* The encoder processes input sequences and returns internal LSTM states.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 139–151: Build the decoder model**\n",
        "\n",
        "```python\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "```\n",
        "\n",
        "* Decoder uses the encoder's internal states to generate output sequences.\n",
        "* Applies a dense softmax layer for final output prediction.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 153–155: Define and compile the model**\n",
        "\n",
        "```python\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "* Wraps the encoder-decoder into a full model and compiles it with categorical loss and accuracy metric.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 157–159: Train the model**\n",
        "\n",
        "```python\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "```\n",
        "\n",
        "* Begins training the model with the input and target data.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 161–174: Define inference models (for translation)**\n",
        "\n",
        "* Separates encoder and decoder models for inference (translating new sentences).\n",
        "* Necessary because in inference, you generate one token at a time.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 176–196: Decode sequence function**\n",
        "\n",
        "* Translates input sentences using the trained model and inference loop.\n",
        "\n",
        "---\n",
        "\n",
        "**Lines 198–202: Display sample translations**\n",
        "\n",
        "* Tests the model on a few input sequences and prints the predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## Character-Level Tokenization with `CountVectorizer`\n",
        "\n",
        "When using `sklearn.feature_extraction.text.CountVectorizer`, you can specify how text is tokenized using the `analyzer` argument.\n",
        "\n",
        "### 🔸 `analyzer='char'`\n",
        "\n",
        "* **Character n-grams** across words.\n",
        "* E.g., `\"This movie\"` → tokens like `'T'`, `'Th'`, `'his'`, `'is '`, `'s m'`, `' mo'`, etc.\n",
        "\n",
        "### 🔸 `analyzer='char_wb'`\n",
        "\n",
        "* **Character n-grams** only *within* word boundaries.\n",
        "* Avoids capturing patterns like `'s m'` (from `This movie`) which cross word boundaries.\n",
        "\n",
        "Use case:\n",
        "\n",
        "* **char**: Useful for language modeling and machine translation\n",
        "* **char\\_wb**: Better for morphology-sensitive tasks (e.g., spelling)\n",
        "\n",
        "**Docs:**\n",
        "[CountVectorizer – scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1yArGwQTSqal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Running a pre-trained model for image captioning"
      ],
      "metadata": {
        "id": "ZpS5UXRidUp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RNJp8iqbSHWE",
        "outputId": "d7941a99-76b8-4e8c-e37c-b1c8698a1fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "978cdffb5f9449239e65581295c983fb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# set up the environment i.e. install dependencies\n",
        "!pip install torch torchvision pillow numpy matplotlib nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Download NLTK Word Tokenizer (required for text preprocessing):"
      ],
      "metadata": {
        "id": "OC4DnQ-CdxUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_xxrNQOdxBF",
        "outputId": "c1402dad-c20c-427e-cb69-31675c948e76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yunjey/pytorch-tutorial.git\n",
        "!cd pytorch-tutorial/tutorials/03-advanced/image_captioning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5SjJaLudrGN",
        "outputId": "6a532f3b-7845-4fc4-e5bf-8cd341c0acc4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-tutorial'...\n",
            "remote: Enumerating objects: 917, done.\u001b[K\n",
            "remote: Total 917 (delta 0), reused 0 (delta 0), pack-reused 917 (from 1)\u001b[K\n",
            "Receiving objects: 100% (917/917), 12.80 MiB | 17.21 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''2. Using gdown:\n",
        "Upload to Google Drive: Upload your file to Google Drive and make it shareable with \"anyone with the link.\"\n",
        "Copy File ID: Extract the file ID from the shareable link.\n",
        "Download in Colab: Using the gdown command in a Colab cell:\n",
        "'''\n",
        "!gdown 1Wmq6aKkItmTufvachL9mFeMCT-3-g2qH\n",
        "!gdown 1iegY6ZVt1dm8cYeHu7CA2QYupJY6kDiC"
      ],
      "metadata": {
        "id": "ICKnCvJ2sVIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a72959f-94ef-4ff0-fab8-dd6297d361fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Wmq6aKkItmTufvachL9mFeMCT-3-g2qH\n",
            "From (redirected): https://drive.google.com/uc?id=1Wmq6aKkItmTufvachL9mFeMCT-3-g2qH&confirm=t&uuid=65a44b95-ecd4-4194-83a8-f84ddb10ca0a\n",
            "To: /content/encoder-5-3000.pkl\n",
            "100% 235M/235M [00:04<00:00, 51.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iegY6ZVt1dm8cYeHu7CA2QYupJY6kDiC\n",
            "To: /content/decoder-5-3000.pkl\n",
            "100% 36.9M/36.9M [00:00<00:00, 41.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r /content/pytorch-tutorial/data .\n",
        "!cp -r /content/pytorch-tutorial/tutorials/03-advanced/image_captioning/data .\n",
        "#!cp -r /content/pytorch-tutorial/models .\n",
        "!cp -r /content/pytorch-tutorial/tutorials/03-advanced/image_captioning/models ."
      ],
      "metadata": {
        "id": "LR6r8v8ifTmj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Construct the full paths to the model files\n",
        "encoder_path = '/content/encoder-5-3000.pkl'\n",
        "decoder_path = '/content/decoder-5-3000.pkl'\n",
        "\n",
        "# Load the models\n",
        "encoder = torch.load(encoder_path, map_location='cpu')  # or 'cuda'\n",
        "decoder = torch.load(decoder_path, map_location='cpu')\n",
        "\n",
        "print(\"Encoder keys:\", encoder.keys())  # Should show model weights\n",
        "print(\"Decoder keys:\", decoder.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQJsJlUpiDb",
        "outputId": "4d5e2c12-2c16-4df6-a381-2eb88e570627"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder keys: odict_keys(['resnet.0.weight', 'resnet.1.weight', 'resnet.1.bias', 'resnet.1.running_mean', 'resnet.1.running_var', 'resnet.4.0.conv1.weight', 'resnet.4.0.bn1.weight', 'resnet.4.0.bn1.bias', 'resnet.4.0.bn1.running_mean', 'resnet.4.0.bn1.running_var', 'resnet.4.0.conv2.weight', 'resnet.4.0.bn2.weight', 'resnet.4.0.bn2.bias', 'resnet.4.0.bn2.running_mean', 'resnet.4.0.bn2.running_var', 'resnet.4.0.conv3.weight', 'resnet.4.0.bn3.weight', 'resnet.4.0.bn3.bias', 'resnet.4.0.bn3.running_mean', 'resnet.4.0.bn3.running_var', 'resnet.4.0.downsample.0.weight', 'resnet.4.0.downsample.1.weight', 'resnet.4.0.downsample.1.bias', 'resnet.4.0.downsample.1.running_mean', 'resnet.4.0.downsample.1.running_var', 'resnet.4.1.conv1.weight', 'resnet.4.1.bn1.weight', 'resnet.4.1.bn1.bias', 'resnet.4.1.bn1.running_mean', 'resnet.4.1.bn1.running_var', 'resnet.4.1.conv2.weight', 'resnet.4.1.bn2.weight', 'resnet.4.1.bn2.bias', 'resnet.4.1.bn2.running_mean', 'resnet.4.1.bn2.running_var', 'resnet.4.1.conv3.weight', 'resnet.4.1.bn3.weight', 'resnet.4.1.bn3.bias', 'resnet.4.1.bn3.running_mean', 'resnet.4.1.bn3.running_var', 'resnet.4.2.conv1.weight', 'resnet.4.2.bn1.weight', 'resnet.4.2.bn1.bias', 'resnet.4.2.bn1.running_mean', 'resnet.4.2.bn1.running_var', 'resnet.4.2.conv2.weight', 'resnet.4.2.bn2.weight', 'resnet.4.2.bn2.bias', 'resnet.4.2.bn2.running_mean', 'resnet.4.2.bn2.running_var', 'resnet.4.2.conv3.weight', 'resnet.4.2.bn3.weight', 'resnet.4.2.bn3.bias', 'resnet.4.2.bn3.running_mean', 'resnet.4.2.bn3.running_var', 'resnet.5.0.conv1.weight', 'resnet.5.0.bn1.weight', 'resnet.5.0.bn1.bias', 'resnet.5.0.bn1.running_mean', 'resnet.5.0.bn1.running_var', 'resnet.5.0.conv2.weight', 'resnet.5.0.bn2.weight', 'resnet.5.0.bn2.bias', 'resnet.5.0.bn2.running_mean', 'resnet.5.0.bn2.running_var', 'resnet.5.0.conv3.weight', 'resnet.5.0.bn3.weight', 'resnet.5.0.bn3.bias', 'resnet.5.0.bn3.running_mean', 'resnet.5.0.bn3.running_var', 'resnet.5.0.downsample.0.weight', 'resnet.5.0.downsample.1.weight', 'resnet.5.0.downsample.1.bias', 'resnet.5.0.downsample.1.running_mean', 'resnet.5.0.downsample.1.running_var', 'resnet.5.1.conv1.weight', 'resnet.5.1.bn1.weight', 'resnet.5.1.bn1.bias', 'resnet.5.1.bn1.running_mean', 'resnet.5.1.bn1.running_var', 'resnet.5.1.conv2.weight', 'resnet.5.1.bn2.weight', 'resnet.5.1.bn2.bias', 'resnet.5.1.bn2.running_mean', 'resnet.5.1.bn2.running_var', 'resnet.5.1.conv3.weight', 'resnet.5.1.bn3.weight', 'resnet.5.1.bn3.bias', 'resnet.5.1.bn3.running_mean', 'resnet.5.1.bn3.running_var', 'resnet.5.2.conv1.weight', 'resnet.5.2.bn1.weight', 'resnet.5.2.bn1.bias', 'resnet.5.2.bn1.running_mean', 'resnet.5.2.bn1.running_var', 'resnet.5.2.conv2.weight', 'resnet.5.2.bn2.weight', 'resnet.5.2.bn2.bias', 'resnet.5.2.bn2.running_mean', 'resnet.5.2.bn2.running_var', 'resnet.5.2.conv3.weight', 'resnet.5.2.bn3.weight', 'resnet.5.2.bn3.bias', 'resnet.5.2.bn3.running_mean', 'resnet.5.2.bn3.running_var', 'resnet.5.3.conv1.weight', 'resnet.5.3.bn1.weight', 'resnet.5.3.bn1.bias', 'resnet.5.3.bn1.running_mean', 'resnet.5.3.bn1.running_var', 'resnet.5.3.conv2.weight', 'resnet.5.3.bn2.weight', 'resnet.5.3.bn2.bias', 'resnet.5.3.bn2.running_mean', 'resnet.5.3.bn2.running_var', 'resnet.5.3.conv3.weight', 'resnet.5.3.bn3.weight', 'resnet.5.3.bn3.bias', 'resnet.5.3.bn3.running_mean', 'resnet.5.3.bn3.running_var', 'resnet.5.4.conv1.weight', 'resnet.5.4.bn1.weight', 'resnet.5.4.bn1.bias', 'resnet.5.4.bn1.running_mean', 'resnet.5.4.bn1.running_var', 'resnet.5.4.conv2.weight', 'resnet.5.4.bn2.weight', 'resnet.5.4.bn2.bias', 'resnet.5.4.bn2.running_mean', 'resnet.5.4.bn2.running_var', 'resnet.5.4.conv3.weight', 'resnet.5.4.bn3.weight', 'resnet.5.4.bn3.bias', 'resnet.5.4.bn3.running_mean', 'resnet.5.4.bn3.running_var', 'resnet.5.5.conv1.weight', 'resnet.5.5.bn1.weight', 'resnet.5.5.bn1.bias', 'resnet.5.5.bn1.running_mean', 'resnet.5.5.bn1.running_var', 'resnet.5.5.conv2.weight', 'resnet.5.5.bn2.weight', 'resnet.5.5.bn2.bias', 'resnet.5.5.bn2.running_mean', 'resnet.5.5.bn2.running_var', 'resnet.5.5.conv3.weight', 'resnet.5.5.bn3.weight', 'resnet.5.5.bn3.bias', 'resnet.5.5.bn3.running_mean', 'resnet.5.5.bn3.running_var', 'resnet.5.6.conv1.weight', 'resnet.5.6.bn1.weight', 'resnet.5.6.bn1.bias', 'resnet.5.6.bn1.running_mean', 'resnet.5.6.bn1.running_var', 'resnet.5.6.conv2.weight', 'resnet.5.6.bn2.weight', 'resnet.5.6.bn2.bias', 'resnet.5.6.bn2.running_mean', 'resnet.5.6.bn2.running_var', 'resnet.5.6.conv3.weight', 'resnet.5.6.bn3.weight', 'resnet.5.6.bn3.bias', 'resnet.5.6.bn3.running_mean', 'resnet.5.6.bn3.running_var', 'resnet.5.7.conv1.weight', 'resnet.5.7.bn1.weight', 'resnet.5.7.bn1.bias', 'resnet.5.7.bn1.running_mean', 'resnet.5.7.bn1.running_var', 'resnet.5.7.conv2.weight', 'resnet.5.7.bn2.weight', 'resnet.5.7.bn2.bias', 'resnet.5.7.bn2.running_mean', 'resnet.5.7.bn2.running_var', 'resnet.5.7.conv3.weight', 'resnet.5.7.bn3.weight', 'resnet.5.7.bn3.bias', 'resnet.5.7.bn3.running_mean', 'resnet.5.7.bn3.running_var', 'resnet.6.0.conv1.weight', 'resnet.6.0.bn1.weight', 'resnet.6.0.bn1.bias', 'resnet.6.0.bn1.running_mean', 'resnet.6.0.bn1.running_var', 'resnet.6.0.conv2.weight', 'resnet.6.0.bn2.weight', 'resnet.6.0.bn2.bias', 'resnet.6.0.bn2.running_mean', 'resnet.6.0.bn2.running_var', 'resnet.6.0.conv3.weight', 'resnet.6.0.bn3.weight', 'resnet.6.0.bn3.bias', 'resnet.6.0.bn3.running_mean', 'resnet.6.0.bn3.running_var', 'resnet.6.0.downsample.0.weight', 'resnet.6.0.downsample.1.weight', 'resnet.6.0.downsample.1.bias', 'resnet.6.0.downsample.1.running_mean', 'resnet.6.0.downsample.1.running_var', 'resnet.6.1.conv1.weight', 'resnet.6.1.bn1.weight', 'resnet.6.1.bn1.bias', 'resnet.6.1.bn1.running_mean', 'resnet.6.1.bn1.running_var', 'resnet.6.1.conv2.weight', 'resnet.6.1.bn2.weight', 'resnet.6.1.bn2.bias', 'resnet.6.1.bn2.running_mean', 'resnet.6.1.bn2.running_var', 'resnet.6.1.conv3.weight', 'resnet.6.1.bn3.weight', 'resnet.6.1.bn3.bias', 'resnet.6.1.bn3.running_mean', 'resnet.6.1.bn3.running_var', 'resnet.6.2.conv1.weight', 'resnet.6.2.bn1.weight', 'resnet.6.2.bn1.bias', 'resnet.6.2.bn1.running_mean', 'resnet.6.2.bn1.running_var', 'resnet.6.2.conv2.weight', 'resnet.6.2.bn2.weight', 'resnet.6.2.bn2.bias', 'resnet.6.2.bn2.running_mean', 'resnet.6.2.bn2.running_var', 'resnet.6.2.conv3.weight', 'resnet.6.2.bn3.weight', 'resnet.6.2.bn3.bias', 'resnet.6.2.bn3.running_mean', 'resnet.6.2.bn3.running_var', 'resnet.6.3.conv1.weight', 'resnet.6.3.bn1.weight', 'resnet.6.3.bn1.bias', 'resnet.6.3.bn1.running_mean', 'resnet.6.3.bn1.running_var', 'resnet.6.3.conv2.weight', 'resnet.6.3.bn2.weight', 'resnet.6.3.bn2.bias', 'resnet.6.3.bn2.running_mean', 'resnet.6.3.bn2.running_var', 'resnet.6.3.conv3.weight', 'resnet.6.3.bn3.weight', 'resnet.6.3.bn3.bias', 'resnet.6.3.bn3.running_mean', 'resnet.6.3.bn3.running_var', 'resnet.6.4.conv1.weight', 'resnet.6.4.bn1.weight', 'resnet.6.4.bn1.bias', 'resnet.6.4.bn1.running_mean', 'resnet.6.4.bn1.running_var', 'resnet.6.4.conv2.weight', 'resnet.6.4.bn2.weight', 'resnet.6.4.bn2.bias', 'resnet.6.4.bn2.running_mean', 'resnet.6.4.bn2.running_var', 'resnet.6.4.conv3.weight', 'resnet.6.4.bn3.weight', 'resnet.6.4.bn3.bias', 'resnet.6.4.bn3.running_mean', 'resnet.6.4.bn3.running_var', 'resnet.6.5.conv1.weight', 'resnet.6.5.bn1.weight', 'resnet.6.5.bn1.bias', 'resnet.6.5.bn1.running_mean', 'resnet.6.5.bn1.running_var', 'resnet.6.5.conv2.weight', 'resnet.6.5.bn2.weight', 'resnet.6.5.bn2.bias', 'resnet.6.5.bn2.running_mean', 'resnet.6.5.bn2.running_var', 'resnet.6.5.conv3.weight', 'resnet.6.5.bn3.weight', 'resnet.6.5.bn3.bias', 'resnet.6.5.bn3.running_mean', 'resnet.6.5.bn3.running_var', 'resnet.6.6.conv1.weight', 'resnet.6.6.bn1.weight', 'resnet.6.6.bn1.bias', 'resnet.6.6.bn1.running_mean', 'resnet.6.6.bn1.running_var', 'resnet.6.6.conv2.weight', 'resnet.6.6.bn2.weight', 'resnet.6.6.bn2.bias', 'resnet.6.6.bn2.running_mean', 'resnet.6.6.bn2.running_var', 'resnet.6.6.conv3.weight', 'resnet.6.6.bn3.weight', 'resnet.6.6.bn3.bias', 'resnet.6.6.bn3.running_mean', 'resnet.6.6.bn3.running_var', 'resnet.6.7.conv1.weight', 'resnet.6.7.bn1.weight', 'resnet.6.7.bn1.bias', 'resnet.6.7.bn1.running_mean', 'resnet.6.7.bn1.running_var', 'resnet.6.7.conv2.weight', 'resnet.6.7.bn2.weight', 'resnet.6.7.bn2.bias', 'resnet.6.7.bn2.running_mean', 'resnet.6.7.bn2.running_var', 'resnet.6.7.conv3.weight', 'resnet.6.7.bn3.weight', 'resnet.6.7.bn3.bias', 'resnet.6.7.bn3.running_mean', 'resnet.6.7.bn3.running_var', 'resnet.6.8.conv1.weight', 'resnet.6.8.bn1.weight', 'resnet.6.8.bn1.bias', 'resnet.6.8.bn1.running_mean', 'resnet.6.8.bn1.running_var', 'resnet.6.8.conv2.weight', 'resnet.6.8.bn2.weight', 'resnet.6.8.bn2.bias', 'resnet.6.8.bn2.running_mean', 'resnet.6.8.bn2.running_var', 'resnet.6.8.conv3.weight', 'resnet.6.8.bn3.weight', 'resnet.6.8.bn3.bias', 'resnet.6.8.bn3.running_mean', 'resnet.6.8.bn3.running_var', 'resnet.6.9.conv1.weight', 'resnet.6.9.bn1.weight', 'resnet.6.9.bn1.bias', 'resnet.6.9.bn1.running_mean', 'resnet.6.9.bn1.running_var', 'resnet.6.9.conv2.weight', 'resnet.6.9.bn2.weight', 'resnet.6.9.bn2.bias', 'resnet.6.9.bn2.running_mean', 'resnet.6.9.bn2.running_var', 'resnet.6.9.conv3.weight', 'resnet.6.9.bn3.weight', 'resnet.6.9.bn3.bias', 'resnet.6.9.bn3.running_mean', 'resnet.6.9.bn3.running_var', 'resnet.6.10.conv1.weight', 'resnet.6.10.bn1.weight', 'resnet.6.10.bn1.bias', 'resnet.6.10.bn1.running_mean', 'resnet.6.10.bn1.running_var', 'resnet.6.10.conv2.weight', 'resnet.6.10.bn2.weight', 'resnet.6.10.bn2.bias', 'resnet.6.10.bn2.running_mean', 'resnet.6.10.bn2.running_var', 'resnet.6.10.conv3.weight', 'resnet.6.10.bn3.weight', 'resnet.6.10.bn3.bias', 'resnet.6.10.bn3.running_mean', 'resnet.6.10.bn3.running_var', 'resnet.6.11.conv1.weight', 'resnet.6.11.bn1.weight', 'resnet.6.11.bn1.bias', 'resnet.6.11.bn1.running_mean', 'resnet.6.11.bn1.running_var', 'resnet.6.11.conv2.weight', 'resnet.6.11.bn2.weight', 'resnet.6.11.bn2.bias', 'resnet.6.11.bn2.running_mean', 'resnet.6.11.bn2.running_var', 'resnet.6.11.conv3.weight', 'resnet.6.11.bn3.weight', 'resnet.6.11.bn3.bias', 'resnet.6.11.bn3.running_mean', 'resnet.6.11.bn3.running_var', 'resnet.6.12.conv1.weight', 'resnet.6.12.bn1.weight', 'resnet.6.12.bn1.bias', 'resnet.6.12.bn1.running_mean', 'resnet.6.12.bn1.running_var', 'resnet.6.12.conv2.weight', 'resnet.6.12.bn2.weight', 'resnet.6.12.bn2.bias', 'resnet.6.12.bn2.running_mean', 'resnet.6.12.bn2.running_var', 'resnet.6.12.conv3.weight', 'resnet.6.12.bn3.weight', 'resnet.6.12.bn3.bias', 'resnet.6.12.bn3.running_mean', 'resnet.6.12.bn3.running_var', 'resnet.6.13.conv1.weight', 'resnet.6.13.bn1.weight', 'resnet.6.13.bn1.bias', 'resnet.6.13.bn1.running_mean', 'resnet.6.13.bn1.running_var', 'resnet.6.13.conv2.weight', 'resnet.6.13.bn2.weight', 'resnet.6.13.bn2.bias', 'resnet.6.13.bn2.running_mean', 'resnet.6.13.bn2.running_var', 'resnet.6.13.conv3.weight', 'resnet.6.13.bn3.weight', 'resnet.6.13.bn3.bias', 'resnet.6.13.bn3.running_mean', 'resnet.6.13.bn3.running_var', 'resnet.6.14.conv1.weight', 'resnet.6.14.bn1.weight', 'resnet.6.14.bn1.bias', 'resnet.6.14.bn1.running_mean', 'resnet.6.14.bn1.running_var', 'resnet.6.14.conv2.weight', 'resnet.6.14.bn2.weight', 'resnet.6.14.bn2.bias', 'resnet.6.14.bn2.running_mean', 'resnet.6.14.bn2.running_var', 'resnet.6.14.conv3.weight', 'resnet.6.14.bn3.weight', 'resnet.6.14.bn3.bias', 'resnet.6.14.bn3.running_mean', 'resnet.6.14.bn3.running_var', 'resnet.6.15.conv1.weight', 'resnet.6.15.bn1.weight', 'resnet.6.15.bn1.bias', 'resnet.6.15.bn1.running_mean', 'resnet.6.15.bn1.running_var', 'resnet.6.15.conv2.weight', 'resnet.6.15.bn2.weight', 'resnet.6.15.bn2.bias', 'resnet.6.15.bn2.running_mean', 'resnet.6.15.bn2.running_var', 'resnet.6.15.conv3.weight', 'resnet.6.15.bn3.weight', 'resnet.6.15.bn3.bias', 'resnet.6.15.bn3.running_mean', 'resnet.6.15.bn3.running_var', 'resnet.6.16.conv1.weight', 'resnet.6.16.bn1.weight', 'resnet.6.16.bn1.bias', 'resnet.6.16.bn1.running_mean', 'resnet.6.16.bn1.running_var', 'resnet.6.16.conv2.weight', 'resnet.6.16.bn2.weight', 'resnet.6.16.bn2.bias', 'resnet.6.16.bn2.running_mean', 'resnet.6.16.bn2.running_var', 'resnet.6.16.conv3.weight', 'resnet.6.16.bn3.weight', 'resnet.6.16.bn3.bias', 'resnet.6.16.bn3.running_mean', 'resnet.6.16.bn3.running_var', 'resnet.6.17.conv1.weight', 'resnet.6.17.bn1.weight', 'resnet.6.17.bn1.bias', 'resnet.6.17.bn1.running_mean', 'resnet.6.17.bn1.running_var', 'resnet.6.17.conv2.weight', 'resnet.6.17.bn2.weight', 'resnet.6.17.bn2.bias', 'resnet.6.17.bn2.running_mean', 'resnet.6.17.bn2.running_var', 'resnet.6.17.conv3.weight', 'resnet.6.17.bn3.weight', 'resnet.6.17.bn3.bias', 'resnet.6.17.bn3.running_mean', 'resnet.6.17.bn3.running_var', 'resnet.6.18.conv1.weight', 'resnet.6.18.bn1.weight', 'resnet.6.18.bn1.bias', 'resnet.6.18.bn1.running_mean', 'resnet.6.18.bn1.running_var', 'resnet.6.18.conv2.weight', 'resnet.6.18.bn2.weight', 'resnet.6.18.bn2.bias', 'resnet.6.18.bn2.running_mean', 'resnet.6.18.bn2.running_var', 'resnet.6.18.conv3.weight', 'resnet.6.18.bn3.weight', 'resnet.6.18.bn3.bias', 'resnet.6.18.bn3.running_mean', 'resnet.6.18.bn3.running_var', 'resnet.6.19.conv1.weight', 'resnet.6.19.bn1.weight', 'resnet.6.19.bn1.bias', 'resnet.6.19.bn1.running_mean', 'resnet.6.19.bn1.running_var', 'resnet.6.19.conv2.weight', 'resnet.6.19.bn2.weight', 'resnet.6.19.bn2.bias', 'resnet.6.19.bn2.running_mean', 'resnet.6.19.bn2.running_var', 'resnet.6.19.conv3.weight', 'resnet.6.19.bn3.weight', 'resnet.6.19.bn3.bias', 'resnet.6.19.bn3.running_mean', 'resnet.6.19.bn3.running_var', 'resnet.6.20.conv1.weight', 'resnet.6.20.bn1.weight', 'resnet.6.20.bn1.bias', 'resnet.6.20.bn1.running_mean', 'resnet.6.20.bn1.running_var', 'resnet.6.20.conv2.weight', 'resnet.6.20.bn2.weight', 'resnet.6.20.bn2.bias', 'resnet.6.20.bn2.running_mean', 'resnet.6.20.bn2.running_var', 'resnet.6.20.conv3.weight', 'resnet.6.20.bn3.weight', 'resnet.6.20.bn3.bias', 'resnet.6.20.bn3.running_mean', 'resnet.6.20.bn3.running_var', 'resnet.6.21.conv1.weight', 'resnet.6.21.bn1.weight', 'resnet.6.21.bn1.bias', 'resnet.6.21.bn1.running_mean', 'resnet.6.21.bn1.running_var', 'resnet.6.21.conv2.weight', 'resnet.6.21.bn2.weight', 'resnet.6.21.bn2.bias', 'resnet.6.21.bn2.running_mean', 'resnet.6.21.bn2.running_var', 'resnet.6.21.conv3.weight', 'resnet.6.21.bn3.weight', 'resnet.6.21.bn3.bias', 'resnet.6.21.bn3.running_mean', 'resnet.6.21.bn3.running_var', 'resnet.6.22.conv1.weight', 'resnet.6.22.bn1.weight', 'resnet.6.22.bn1.bias', 'resnet.6.22.bn1.running_mean', 'resnet.6.22.bn1.running_var', 'resnet.6.22.conv2.weight', 'resnet.6.22.bn2.weight', 'resnet.6.22.bn2.bias', 'resnet.6.22.bn2.running_mean', 'resnet.6.22.bn2.running_var', 'resnet.6.22.conv3.weight', 'resnet.6.22.bn3.weight', 'resnet.6.22.bn3.bias', 'resnet.6.22.bn3.running_mean', 'resnet.6.22.bn3.running_var', 'resnet.6.23.conv1.weight', 'resnet.6.23.bn1.weight', 'resnet.6.23.bn1.bias', 'resnet.6.23.bn1.running_mean', 'resnet.6.23.bn1.running_var', 'resnet.6.23.conv2.weight', 'resnet.6.23.bn2.weight', 'resnet.6.23.bn2.bias', 'resnet.6.23.bn2.running_mean', 'resnet.6.23.bn2.running_var', 'resnet.6.23.conv3.weight', 'resnet.6.23.bn3.weight', 'resnet.6.23.bn3.bias', 'resnet.6.23.bn3.running_mean', 'resnet.6.23.bn3.running_var', 'resnet.6.24.conv1.weight', 'resnet.6.24.bn1.weight', 'resnet.6.24.bn1.bias', 'resnet.6.24.bn1.running_mean', 'resnet.6.24.bn1.running_var', 'resnet.6.24.conv2.weight', 'resnet.6.24.bn2.weight', 'resnet.6.24.bn2.bias', 'resnet.6.24.bn2.running_mean', 'resnet.6.24.bn2.running_var', 'resnet.6.24.conv3.weight', 'resnet.6.24.bn3.weight', 'resnet.6.24.bn3.bias', 'resnet.6.24.bn3.running_mean', 'resnet.6.24.bn3.running_var', 'resnet.6.25.conv1.weight', 'resnet.6.25.bn1.weight', 'resnet.6.25.bn1.bias', 'resnet.6.25.bn1.running_mean', 'resnet.6.25.bn1.running_var', 'resnet.6.25.conv2.weight', 'resnet.6.25.bn2.weight', 'resnet.6.25.bn2.bias', 'resnet.6.25.bn2.running_mean', 'resnet.6.25.bn2.running_var', 'resnet.6.25.conv3.weight', 'resnet.6.25.bn3.weight', 'resnet.6.25.bn3.bias', 'resnet.6.25.bn3.running_mean', 'resnet.6.25.bn3.running_var', 'resnet.6.26.conv1.weight', 'resnet.6.26.bn1.weight', 'resnet.6.26.bn1.bias', 'resnet.6.26.bn1.running_mean', 'resnet.6.26.bn1.running_var', 'resnet.6.26.conv2.weight', 'resnet.6.26.bn2.weight', 'resnet.6.26.bn2.bias', 'resnet.6.26.bn2.running_mean', 'resnet.6.26.bn2.running_var', 'resnet.6.26.conv3.weight', 'resnet.6.26.bn3.weight', 'resnet.6.26.bn3.bias', 'resnet.6.26.bn3.running_mean', 'resnet.6.26.bn3.running_var', 'resnet.6.27.conv1.weight', 'resnet.6.27.bn1.weight', 'resnet.6.27.bn1.bias', 'resnet.6.27.bn1.running_mean', 'resnet.6.27.bn1.running_var', 'resnet.6.27.conv2.weight', 'resnet.6.27.bn2.weight', 'resnet.6.27.bn2.bias', 'resnet.6.27.bn2.running_mean', 'resnet.6.27.bn2.running_var', 'resnet.6.27.conv3.weight', 'resnet.6.27.bn3.weight', 'resnet.6.27.bn3.bias', 'resnet.6.27.bn3.running_mean', 'resnet.6.27.bn3.running_var', 'resnet.6.28.conv1.weight', 'resnet.6.28.bn1.weight', 'resnet.6.28.bn1.bias', 'resnet.6.28.bn1.running_mean', 'resnet.6.28.bn1.running_var', 'resnet.6.28.conv2.weight', 'resnet.6.28.bn2.weight', 'resnet.6.28.bn2.bias', 'resnet.6.28.bn2.running_mean', 'resnet.6.28.bn2.running_var', 'resnet.6.28.conv3.weight', 'resnet.6.28.bn3.weight', 'resnet.6.28.bn3.bias', 'resnet.6.28.bn3.running_mean', 'resnet.6.28.bn3.running_var', 'resnet.6.29.conv1.weight', 'resnet.6.29.bn1.weight', 'resnet.6.29.bn1.bias', 'resnet.6.29.bn1.running_mean', 'resnet.6.29.bn1.running_var', 'resnet.6.29.conv2.weight', 'resnet.6.29.bn2.weight', 'resnet.6.29.bn2.bias', 'resnet.6.29.bn2.running_mean', 'resnet.6.29.bn2.running_var', 'resnet.6.29.conv3.weight', 'resnet.6.29.bn3.weight', 'resnet.6.29.bn3.bias', 'resnet.6.29.bn3.running_mean', 'resnet.6.29.bn3.running_var', 'resnet.6.30.conv1.weight', 'resnet.6.30.bn1.weight', 'resnet.6.30.bn1.bias', 'resnet.6.30.bn1.running_mean', 'resnet.6.30.bn1.running_var', 'resnet.6.30.conv2.weight', 'resnet.6.30.bn2.weight', 'resnet.6.30.bn2.bias', 'resnet.6.30.bn2.running_mean', 'resnet.6.30.bn2.running_var', 'resnet.6.30.conv3.weight', 'resnet.6.30.bn3.weight', 'resnet.6.30.bn3.bias', 'resnet.6.30.bn3.running_mean', 'resnet.6.30.bn3.running_var', 'resnet.6.31.conv1.weight', 'resnet.6.31.bn1.weight', 'resnet.6.31.bn1.bias', 'resnet.6.31.bn1.running_mean', 'resnet.6.31.bn1.running_var', 'resnet.6.31.conv2.weight', 'resnet.6.31.bn2.weight', 'resnet.6.31.bn2.bias', 'resnet.6.31.bn2.running_mean', 'resnet.6.31.bn2.running_var', 'resnet.6.31.conv3.weight', 'resnet.6.31.bn3.weight', 'resnet.6.31.bn3.bias', 'resnet.6.31.bn3.running_mean', 'resnet.6.31.bn3.running_var', 'resnet.6.32.conv1.weight', 'resnet.6.32.bn1.weight', 'resnet.6.32.bn1.bias', 'resnet.6.32.bn1.running_mean', 'resnet.6.32.bn1.running_var', 'resnet.6.32.conv2.weight', 'resnet.6.32.bn2.weight', 'resnet.6.32.bn2.bias', 'resnet.6.32.bn2.running_mean', 'resnet.6.32.bn2.running_var', 'resnet.6.32.conv3.weight', 'resnet.6.32.bn3.weight', 'resnet.6.32.bn3.bias', 'resnet.6.32.bn3.running_mean', 'resnet.6.32.bn3.running_var', 'resnet.6.33.conv1.weight', 'resnet.6.33.bn1.weight', 'resnet.6.33.bn1.bias', 'resnet.6.33.bn1.running_mean', 'resnet.6.33.bn1.running_var', 'resnet.6.33.conv2.weight', 'resnet.6.33.bn2.weight', 'resnet.6.33.bn2.bias', 'resnet.6.33.bn2.running_mean', 'resnet.6.33.bn2.running_var', 'resnet.6.33.conv3.weight', 'resnet.6.33.bn3.weight', 'resnet.6.33.bn3.bias', 'resnet.6.33.bn3.running_mean', 'resnet.6.33.bn3.running_var', 'resnet.6.34.conv1.weight', 'resnet.6.34.bn1.weight', 'resnet.6.34.bn1.bias', 'resnet.6.34.bn1.running_mean', 'resnet.6.34.bn1.running_var', 'resnet.6.34.conv2.weight', 'resnet.6.34.bn2.weight', 'resnet.6.34.bn2.bias', 'resnet.6.34.bn2.running_mean', 'resnet.6.34.bn2.running_var', 'resnet.6.34.conv3.weight', 'resnet.6.34.bn3.weight', 'resnet.6.34.bn3.bias', 'resnet.6.34.bn3.running_mean', 'resnet.6.34.bn3.running_var', 'resnet.6.35.conv1.weight', 'resnet.6.35.bn1.weight', 'resnet.6.35.bn1.bias', 'resnet.6.35.bn1.running_mean', 'resnet.6.35.bn1.running_var', 'resnet.6.35.conv2.weight', 'resnet.6.35.bn2.weight', 'resnet.6.35.bn2.bias', 'resnet.6.35.bn2.running_mean', 'resnet.6.35.bn2.running_var', 'resnet.6.35.conv3.weight', 'resnet.6.35.bn3.weight', 'resnet.6.35.bn3.bias', 'resnet.6.35.bn3.running_mean', 'resnet.6.35.bn3.running_var', 'resnet.7.0.conv1.weight', 'resnet.7.0.bn1.weight', 'resnet.7.0.bn1.bias', 'resnet.7.0.bn1.running_mean', 'resnet.7.0.bn1.running_var', 'resnet.7.0.conv2.weight', 'resnet.7.0.bn2.weight', 'resnet.7.0.bn2.bias', 'resnet.7.0.bn2.running_mean', 'resnet.7.0.bn2.running_var', 'resnet.7.0.conv3.weight', 'resnet.7.0.bn3.weight', 'resnet.7.0.bn3.bias', 'resnet.7.0.bn3.running_mean', 'resnet.7.0.bn3.running_var', 'resnet.7.0.downsample.0.weight', 'resnet.7.0.downsample.1.weight', 'resnet.7.0.downsample.1.bias', 'resnet.7.0.downsample.1.running_mean', 'resnet.7.0.downsample.1.running_var', 'resnet.7.1.conv1.weight', 'resnet.7.1.bn1.weight', 'resnet.7.1.bn1.bias', 'resnet.7.1.bn1.running_mean', 'resnet.7.1.bn1.running_var', 'resnet.7.1.conv2.weight', 'resnet.7.1.bn2.weight', 'resnet.7.1.bn2.bias', 'resnet.7.1.bn2.running_mean', 'resnet.7.1.bn2.running_var', 'resnet.7.1.conv3.weight', 'resnet.7.1.bn3.weight', 'resnet.7.1.bn3.bias', 'resnet.7.1.bn3.running_mean', 'resnet.7.1.bn3.running_var', 'resnet.7.2.conv1.weight', 'resnet.7.2.bn1.weight', 'resnet.7.2.bn1.bias', 'resnet.7.2.bn1.running_mean', 'resnet.7.2.bn1.running_var', 'resnet.7.2.conv2.weight', 'resnet.7.2.bn2.weight', 'resnet.7.2.bn2.bias', 'resnet.7.2.bn2.running_mean', 'resnet.7.2.bn2.running_var', 'resnet.7.2.conv3.weight', 'resnet.7.2.bn3.weight', 'resnet.7.2.bn3.bias', 'resnet.7.2.bn3.running_mean', 'resnet.7.2.bn3.running_var', 'linear.weight', 'linear.bias', 'bn.weight', 'bn.bias', 'bn.running_mean', 'bn.running_var'])\n",
            "Decoder keys: odict_keys(['embed.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'linear.weight', 'linear.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df0d91bd",
        "outputId": "f71e1dc4-a826-41f9-8bb3-b8d3ba135290"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Path to the pytorch-tutorial directory\n",
        "pytorch_tutorial_path = '/content/pytorch-tutorial/tutorials/03-advanced/image_captioning'\n",
        "\n",
        "# Add the pytorch-tutorial directory to the system path to import necessary modules\n",
        "sys.path.insert(0, pytorch_tutorial_path)\n",
        "\n",
        "# Change the current working directory to the pytorch-tutorial directory\n",
        "os.chdir(pytorch_tutorial_path)\n",
        "\n",
        "# Import the necessary file that defines the Vocabulary class\n",
        "# Assuming the Vocabulary class is in a file named 'build_vocab.py' or similar\n",
        "# You might need to adjust the import based on the actual file name in the repository\n",
        "try:\n",
        "    from build_vocab import Vocabulary\n",
        "except ImportError:\n",
        "    # If the above import fails, try importing from data_loader\n",
        "    try:\n",
        "        from data_loader import Vocabulary\n",
        "    except ImportError:\n",
        "        print(\"Could not find the 'Vocabulary' class definition. Please check the file name.\")\n",
        "        # It's important to exit or handle this error if the class isn't found\n",
        "        # For now, we'll just print a message and continue, which might lead to further errors.\n",
        "        # A better approach might be to raise an error or stop execution.\n",
        "        pass\n",
        "\n",
        "\n",
        "# Path to the vocabulary file\n",
        "vocab_path = os.path.join('/content/pytorch-tutorial/data', 'vocab.pkl')\n",
        "\n",
        "# Load the vocabulary wrapper\n",
        "# Add error handling for file not found just in case\n",
        "try:\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "    # You can inspect some words in the vocabulary if needed\n",
        "    # print(vocab.idx2word[:10])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Vocabulary file not found at {vocab_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the vocabulary: {e}\")\n",
        "\n",
        "# It's generally good practice to change back to the original directory if needed\n",
        "# os.chdir('/content') # Uncomment if you need to change back"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/decoder-5-3000.pkl /content/encoder-5-3000.pkl /content/pytorch-tutorial/tutorials/03-advanced/image_captioning/models/"
      ],
      "metadata": {
        "id": "hhj0x5KSv54j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flower image - my_image.jpg\n",
        "!python sample.py --image my_image.jpg | tee -a sample_predictions.txt\n",
        "!python sample.py --image animal.jpg | tee -a sample_predictions.txt\n",
        "!python sample.py --image merc.jpg | tee -a sample_predictions.txt"
      ],
      "metadata": {
        "id": "k9IkDt-YwQ0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0b9242-2dc6-447e-bd9c-06be77ed4db3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> a person holding a piece of cake with a fork . <end>\n",
            "<start> a black and white cat is sitting on a couch . <end>\n",
            "<start> a car parked in front of a car . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate what to do if you want to run it with Keras\n",
        "I have tried to implement it in PyTorch, but please investigate what steps I should take if I want to run it in Keras. In particular, please mention how to make the trained weights in PyTorch usable in Keras."
      ],
      "metadata": {
        "id": "3Vzfq5gsERtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running **image captioning** in **Keras** instead of PyTorch involves several challenges, especially when it comes to using **pre-trained PyTorch weights in Keras**. PyTorch and Keras (TensorFlow backend) use different model definitions, serialization formats, and internal layer representations.\n",
        "\n",
        "---\n",
        "### Summary of Steps\n",
        "\n",
        "#### 1. **Find or Build a Keras Image Captioning Model**\n",
        "\n",
        "Since Keras doesn’t have an official end-to-end image captioning implementation with pre-trained weights like Yunjey’s PyTorch version, you have two choices:\n",
        "\n",
        "* **Option A: Build from scratch in Keras**\n",
        "* **Option B: Convert PyTorch model + weights to Keras** (complex and error-prone)\n",
        "\n",
        "---\n",
        "\n",
        "### Option A: Build Image Captioning Model in Keras\n",
        "\n",
        "#### **High-level architecture**:\n",
        "\n",
        "1. **Encoder**: Pre-trained CNN (e.g., InceptionV3 or ResNet50)\n",
        "2. **Decoder**: RNN (typically LSTM) with an attention mechanism\n",
        "3. **Output**: Word-by-word caption generation\n",
        "\n",
        "#### Resources:\n",
        "\n",
        "* **Keras example** (without pre-trained decoder weights):\n",
        "  [Image captioning with visual attention (Keras)](https://keras.io/examples/vision/image_captioning/)\n",
        "\n",
        "#### Key Steps:\n",
        "\n",
        "1. Use **InceptionV3 / ResNet50** as CNN encoder\n",
        "2. Extract features from the image using the CNN\n",
        "3. Use an **LSTM decoder** with attention to generate the caption\n",
        "4. Train or fine-tune on a dataset like COCO or Flickr8k\n",
        "\n",
        "---\n",
        "\n",
        "### Option B: Convert PyTorch Weights to Keras\n",
        "\n",
        "This requires deep familiarity with both frameworks.\n",
        "\n",
        "### Conversion is difficult because:\n",
        "\n",
        "* PyTorch and Keras store weights **differently**\n",
        "* Layer names and architectures **don’t match 1:1**\n",
        "* No official tool exists for **automatic weight translation**\n",
        "\n",
        "###  Possible Workarounds:\n",
        "\n",
        "1. **Manually port weights** layer-by-layer:\n",
        "\n",
        "   * Export PyTorch weights (`.pt` or `.pkl`)\n",
        "   * Convert them to NumPy\n",
        "   * Load them into matching Keras layers using `set_weights()`\n",
        "   * Painstaking and error-prone\n",
        "\n",
        "2. **Use ONNX as an intermediate**:\n",
        "\n",
        "   * Convert PyTorch → ONNX\n",
        "\n",
        "   * Try ONNX → TensorFlow (via `onnx-tf`)\n",
        "\n",
        "   * Then load model into Keras\n",
        "\n",
        "   > But ONNX → Keras conversion is very fragile for custom models.\n",
        "\n",
        "### Example conversion path:\n",
        "\n",
        "```bash\n",
        "# Convert PyTorch to ONNX\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
        "\n",
        "# Convert ONNX to TensorFlow\n",
        "onnx-tf convert -i model.onnx -o tf_model\n",
        "\n",
        "# Attempt to load TF model in Keras\n",
        "tf.keras.models.load_model('tf_model')\n",
        "```\n",
        "\n",
        "This might work for **simple feed-forward models**, but often fails with **custom models like encoder-decoder architectures with attention**.\n",
        "\n",
        "---\n",
        "\n",
        "### Recommended Path\n",
        "\n",
        "If aiming to work in **Keras**, the **best approach** is:\n",
        "\n",
        "#### Use this official example from Keras:\n",
        "\n",
        " [Image Captioning with Visual Attention (Keras)](https://keras.io/examples/vision/image_captioning/)\n",
        "\n",
        "This implementation:\n",
        "\n",
        "* Uses TensorFlow/Keras\n",
        "* Extracts image features with InceptionV3\n",
        "* Uses a custom LSTM decoder with Bahdanau attention\n",
        "* Can be fine-tuned or extended\n",
        "\n",
        "### If we want to reuse PyTorch-trained captions or vocabulary:\n",
        "\n",
        "* Export the vocabulary (word2idx) as a JSON or pickle\n",
        "* Load it into your Keras model\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Task            | PyTorch             | Keras                        | Notes                                                |\n",
        "| --------------- | ------------------- | ---------------------------- | ---------------------------------------------------- |\n",
        "| Encoder CNN     | Pre-trained ResNet  | Pre-trained InceptionV3      | Replaceable                                          |\n",
        "| Decoder RNN     | Custom LSTM         | LSTM w/ attention            | Must build anew in Keras                             |\n",
        "| Weight transfer | Pickle/pt           | HDF5                         | Manual conversion or retraining required             |\n",
        "| Model loading   | `torch.load`        | `tf.keras.models.load_model` | Incompatible formats                                 |\n",
        "| Best option     | Use PyTorch version | Use Keras example            | Start fresh in Keras if you want to stay in TF/Keras |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-JTPHlpZEyPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(Advanced assignment) Code reading and rewriting\n",
        "The model part is written in [model.py], but please think about how to write this model in Keras and write the actual code. At this time, the machine translated sample code will be helpful."
      ],
      "metadata": {
        "id": "M9p23OYrGnKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll implement:\n",
        "\n",
        "1. **EncoderCNN** using a pre-trained **ResNet152**\n",
        "2. **DecoderRNN** using **Embedding + LSTM**\n",
        "3. Greedy caption sampling\n",
        "\n",
        "---\n",
        "\n",
        "#### Architecture Notes\n",
        "\n",
        "| PyTorch                             | Keras                                                 |\n",
        "| ----------------------------------- | ----------------------------------------------------- |\n",
        "| `models.resnet152(pretrained=True)` | `tf.keras.applications.ResNet152(weights='imagenet')` |\n",
        "| `nn.Linear(...)`                    | `Dense(...)`                                          |\n",
        "| `nn.LSTM(...)`                      | `tf.keras.layers.LSTM(...)`                           |\n",
        "| `Embedding(vocab_size, embed_dim)`  | `tf.keras.layers.Embedding(...)`                      |\n",
        "| `sample()`                          | Greedy decoding in Keras with `predict()` and loops   |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rXgMLKGcG1uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Keras Implementation of `model.py`\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "\n",
        "\n",
        "class EncoderCNN(tf.keras.Model):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        # Load ResNet152 without the top layer\n",
        "        base_model = ResNet152(include_top=False, weights='imagenet', pooling='avg')\n",
        "        base_model.trainable = False  # Freeze base model\n",
        "        self.resnet = base_model\n",
        "        self.fc = layers.Dense(embed_size)\n",
        "        self.bn = layers.BatchNormalization(momentum=0.01)\n",
        "\n",
        "    def call(self, images):\n",
        "        x = preprocess_input(images)  # ResNet preprocessing\n",
        "        features = self.resnet(x)\n",
        "        features = self.fc(features)\n",
        "        features = self.bn(features)\n",
        "        return features  # shape: (batch_size, embed_size)\n",
        "\n",
        "\n",
        "class DecoderRNN(tf.keras.Model):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, max_seq_length=20):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = layers.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "        self.dense = layers.Dense(vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def call(self, features, captions, lengths):\n",
        "        # Remove the last token (e.g. <end>) during training\n",
        "        captions_input = captions[:, :-1]\n",
        "        embeddings = self.embed(captions_input)\n",
        "\n",
        "        # Prepend image features as the first \"word\"\n",
        "        features = tf.expand_dims(features, 1)\n",
        "        inputs = tf.concat([features, embeddings], axis=1)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        outputs, _, _ = self.lstm(inputs)\n",
        "        logits = self.dense(outputs)\n",
        "        return logits  # shape: (batch_size, caption_len, vocab_size)\n",
        "\n",
        "    def sample(self, features, start_token, end_token):\n",
        "        # Greedy decoding loop\n",
        "        input_word = tf.expand_dims([start_token], 0)  # shape: (1, 1)\n",
        "        caption = []\n",
        "\n",
        "        state_h, state_c = None, None\n",
        "        inputs = tf.expand_dims(features, 1)\n",
        "\n",
        "        for _ in range(self.max_seq_length):\n",
        "            if state_h is None:\n",
        "                output, state_h, state_c = self.lstm(inputs)\n",
        "            else:\n",
        "                output, state_h, state_c = self.lstm(inputs, initial_state=[state_h, state_c])\n",
        "\n",
        "            logits = self.dense(output)  # (1, 1, vocab_size)\n",
        "            predicted_id = tf.argmax(logits[0, 0]).numpy()\n",
        "            caption.append(predicted_id)\n",
        "\n",
        "            if predicted_id == end_token:\n",
        "                break\n",
        "\n",
        "            inputs = tf.expand_dims(self.embed([predicted_id]), 1)  # shape: (1, 1, embed_size)\n",
        "\n",
        "        return caption"
      ],
      "metadata": {
        "id": "aE8KLtwXF0Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Usage\n",
        "\n",
        "Here's how you'd use the models after building and compiling:"
      ],
      "metadata": {
        "id": "d9gxvreOHljF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "EMBED_SIZE = 256\n",
        "HIDDEN_SIZE = 512\n",
        "VOCAB_SIZE = len(word2idx)\n",
        "MAX_SEQ_LEN = 20\n",
        "\n",
        "# Instantiate models\n",
        "encoder = EncoderCNN(embed_size=EMBED_SIZE)\n",
        "decoder = DecoderRNN(embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, vocab_size=VOCAB_SIZE, max_seq_length=MAX_SEQ_LEN)\n",
        "\n",
        "# Load and preprocess image\n",
        "img = tf.keras.preprocessing.image.load_img(\"my_image.jpg\", target_size=(224, 224))\n",
        "img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "img = tf.expand_dims(img, 0)  # batch dimension\n",
        "\n",
        "# Extract features\n",
        "features = encoder(img)\n",
        "\n",
        "# Generate caption\n",
        "start_token = word2idx['<start>']\n",
        "end_token = word2idx['<end>']\n",
        "generated_ids = decoder.sample(features, start_token, end_token)\n",
        "\n",
        "# Convert tokens to words\n",
        "caption = [idx2word[i] for i in generated_ids]\n",
        "print(' '.join(caption))\n"
      ],
      "metadata": {
        "id": "_LwK8I2IHwHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART 1: Machine Translation – Translating Between Japanese and English\n",
        "\n",
        "#### General Steps\n",
        "\n",
        "When building a **machine translation system** (e.g., Japanese ⇄ English), here’s what you need:\n",
        "\n",
        "1. **Data**\n",
        "\n",
        "   * Parallel corpora: sentence pairs in Japanese and English (e.g., [JParaCrawl](https://opus.nlpl.eu/JParaCrawl.php), [Tatoeba](https://tatoeba.org/))\n",
        "   * Preprocessing: tokenization, subword units (e.g., SentencePiece or Byte-Pair Encoding)\n",
        "\n",
        "2. **Model Choices**\n",
        "\n",
        "   * **Seq2Seq with Attention**\n",
        "   * **Transformer models** (e.g., T5, MarianMT, mBART, mT5)\n",
        "\n",
        "3. **Training**\n",
        "\n",
        "   * Loss: Cross-entropy loss over vocabulary\n",
        "   * Metrics: BLEU, METEOR, or COMET scores\n",
        "\n",
        "4. **Inference**\n",
        "\n",
        "   * Greedy decoding / Beam Search\n",
        "   * Token to string conversion\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Methods of Machine Translation\n",
        "\n",
        "#### 1. **Attention Mechanism**\n",
        "\n",
        "* Introduced in [Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473)\n",
        "* Learns where to focus in the source sentence when generating each word in the target sentence.\n",
        "* Led to significantly improved performance over vanilla Seq2Seq models.\n",
        "\n",
        "#### 2. **Transformer Model (Vaswani et al., 2017)**\n",
        "\n",
        "* Uses **multi-head self-attention** instead of RNNs.\n",
        "* Faster training (parallelizable), better long-range dependency modeling.\n",
        "* Foundation of modern translation models: BERT, GPT, T5, etc.\n",
        "\n",
        "#### 3. **Pre-trained Multilingual Models**\n",
        "\n",
        "* **MarianMT**: Trained on many language pairs using the Transformer architecture.\n",
        "* **mBART**: Pre-trained on denoising multilingual text; fine-tuned for translation tasks.\n",
        "* **mT5**: A multilingual variant of the T5 model by Google.\n",
        "* These support zero-shot and few-shot translation across many languages.\n",
        "\n",
        "---\n",
        "\n",
        "### Evolutionary Approaches (Beyond Transformers)\n",
        "\n",
        "Some **experimental or hybrid techniques** being researched:\n",
        "\n",
        "* **Neuroevolution of Augmenting Topologies (NEAT)** for evolving encoder-decoder architectures\n",
        "* **Reinforcement learning-based translation** where BLEU or human ratings are the reward\n",
        "* **Meta-learning** for low-resource language pairs\n",
        "* **Multimodal translation** (text + image or speech → text translation)\n",
        "\n",
        "---\n",
        "\n",
        "### PART 2: Generating Images from Text (Opposite of Image Captioning)\n",
        "\n",
        "This falls under **text-to-image synthesis**, and has advanced rapidly in recent years.\n",
        "\n",
        "### Key Technologies\n",
        "\n",
        "| Model                                      | Description                                                                                  |\n",
        "| ------------------------------------------ | -------------------------------------------------------------------------------------------- |\n",
        "| **GANs (Generative Adversarial Networks)** | Early models like StackGAN, AttnGAN used GANs with text embeddings.                          |\n",
        "| **VQ-VAE**                                 | Vector-quantized autoencoders used in early DALL·E.                                          |\n",
        "| **CLIP + Diffusion**                       | Used by modern SOTA (e.g., DALL·E 2, Stable Diffusion). CLIP connects text and image spaces. |\n",
        "| **Transformer-based Models**               | DALL·E and CogView use autoregressive transformers for image generation.                     |\n",
        "\n",
        "---\n",
        "\n",
        "### Modern Text-to-Image Models\n",
        "\n",
        "| Model                | Publisher    | Highlights                                                                         |\n",
        "| -------------------- | ------------ | ---------------------------------------------------------------------------------- |\n",
        "| **DALL·E 2 / 3**     | OpenAI       | Uses CLIP + diffusion model. High-quality and coherent images.                     |\n",
        "| **Stable Diffusion** | Stability AI | Open-source, supports custom models and fine-tuning.                               |\n",
        "| **Midjourney**       | Independent  | Artistic, stylized images. Prompt-based.                                           |\n",
        "| **Imagen**           | Google       | State-of-the-art text-to-image with unprecedented realism (not publicly released). |\n",
        "| **DeepFloyd IF**     | DeepFloyd    | Modular diffusion-based image generation model.                                    |\n",
        "\n",
        "---\n",
        "\n",
        "### Workflow Overview\n",
        "\n",
        "1. **Input**: Text prompt (e.g., “A robot reading a book under a cherry blossom tree”)\n",
        "2. **Text encoder**: Convert prompt into embeddings (CLIP, T5, etc.)\n",
        "3. **Image generator**: Use diffusion or autoregressive model to generate an image\n",
        "4. **Upsampling**: Increase resolution using super-resolution models\n",
        "\n",
        "---\n",
        "\n",
        "### Practical Toolkits\n",
        "\n",
        "* `diffusers` library by HuggingFace (for Stable Diffusion, DeepFloyd IF)\n",
        "* OpenAI’s DALL·E API\n",
        "* RunwayML (no-code tools)\n",
        "\n",
        "---\n",
        "\n",
        "### Connecting the Dots\n",
        "\n",
        "| Captioning        | Translation         | Text-to-Image           |\n",
        "| ----------------- | ------------------- | ----------------------- |\n",
        "| Image → Text      | Text → Text         | Text → Image            |\n",
        "| CNN + RNN         | Transformer         | Transformer + Diffusion |\n",
        "| e.g., Show & Tell | e.g., MarianMT, mT5 | e.g., DALL·E, SD        |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PTuQWDasH5uC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bc09670"
      },
      "source": [
        "# Task\n",
        "Correct the information about the Keras example in the notebook, execute the image captioning code with a user-provided image, record the output, and update the information about Keras image captioning based on the provided code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ac68e13"
      },
      "source": [
        "## Correct information about the keras example\n",
        "\n",
        "### Subtask:\n",
        "Update the markdown cell `-JTPHlpZEyPD` to accurately reflect that the Keras example provides a basis for image captioning with attention, not English to French translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3205004"
      },
      "source": [
        "## Execute the image captioning code\n",
        "\n",
        "### Subtask:\n",
        "Modify the existing code to use a user-provided image, run the inference, and capture the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dbe91d8"
      },
      "source": [
        "**Reasoning**:\n",
        "The goal is to use a user-provided image for inference. The existing code in cell `_LwK8I2IHwHb` loads the image from a hardcoded path 'my_image.jpg'. I need to modify this cell to use the user-provided image path instead. Since the user provided an image in the previous turn, I will use 'my_image.jpg' as the user-provided image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lt0Jv0NtpQhW"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}